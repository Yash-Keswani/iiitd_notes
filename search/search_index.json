{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>notes go here (maybe)</p>"},{"location":"CSE112_CO/","title":"Index","text":"<p>notes for CSE112 CO and also for CA</p>"},{"location":"CSE112_CO/1..computers/1..a_computer_program/","title":"A Computer Program","text":""},{"location":"CSE112_CO/1..computers/1..a_computer_program/#computer","title":"Computer","text":"<p>A computer is a machine, which operates on something (input) and gives some meaningful result (output) following a predefined process (program)</p> <p>Modern day computers use transistors made of silicon. However, it is not necessary that computers be built this way. The abacus is often regarded as a computer.</p>"},{"location":"CSE112_CO/1..computers/1..a_computer_program/#program","title":"Program","text":"<p>A program is a set of instructions given to a computer.</p>"},{"location":"CSE112_CO/1..computers/2..abstraction/","title":"Abstraction","text":"<p>Abstraction is perhaps, one of the most important concepts to understand in the realm of computing.</p> <p>Abstraction is the process by which we define an entity with only the information relevant to a user of the entity. </p> <p>When we use a function <code>print()</code>, the process by which text is sent to the terminal (stdout) is omitted. We identify this entity (function) just by its behaviour and the properties related to its behaviour (say, the stream that it is output to, and the text that is output by it)</p>"},{"location":"CSE112_CO/1..computers/2..abstraction/#why-abstract","title":"Why Abstract?","text":"<ul> <li>Code Readability - using abstractions like functions makes our code cleaner to read, because someone looking at them can evaluate the purpose of a chunk of code by just understanding the IO characteristics of the function. </li> <li>Size - using abstractions reduces the size by your code. </li> <li>Universality - the good thing about abstractions is we can mix and match layers. Code written on C for one microarchitecture can be compiled for another microarchitecture as well, using a different compiler. </li> <li>Specialisation - in a system running without abstractions, every programmer would need to understand every aspect of the system. However, thanks to abstraction, a C developer (you!) need not know the specifications of your RAM to be able to store a variable in the memory.</li> </ul>"},{"location":"CSE112_CO/1..computers/2..abstraction/#downsides-of-abstraction","title":"Downsides of Abstraction","text":"<ul> <li>This mix-and-match compatibility can often be expensive. We want each layer to be isolated in its behaviour, which is very difficult to achieve in practice.</li> <li>A monolithic architecture can be faster, as it does not involve the overheads needed to keep layers separate.</li> </ul>"},{"location":"CSE112_CO/1..computers/3..architecture_and_organisation/","title":"Architecture And Organisation","text":""},{"location":"CSE112_CO/1..computers/3..architecture_and_organisation/#architecture","title":"Architecture","text":"<p>The architecture of a computer is an analysis of the computer done by a software designer. When we think about a computer's architecture, we are concerned about how it behaves.</p>"},{"location":"CSE112_CO/1..computers/3..architecture_and_organisation/#organisation","title":"Organisation","text":"<p>The organisation of a computer is how the computer is implemented in hardware. When we think about a computer's organisation, we are concerned about how its made.</p> <p>To compare it to biology, one might say that architecture is the physiology of a computer, while organisation is its anatomy.</p>"},{"location":"CSE112_CO/1..computers/4..instruction_processing/","title":"Instruction Processing","text":""},{"location":"CSE112_CO/1..computers/4..instruction_processing/#processing-unit","title":"Processing Unit","text":"<p>The processing unit consists of:-</p> <ul> <li>Control Unit - manages decoding of instructions, as well as IO performed from the processor to registers / memory.</li> <li>(Arithmetic) Logic Unit - manages arithmetic operations, as well as execution of logical / relational operations.</li> <li>Register Files - register files can be read from or written into to persist operation results between instructions.</li> </ul>"},{"location":"CSE112_CO/1..computers/4..instruction_processing/#instruction-register","title":"Instruction Register","text":"<p>The Instruction Register stores the instruction that is currently being executed by the processor. When the processor interprets the opcode, instruction sources, or instruction destinations, it refers to the instruction register for decoding.</p>"},{"location":"CSE112_CO/1..computers/4..instruction_processing/#program-counter","title":"Program Counter","text":"<p>The Program Counter stores either the index of either:</p> <ul> <li>The instruction currently being executed</li> <li>The next instruction to be executed</li> </ul> <p>Depending upon the implementation of the processor. It can be thought of as a cursor, scrolling to whichever line of the code we're running.</p>"},{"location":"CSE112_CO/1..computers/5..memory_implementation/","title":"Memory Implementation","text":""},{"location":"CSE112_CO/1..computers/5..memory_implementation/#bus","title":"Bus","text":"<p>A bus is used to transmit information between components of a computer. Note that this bus is the same as the bus in USB (Universal Serial Bus).</p>"},{"location":"CSE112_CO/1..computers/5..memory_implementation/#von-neumann-architecture","title":"Von Neumann Architecture","text":"<p>In a Von Neumann machine, Program and Data (Information) are stored in the same memory.</p>"},{"location":"CSE112_CO/1..computers/5..memory_implementation/#harvard-machine","title":"Harvard Machine","text":"<p>In a Harvard machine, separate memories exist for Program and Data.</p>"},{"location":"CSE112_CO/1..computers/5..memory_implementation/#comparison","title":"Comparison","text":"<p>A Von Neumann machine has a single entry point to the memory, which is shared by Data and Instructions. This causes what is known as a Von Neumann bottleneck, that reduces speed. Meanwhile, Harvard memory has two buses, which allows for faster speed; but the fixed division can lead to potential wastage of memory</p> <p>Von Neumann architecture allows us to treat instructions as data. This is very important for us, because it allows us to alter the code being run by our machine as if it were data. This allows computers to be (conveniently) programmable. This is why most general purpose computers use Von Neumann architecture. </p>"},{"location":"CSE112_CO/2..ISA/1..instruction/","title":"Instruction","text":"<p>An instruction is something that we tell a computer to do. </p> <p>When we talk to someone who speaks Hindi but not English, we must talk to them in Hindi, not English. Likewise, an instruction is a command we give to the computer in its own language.</p>"},{"location":"CSE112_CO/2..ISA/1..instruction/#what-can-an-instruction-do","title":"What can an Instruction do?","text":"<p>An instruction can do many things. It typically involves the following :-</p> <ul> <li>Fetch some information from a memory location or register</li> <li>Perform some operation or comparison with this information</li> <li>Either store the result of this somewhere, or make some decision based on it</li> </ul>"},{"location":"CSE112_CO/2..ISA/1..instruction/#instruction-set","title":"Instruction Set","text":"<p>There is a finite amount of words in any language. From these hundreds of thousands of words, we use a minimal amount (in the thousands) to describe most of the things we communicate in our day-to-day lives. Likewise, an instruction set is a (preferably but not necessarily) minimal set of words that form the language used by a computer.</p>"},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/","title":"Instruction Set Architecture","text":"<p>The ISA (Instruction Set Architecture) describes the semantics of all instructions supported by a processor. </p>"},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/#types-of-instructions","title":"Types of Instructions","text":"<p>These will be discussed during the Assembly Language section</p>"},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/#desirable-features-in-isa","title":"Desirable Features in ISA","text":""},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/#complete","title":"Complete","text":"<p>The ISA is complete, and comprehensively and exhaustively capable of performing all the things we might want it to do.</p>"},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/#concise","title":"Concise","text":"<p>The Instruction Set should be limited in size. It need not be absolute minimal (as is the case of CISC ISAs which we will see later), but it should be reasonably small (32~1000 instructions is the range of sizes we see on average)</p>"},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/#simple","title":"Simple","text":"<p>These instructions have to be written by some programmers somewhere, so they should be intuitive for them.</p>"},{"location":"CSE112_CO/2..ISA/2..instruction_set_architecture/#generic","title":"Generic","text":"<p>Instructions such as a hypothetical <code>add14</code> are unnecessarily specialised, and bloat space in the instruction set without providing enough functionality to justify their addition.</p>"},{"location":"CSE112_CO/2..ISA/3..RISC_and_CISC_paradigms/","title":"RISC And CISC Paradigms","text":""},{"location":"CSE112_CO/2..ISA/3..RISC_and_CISC_paradigms/#risc","title":"RISC","text":"<p>RISC stands for Reduced Instruction Set Computer. In RISC, instructions are simple and the instruction set has a small size. Examples include ARM and MIPS.</p>"},{"location":"CSE112_CO/2..ISA/3..RISC_and_CISC_paradigms/#cisc","title":"CISC","text":"<p>CISC stands for Complex Instruction Set Computer. In CISC, instructions are complex, have irregular formats, more operands, and complex functionalities. The instruction set has a large size. Examples include x86 and VAX</p>"},{"location":"CSE112_CO/3..assembly_language/1..addressing_modes/","title":"Addressing Modes","text":"<p>Addressing Modes are ways to get operands from the memory / register files.</p> <ul> <li><code>Register</code> - ADD R4, R3, R2</li> <li><code>Immediate</code> - ADD R4, R3, #5</li> <li><code>Register Indirect</code> - ADD R4, R3, (R1)</li> <li><code>Displacement</code> - ADD R4, R3, 100(R1): fetches value in the memory located at an offset of R1 from 100.</li> <li><code>Memory Indirect</code> - ADD R4, R3, @(R1): fetches value in the memory address located at {value present in R1 address}.</li> <li><code>Absolute</code> - ADD R4, R3, (0x475)</li> <li><code>PC-Relative</code> - ADD R4, R3, 100(PC)</li> <li><code>Scaled</code> - ADD R4, R3, 100(R1)[R5]: fetches value in the memory present R5 strides and R1 offset from 100.</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/2..calling_convention/","title":"Calling Convention","text":""},{"location":"CSE112_CO/3..assembly_language/2..calling_convention/#issues-with-function-calling","title":"Issues with Function-Calling","text":"<ul> <li>Storing the value of each register in each function call takes more space than what we have.</li> <li>Registers can be overwritten after calling a function. The function caller may not know which registers it uses!</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/2..calling_convention/#naive-solutions","title":"Naive Solutions","text":"<ul> <li>The caller pushes all registers that it (caller) uses on the stack before calling a function, and pops them after the function returns.</li> <li>The callee pushes all registers that it (callee) uses on the stack at the start of a function, and pops them before returning.</li> <li>If there is no overlap between registers used by the caller and the callee, using any of these will be pointless.</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/2..calling_convention/#register-spilling-in-practice","title":"Register Spilling in Practice","text":"<p>It was found that having an equal amount of designated caller and callee saved registers gives the best performance. Caller and callee saving is a convention, which means it isn't enforced by the language. However, it is best practice to follow it.</p>"},{"location":"CSE112_CO/3..assembly_language/2..calling_convention/#calling-convention_1","title":"Calling Convention","text":"<p>The calling convention tells us a lot of things about the ISA, including</p> <ul> <li>which registers are used to send function parameters</li> <li>which registers are used to return value from function</li> <li>which registers are caller-saved</li> <li>which registers are callee-saved</li> <li>which register is the stack pointer</li> </ul> <p>Understand that a Calling Convention must be Universal to an ISA. This is because lots of different programmers will try using the same language. If you import a library written with a different calling conventions, it will break your code completely. </p>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/","title":"Instruction Categories","text":"<p>Not all of these instructions will be present in an ISA, in the exact given format. This is to give a general idea as to what kinds of instructions you will run into.</p>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#arithmetic-operations","title":"Arithmetic Operations","text":"<ul> <li>add </li> <li>sub</li> <li>mul</li> <li>div</li> <li>mod</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#operation-formats","title":"Operation Formats","text":"<p>Operations can be performed between two registers, or a register and an immediate. This result will be stored in a destination register.</p>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#logical-operations","title":"Logical Operations","text":"<ul> <li>and</li> <li>or</li> <li>not</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#operation-formats_1","title":"Operation Formats","text":"<p><code>and</code> and <code>or</code> are typically performed only between two registers. This result is stored in a destination register. <code>not</code> accepts only one source, and stores its negation in the destination register.</p>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#branching-operations","title":"Branching Operations","text":"<ul> <li><code>bz (RS) (addr)</code> - branches to address (addr) if register (RS) is storing value 0.</li> <li><code>bnz (RS) (addr)</code> - branches to address (addr) if register (RS) is not storing value 0.</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#alternative-branching-approaches","title":"Alternative Branching Approaches","text":"<ul> <li><code>be, blt, bgt</code> - compare the value of two registers, and branch depending on them.</li> <li><code>cmp</code> - sets the value of flags, and depending on these flags the next instruction may branch somewhere.</li> </ul> <p>An ISA will typically use one of these approaches to branch</p>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#function-operations","title":"Function Operations","text":"<ul> <li><code>call (lbl)</code> - calls the function referred to with label.</li> <li><code>ret</code> - returns from the current function</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#data-transfer-persistence","title":"Data Transfer / Persistence","text":"<ul> <li><code>ld (RD) (addr)</code> - loads value from memory address (addr) to register (RD)</li> <li><code>st (RS) (addr)</code> - stores value in register (RS) to memory address (addr)</li> <li><code>mov (RD) (RS / imm)</code> - stores value from either a source register (RS) or an immediate (imm) to a destination register (RD)</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#stack-operations","title":"Stack Operations","text":"<ul> <li><code>push (RS)</code> - pushes value of (RS) on the stack</li> <li><code>pop (RD)</code> - pops the value at the top of stack into (RD)</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/3..instruction_categories/#io-instructions","title":"IO Instructions","text":"<ul> <li><code>input (RD)</code> - inputs a value into a register (RD)</li> <li><code>output (RS)</code> - outputs the value of register (RS)</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/4..instruction_encoding/","title":"Instruction Encoding","text":"<p>Encoding is the process by which the assembler converts human-readable assembly code to machine-readable machine code. Machine Code exists in binary formatting, and is interpreted directly by the electronics in the processor.</p>"},{"location":"CSE112_CO/3..assembly_language/4..instruction_encoding/#encoding-formats","title":"Encoding Formats","text":"<p>Each instruction is encoded in some format, which gives the processor all the info needed. This encoding can contain:</p> <ul> <li>Opcode (mandatory) - tells the processor which instruction to run on this line.</li> <li>Source / Destination Registers - may be read from or written into.</li> <li>Source / Destination Memory Addresses - as used by load / store instructions. Memory addresses can also be used to specify where to branch.</li> <li>Modifiers - apply certain changes to the instruction's execution.</li> </ul> <p>The number of bits allocated for each address decides the size of each address space (i.e. the range of possible immediates, memory addresses that can be accessed, and registers that can be accessed.)</p>"},{"location":"CSE112_CO/3..assembly_language/4..instruction_encoding/#features","title":"Features","text":"<ul> <li>We want all instruction encodings to be equal in size for a certain ISA. For this, we add padding bits to the end if needed.</li> <li>All encoded instructions are read from the memory during program execution.</li> </ul>"},{"location":"CSE112_CO/3..assembly_language/4..instruction_encoding/#impact-of-encoding-width","title":"Impact of Encoding Width","text":"<p><code>Fixed Width</code> ISAs occupy more space in the memory (and thus cache), but are decoded faster. </p> <p><code>Variable Width</code> ISAs occupy less space in the memory (and thus cache), but are decoded slower.</p> <p>A difference of execution speed between the two is indeterminate unless other factors are provided.</p>"},{"location":"CSE112_CO/3..assembly_language/5..the_stack/","title":"The Stack","text":""},{"location":"CSE112_CO/3..assembly_language/5..the_stack/#stack-memory","title":"Stack Memory","text":"<p>A Stack is a data structure which should hopefully need no explanation. But why do we use Stacks for managing memory related to functions, as opposed to a Queue or a List?</p> <p>Calling functions inside functions behaves like a stack. The last function to be called is the first to be returned. This is why we model them as a stack.</p>"},{"location":"CSE112_CO/3..assembly_language/5..the_stack/#what-is-stored-on-the-stack","title":"What is stored on the Stack?","text":"<p>The stack stores the addresses that the function is supposed to return to. Additionally, it can store register values that are spilt by the caller / callee. The caller-saved registers are pushed first, then the return address, then the callee-saved registers. They are popped in the reverse order.</p> <p>Stacks can be either increasing or decreasing. Increasing stacks go to higher memory addresses when values are pushed, with decreasing stacks doing the opposite. </p>"},{"location":"CSE112_CO/3..assembly_language/5..the_stack/#stack-pointer","title":"Stack Pointer","text":"<p>The stack pointer is a register that stores the current head of the stack. The register used for this is decided by the ISA.</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/","title":"Integer Representation","text":""},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#why-binary","title":"Why Binary?","text":"<p>Storing in base 3, or higher, would allow us to compress more numbers in less electronic components, but it means each digit has more possible values, leading to a higher noise margin.</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#unsigned-integer","title":"Unsigned Integer","text":"<p>\\((100)_{10} = (0110\\_0100)_{2u}\\)</p> <p>\\((019)_{10} = (0001\\_0011)_{2u}\\)</p> <ul> <li>Conversion from binary to decimal can be done via polynomial expansion.</li> <li>Conversion from decimal to binary can be done via repeated division.</li> </ul> <p>Range of Representation - with \\(n\\) bits, we can represent integers in the range \\([0, 2^n)\\). This can be verified by the cardinality of this set being \\(2^n\\).</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#sign-magnitude","title":"Sign Magnitude","text":"<p>\\((-5)_{10} = -(101)_{2u} = ([1]101)_{2s}\\)</p> <p>\\((+5)_{10} = +(101)_{2u} = ([0]101)_{2s}\\)</p> <ul> <li>Conversion from binary to decimal and vice versa can be done by treating the non-sign bits as an unsigned integer, and then applying the sign.</li> </ul> <p>Range of Representation - with \\(n\\) bits, we can represent integers in the range \\((-2^{n-1}, 2^{n-1})\\). The cardinality of this set is \\(2^n-1\\), as \\((0)_{10}\\) may be represented as either positive or negative.</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#twos-complement","title":"Two's Complement","text":"<p>\\((-5)_{10} = (1011)_{2t} = (1111\\_1011)_{2t}\\)</p> <p>Interestingly, \\((1011)_{2u} = (11)_{10} = 16 - 5\\) </p> <p>The two in two's complement comes from us taking the complement, which when added with the unsigned number gives us \\(2^n\\). </p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#note-1","title":"Note 1","text":"<p>Due to the nature of the Two's Complement Notation, the first bit of the number also tells us what its sign is, similar to a sign bit.</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#note-2","title":"Note 2","text":"<p>Suffixes attached to the base are indicative of the encoding that they follow. These are attached for understanding purposes, and are not needed in your own working. \\(X_{2t}\\) signifies that \\(X\\) only has the intended meaning when we interpret it in binary two's complement notation. </p> <p>This also shows that a binary number, by itself, does not equate to a decimal number. We need to know the encoding process to figure it out. Decoding the same binary nubmer assuming two different encodings can lead to different results.</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#subtraction-via-the-twos-complement-method","title":"Subtraction via the Two's Complement Method","text":"<p>A great video on this topic</p> <p>The video explains how we can perform two's complement addition in base 10, which helps build a cleaner idea of what it does in base 2.</p>"},{"location":"CSE112_CO/4..number_representation/1..integer_representation/#range-calculation","title":"Range Calculation","text":"<p>Suppose you want to calculate the possible range of some operation between an \\(m\\) bit and an \\(n\\) integer in some \\(R\\) representation. </p> <ul> <li>Calculate the range of the two starting numbers in decimal notation.</li> <li>Calculate the range of the operation in decimal notation.</li> <li>Convert this range back to \\(R\\) representation, and find the number of bits needed. While doing this, the \\(log\\) function's result is rounded up.</li> </ul>"},{"location":"CSE112_CO/4..number_representation/2..real_number_representation/","title":"Real Number Representation","text":""},{"location":"CSE112_CO/4..number_representation/2..real_number_representation/#fixed-point","title":"Fixed Point","text":"<p>It's a known fact that we can't store decimals in a computer. Therefore, we'd like a function that maps from \\(R \\to Z\\). Consider the numbers 41.50, 2.3, and 11.503. Is there a consistent function that would map all of them to integers? Well yes! \\(f(x) = 1000x\\) gets that job done. We can impose a fixed number of digits after decimal point, and convert all numbers to hit that (either by appending 0s or truncating). </p> <p>\\(41.50 = 41500 * 10^{-3}\\) </p> <p>Hold on, this looks quite a lot like the IEEE 754 representation up ahead right? However, in this representation, \\(-3\\) is fixed, and shared by all numbers belonging to this datatype. This is why we call the position of the decimal point as fixed.</p>"},{"location":"CSE112_CO/4..number_representation/2..real_number_representation/#floating-point","title":"Floating Point","text":"<p>Contrary to this, floating point representation does not make such an assumption. In this, the decimal point is allowed to float.</p> <p> </p> <p>Images courtesy of GeeksForGeeks.</p> <p>Call the mantissa M, exponent E, and sign S</p> <p>\\(N = -S * 2^{E-127} * (1+M)\\)</p> <p>for single precision floating point numbers.</p>"},{"location":"CSE112_CO/4..number_representation/2..real_number_representation/#edge-cases","title":"Edge Cases","text":"S E M Result X 0 0 0 + 255 0 +Inf - 255 0 -Inf X 255 !0 NAN X 0 !0 Denormal <p>X means that the sign is irrelevant.</p>"},{"location":"CSE112_CO/4..number_representation/3..overflows_and_underflows/","title":"Overflows And Underflows","text":""},{"location":"CSE112_CO/4..number_representation/3..overflows_and_underflows/#overflow-underflow-detection-in-twos-complement","title":"Overflow / Underflow Detection in Two's Complement","text":"<p>+ve + +ve = -ve =&gt; Overflow Detected</p> <p>-ve + -ve = +ve =&gt; Undeflow Detected</p> <p>During assessments, an underflow may be referred to as overflow as well.</p> <p>But what about one positive and one negative integer? These cannot give any form of overflow, as the range of their addition cannot exceed beyond the domain (they're opposing each other).</p> <p>We can use this to define the overflow check. Consider \\(a_k\\) and \\(b_k\\) as MSBs, and \\(s_{k-1}\\) and \\(s_k\\) as carry overs before and after their addition. An overflow is observed if,</p>"},{"location":"CSE112_CO/4..number_representation/3..overflows_and_underflows/#expressions-to-check-for-overflow-underflow","title":"Expressions to check for Overflow / Underflow","text":"<ul> <li>\\(C_i \\oplus C_j\\)</li> <li>\\(a_{k-1} * b_{k-1} * \\overline{s_{k-1}} + \\overline{a_{k-1}} * \\overline{b_{k-1}} * s_{k-1}\\)</li> </ul>"},{"location":"CSE112_CO/4..number_representation/3..overflows_and_underflows/#what-if-overflow","title":"What if Overflow?","text":"<ul> <li>Raise an error or a flag. The user can handle this however they want. The FLAGS register in RISC langauges that we'll work in does this.</li> <li>Wrap around. This allows the number to 'fit' within the required space, but can lead to unexpected behaviour. This sounds bad, but Java does this.</li> <li>Cast to a \"larger\" variable type. Python does this.</li> <li>Do whatever the hell you want! It's not your responsibility if there's an overflow, it's the programmer's. C does this.</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/1..cache_loading/","title":"Cache Loading","text":""},{"location":"CSE112_CO/5..cache_and_memory/1..cache_loading/#loading","title":"Loading","text":"<p>Loading is the process by which a requested value is fetched from the memory via the cache.</p> <ul> <li>A load request is issued by the processor. This memory block may or may not be present in the cache</li> <li>If it is present, this is referred to as a cache hit. If it is not, then it is called a cache miss.</li> <li>If there's a cache hit, the value is fetched directly from the cache.</li> <li>If there's a cache miss, the blcok is loaded into the cache, and the value is fetched from there.</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/1..cache_loading/#cache-misses","title":"Cache Misses","text":"<ul> <li>Compulsory Misses - misses that occur the first time that a memory block is loaded</li> <li>Capacity Misses - misses that occur because the cache is too small, and would be unavoidable even with an ideal replacement policy.</li> <li>Conflict Misses - misses that would've been avoidable in a situation with complete associativity</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/1..cache_loading/#improving-loading-time","title":"Improving Loading Time","text":"<ul> <li>A small, less associative cache leads to faster retrieval from cache hits</li> <li>A larger, more associative cache leads to less cache misses</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/2..write_policy/","title":"Write Policy","text":"<p>The write policy dictates how information is written into the cache and memory.</p>"},{"location":"CSE112_CO/5..cache_and_memory/2..write_policy/#hit-policy","title":"Hit Policy","text":"<ul> <li>Write Through - value is written to both the cache and the memory</li> <li>Write Back - value is written only to the cache. However, when the block is evicted it is written to the memory.</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/2..write_policy/#miss-policy","title":"Miss Policy","text":"<ul> <li>No Write Allocate - value is written only into the memory</li> <li>Write Allocate - the block is brought into the cache after writing to the memory</li> </ul> <p>We commonly use either write through with no write allocate, or write back with write allocate</p>"},{"location":"CSE112_CO/5..cache_and_memory/3..cache_associativity/","title":"Cache Associativity","text":""},{"location":"CSE112_CO/5..cache_and_memory/3..cache_associativity/#associativity","title":"Associativity","text":"<p>Associativity dictates how a block of memory may be loaded into a cache slot. Cache policies in ascending order of associativity are.</p>"},{"location":"CSE112_CO/5..cache_and_memory/3..cache_associativity/#direct-mapped","title":"Direct Mapped","text":"<p>One block can only fit in one cache slot</p>"},{"location":"CSE112_CO/5..cache_and_memory/3..cache_associativity/#n-way-set-associative","title":"n-way Set Associative","text":"<p>A block can fit in one of n cache slots</p>"},{"location":"CSE112_CO/5..cache_and_memory/3..cache_associativity/#fully-associative","title":"Fully Associative","text":"<p>Any block can fit in any cache slot</p>"},{"location":"CSE112_CO/5..cache_and_memory/3..cache_associativity/#effect-of-associativity","title":"Effect of Associativity","text":"<p>The more associative a cache is:</p> <ul> <li>The more time it takes to search a block within the cache.</li> <li>The less often blocks need to be replaced within the cache.</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/4..cache_structure/","title":"Cache Structure","text":"<p>1) Cache 2) Cache Set - an n-way associative set with n cache blocks in it 3) Cache Block - contains k lines in it 4) Cache Line - contains l bytes in it</p> <p>Consider a 32-bit machine with a set-associative cache. The total cache size is 16KB, and it is a 4-way set-associative cache. The length of the memory location is 1 byte, and the block size is 16 Bytes. Each memory address is 32 bit long (assume).</p> <p>In this scenario, the word size is 4B. Each set has 4 blocks or 64B in it.  Each cache block has 4 lines in it. </p> <p>The cache has 2^8 sets in it. Representing a cache set requires 8 bits. Each cache block has 16 bytes in it. Representing a byte requires 4 bits.</p> <p>Therefore, we have an addressing system that looks as such</p> <pre><code>|---------TAG----------|--INDEX--|--OFFSET--|\n32                    12         4          0\n</code></pre> <p>First, we look for the cache set with the matching index. Then, we locate the block with the matching tag (assuming we have a cache hit). Then, we pick the address at the given offset in the block.</p>"},{"location":"CSE112_CO/5..cache_and_memory/5..cache_timing/","title":"Cache Timing","text":""},{"location":"CSE112_CO/5..cache_and_memory/5..cache_timing/#average-access-time","title":"Average Access Time","text":"\\[Average Mem Access Time = t_{hit} + r_{miss} * penalty_{miss} \\]"},{"location":"CSE112_CO/5..cache_and_memory/5..cache_timing/#improve-hit-time","title":"Improve Hit Time","text":"<ul> <li>The smaller the cache, the better the hit time.</li> <li>The lower the associativity, the better the hit time.</li> </ul> <p>However, these will greatly increase the miss rate.</p>"},{"location":"CSE112_CO/5..cache_and_memory/5..cache_timing/#reduce-miss-rate","title":"Reduce Miss Rate","text":"<ul> <li>The larger the block size, the lower the miss rate. Having a very big block makes movement of data too expensive to be worth though. Leads to unnecessary data movement during miss. <ul> <li>The block size is determined more so by the data access pattern than optimising for the miss rate.</li> </ul> </li> <li>Increasing cache size to X2 reduces miss rate by root 2 (empirical rule)</li> <li>Direct-mapped cache of size N has same miss rate as 2-way associative cache of size N/2</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/6..locality/","title":"Locality","text":""},{"location":"CSE112_CO/5..cache_and_memory/6..locality/#temporal-locality","title":"Temporal Locality","text":"<p>Temporal locality is observed when the same resource is fetched repetitively over time. This can be seen in a loop iterator, which is accessed after each iteration.</p>"},{"location":"CSE112_CO/5..cache_and_memory/6..locality/#spatial-locality","title":"Spatial Locality","text":"<p>Spacial locality is observed when close-by memory locations are accessed in succession. This is seen while iterating over an array.</p> <p>The purpose of a cache is to take advantage of both of these localities.</p>"},{"location":"CSE112_CO/5..cache_and_memory/7..replacement_policy/","title":"Replacement Policy","text":"<p>Caches have finite size. When they run out, one block of memory has to be replaced with another (this process is referred to as eviction). Eviction priorities are decided by the cache's replacement policy.</p> <ul> <li>Random - A random cache block is evicted</li> <li>Least Recently Used (LRU) - Suffers from an overhead of maintaining last usage time, thus only preferred for smaller caches</li> <li>Psuedo LRU - only tracks least recently used block</li> <li>FIFO - used in caches of high size</li> <li>Not Most Recently Used (NMRU) - any previous strategy, except with the added rule that the most recently used cache block can't be evicted</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/8..memory/","title":"Memory modes sorted by Bit Density (Ascending)","text":""},{"location":"CSE112_CO/5..cache_and_memory/8..memory/#register","title":"Register","text":"<p>A register is a set of D-flip flops, which stores a binary value.</p>"},{"location":"CSE112_CO/5..cache_and_memory/8..memory/#sram","title":"SRAM","text":"<p>Holds value with two diodes and two capacitors</p>"},{"location":"CSE112_CO/5..cache_and_memory/8..memory/#dram","title":"DRAM","text":"<p>Dynamic memory, runs on the basis of a pair of capacitors</p>"},{"location":"CSE112_CO/5..cache_and_memory/8..memory/#effect-of-bit-density","title":"Effect of Bit Density","text":"<p>The lower the bit density</p> <ul> <li>The lower the memory capacity</li> <li>The lower the memory latency</li> <li>The higher the memory bandwith</li> </ul>"},{"location":"CSE112_CO/5..cache_and_memory/8..memory/#the-memory-hierarchy","title":"The Memory Hierarchy","text":"<p>Processor -&gt; Small Fast SRAM (Cache) -&gt; Large Slow Memory (DRAM)</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/1..pipelining/","title":"Pipelining","text":"<p>In a pipelined processor, different steps of instruction processing are assigned to different \"phases\". Each of these \"phases\" are isolated, which allows for having multiple instructions in different phases at the same time. </p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/1..pipelining/#why-pipeline","title":"Why Pipeline","text":"<p>Pipelined processors are significantly faster than non-pipelined ones, if you can handle the complications they bring along with them properly.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/1..pipelining/#phases-of-the-classic-risc-pipeline","title":"Phases of the Classic RISC Pipeline","text":"<ul> <li>Fetch (F) - In this phase, the instruction is loaded in from the memory.</li> <li>Decode (D) - The instruction's individual parts are decoded in this phase, such as the opcode, the sources, and destinations. The source values are loaded from memory / registers during this phase as well.</li> <li>Execute (E/X) - These source values are sent to the ALU for processing, if needed.</li> <li>Memory (M) - Values that need to be written into the memory are written here.</li> <li>Writeback (W) - Values that need to be written into registers are written here.</li> </ul>"},{"location":"CSE112_CO/6..pipelining_and_hazards/1..pipelining/#pipeline-diagram","title":"Pipeline Diagram","text":"<p>In a pipeline diagram, we note down the state of the machine on each cycle. Each cycle is a column, and each row represents an instruction</p> <pre><code>add R1 R2 R3\nadd R1 R2 R3\nadd R1 R2 R3\nadd R1 R2 R3\nadd R1 R2 R3\n</code></pre> <p>has the pipeline diagram</p> <pre><code>   1  2  3  4  5  6  7  8  9\n1  F  D  X  M  W\n2     F  D  X  M  W\n3        F  D  X  M  W\n4           F  D  X  M  W\n4              F  D  X  M  W\n</code></pre> <p>Note that on one cycle, one phase of the pipeline may only contain one instruction in it. in an \\(n\\) phase pipelined processor, the maximum number of instructions that can be executed simultaneously in that processor is \\(n\\)</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/2..pipeline_diagram_timing/","title":"Pipeline Diagram Timing","text":""},{"location":"CSE112_CO/6..pipelining_and_hazards/2..pipeline_diagram_timing/#timing-the-pipeline-diagram","title":"Timing the Pipeline Diagram","text":"<p>While correcting papers, I saw many strange ways of measuring the time taken for a pipelined program to execute. These aren't silly mistakes, but fundamental conceptual misconceptions.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/2..pipeline_diagram_timing/#pipeline-diagrams-go-left-to-right","title":"Pipeline Diagrams go Left to Right","text":"<p>This may be obvious, but a lot of people don't consider this during their calculations</p> <pre><code>t = 1 cycle\n   1  2  3  4\n1  F\n2\n3\n\nt = 2 cycles\n   1  2  3  4\n1  F  D\n2     F\n3\n\nt = 3 cycles\n   1  2  3  4\n1  F  D  X\n2     F  D\n3        F\n\nand so on\n</code></pre>"},{"location":"CSE112_CO/6..pipelining_and_hazards/2..pipeline_diagram_timing/#each-phase-takes-the-same-time","title":"Each Phase takes the Same Time","text":"<p>The minimum time that could be taken by a phase is not the amount of time that it actually takes in execution. Think about students at mess. The waiting for Corn Flakes is typically much less than that of Omelettes. Yet, the line proceeds at the slowest pace, which is that of the Omelette. </p> <p>Just like that, each cycle of a processor takes the time equal to the longest phase. If F finishes before D, it still has to wait for D to finish before sending the data into that phase, as there cannot be two instructions in the same phase.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/2..pipeline_diagram_timing/#total-time-formula","title":"Total Time Formula","text":"\\[\\begin{align}  &amp;t_{cycle} = max(t_F, t_D, t_X, t_M, t_W)\\\\ &amp;t = t_{cycle} \\times n_{cycles}\\ \\end{align}\\] <p>There may be some other overheads in practical processors, but we will not consider them for this course.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/","title":"Iron Law Of Processor Performance","text":""},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#iron-law-of-processor-performance_1","title":"Iron Law of Processor Performance","text":"<p>$$ \\frac{Time}{Program} = \\frac{Instructions}{Program} \\times \\frac{Cycles}{Instruction} \\times \\frac{Time}{Cycle} $$ The speed of two processors is compared with the Iron Law.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#parameters-of-the-iron-law","title":"Parameters of the Iron Law","text":"<ul> <li>Time / Cycle - The time taken for the processor to finish one processing cycle. This depends completely on the hardware. </li> <li>Cycles / Instruction - The number of cycles taken on average to execute an instruction. Note that while not mentioned here, the CPI is also measured relative to the benchmark program. The CPI can vary from program-to-program (A program with a lot of stalling will end up with higher CPI than a program with no stalling). </li> <li>Instruction / Program - The number of instructions present in the benchmark that we are comparing the processors on. </li> <li>Instructions / Second - This is just a combination of two factors. We measure performance often in MIPS (Millions of Instructions per Second).</li> </ul>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#benchmark","title":"Benchmark","text":"<p>Benchmarks are programs that simulate the use case of the processor. They're supposed to evaluate how quickly it'll be able to perform in practice. They all contain code that performs a certain task in a certain way. An example might be calculating the factorial of a number recursively. The code will vary between ISAs, but the purpose will be the same. Of course, benchmarks are far more complex than this in practice.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#what-affects-which-parameter","title":"What affects which parameter?","text":""},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#risc-vs-cisc","title":"RISC vs CISC","text":"<p>RISC Processors have lower CPI than CISC as they use simpler instructions, but end up with higher Instructions in the benchmark. This makes an objective comparison in the speed of RISC and CISC ISAs impossible {which is what you lost marks over in Quiz 2 ;-) }</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#pipelining","title":"Pipelining","text":"<p>A non-pipelined processor executes one instruction in one clock cycle, which combines all the five phases. This clock cycle is longer than one individual phase, but typically a bit shorter than all the phases combined. Meanwhile a pipelined processor has lower Time / Cycle with higher CPI. While an objective comparison between the two isn't possible, a good pipelined processor will typically perform faster.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#processor","title":"Processor","text":"<p>A faster processor means lower Time / Cycle. This makes the processor work faster (shocking). A 5GHz processor will be always faster than a 2GHz processor running the same ISA. </p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/3..iron_law_of_processor_performance/#branch-delay-penalty","title":"Branch Delay Penalty","text":"<p>Code that involves a lot of branching can lead to a lot of killed instructions. This causes what is referred to as a Branch Delay Penalty. One can avoid this by using less branching, or (preferably) a sophisticated branch predictor.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/4..structural_and_control_hazards/","title":"Structural And Control Hazards","text":""},{"location":"CSE112_CO/6..pipelining_and_hazards/4..structural_and_control_hazards/#structural-hazard","title":"Structural Hazard","text":"<p>Structural hazards occur when two instructions need the same hardware resource at the same time. One such example is trying to read from and write into a memory location simultaneously. This is typically not possible, but there are workarounds for it. </p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/4..structural_and_control_hazards/#control-hazard","title":"Control Hazard","text":"<p>Control hazards occur when you reach any branching instruction. The issue is that we don't immediately know which instruction to go to next.</p> <pre><code>[this is a part of a larger program]\n...\n1) bgt lbl R5 R6\n2) add R1 R2 R3\n3) add R1 R2 R3\n4) add R1 R2 R3\n5) add R1 R2 R3\n6) lbl: add R3 R2 R1\n...\n</code></pre> <pre><code>   1  2\n1  F  D\n</code></pre> <p>On cycle 2, you'd prefer to have another instruction entering the fetch phase, but you don't know which one. </p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/4..structural_and_control_hazards/#resolution-of-control-hazards","title":"Resolution of Control Hazards","text":""},{"location":"CSE112_CO/6..pipelining_and_hazards/4..structural_and_control_hazards/#pipeline-stalling","title":"Pipeline Stalling","text":"<p>Do nothing at all till the branch outcome is resolved. This is inefficient regardless of which outcome is achieved.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/4..structural_and_control_hazards/#speculate","title":"Speculate","text":"<p>Your machine makes an educated guess on whether or not this code would lead to branching. For the sake of simplicity, we assume that the educated guess is always <code>No Branch</code>. Then, it speculatively starts executing the next instruction. Suppose branches are resolved at Decode in this ISA / machine.</p> <pre><code>   1  2  3 | 4  5  6  7  8 \n1  F  D  X | M  W\n2     F  D | X  M  W\n3        F | D  X  M  W\n6          | F  D  X  M  W\n</code></pre> <p>After cycle 3, we decide to perform branching and set PC to 6. This guess has been incorrect. However, leaving this execution as is will lead to the unintended modification of R1 caused by running instructions 2 and 3, which were never supposed to be executed.</p> <p>To resolve this, we stuff instruction 2 and 3 with nops.</p> <pre><code>   1  2  3 | 4  5  6  7  8 \n1  F  D  X | M  W\n2     F  D | -  -  -\n3        F | -  -  -  -\n6          | F  D  X  M  W\n</code></pre>"},{"location":"CSE112_CO/6..pipelining_and_hazards/5..dependencies_and_data_hazards/","title":"Dependencies And Data Hazards","text":""},{"location":"CSE112_CO/6..pipelining_and_hazards/5..dependencies_and_data_hazards/#dependency","title":"Dependency","text":"<p>A dependency is observed between two instructions when an instruction wants to use the value of a resource after it's modified by another instruction.</p> <pre><code>[Assume that this is executed without pipelining]\n1) add R1 R2 R3\n2) add R2 R1 R3\n</code></pre> <p>In the code above, line 2 wishes to use the value of R1 to calculate the final result. This isn't a problem, because the value of R1 will be updated much before instruction 2 even starts processing. </p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/5..dependencies_and_data_hazards/#example","title":"Example","text":"<p>Think of your assignment. You were waiting for the TAs to upload the new test-cases so that you can finalise the code. If they had uploaded them in May, that would've been great. They would've made the test-cases, and you would've tested your code afterwards.</p> <p>Think of the test-cases GitHub repository as a resource here. The TAs write into it, and you read from it. This is a read-after-write dependency. A resource in a computer can be many things, but for us the relevant resources are <code>memory locations</code> and <code>registers</code>.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/5..dependencies_and_data_hazards/#data-hazards","title":"Data Hazards","text":"<p>Hazards occur in pipelined processors when the processor starts executing the next instruction before the current one is finished. If there's a dependency between these instructions, then the value read by the new instruction may be \"stale\". This risk of passing of a stale value is called a read-after-write hazard.</p> <p>There are also write-after-write and write-after-read hazards, but those are not relevant to us.</p> <p>A data hazard is a consequence of a dependency. Not all dependencies cause data hazards, but all data hazards are caused by dependencies.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/5..dependencies_and_data_hazards/#example_1","title":"Example","text":"<p>Coming back to our example from earlier, both you and the TAs were working towards the project. This work was going on in parallel. If we finished writing the test cases before you were done with your code, that's great. No reason to worry, you can just test your code and submit. But if we released them after you were done, there's a problem.</p> <p>You might end up testing on only the older cases, which only cover the Assembler. This is problematic, because you're relying on stale information (test-cases that are outdated for the main deadline). Surely, you would try to avoid this unintended outcome! But how?</p> <p>Likewise, if our code were pipelined, we would have a problem. When instruction 2 is in <code>D</code> phase, instruction 1 is in <code>E</code> phase. There are two more cycles till the stale value in the resource (R1) becomes fresh. We don't want to work with this stale value, so we'll do something else to resolve the Data Hazard.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/","title":"Data Hazard Resolution","text":"<p>Data Hazards in processors are majorly solved in one of two ways.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/#stall","title":"Stall","text":"<p>A Stall policy involves waiting for the desired resource to be fresh. The dependent instruction is 'Stalled' till this condition is satisfied. </p> <pre><code>1) add R1 R2 R3\n2) add R2 R1 R3\n</code></pre>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/#example","title":"Example","text":"<p>You can think of this as waiting till the TAs upload test cases to submit your code.</p> <pre><code>   1  2  3  4  5\n1  F  D  X    \n2     F  D\n</code></pre> <p>Instruction 2 wants the fresh value of R1, but knows that it won't be achieved till cycle 5. So it waits till cycle 5, looping in the decode phase, doing nothing.</p> <pre><code>   1  2  3  4  5\n1  F  D  X  M  W\n2     F  D  D  D\n</code></pre> <p>Depending upon the processor implementation, it may or may not be able to fetch the fresh value on the same cycle that it is written to. For my notes, I will assume that it cannot. Now that the fresh value of R1 has been stored, Instruction 2 can read it and continue processing</p> <pre><code>   1  2  3  4  5  6  7  8  9\n1  F  D  X  M  W  \n2     F  D  D  D  D  X  M  W\n</code></pre> <p>While Instruction 2 is stalling in the Decode phase, Instruction 3 would be stalling in the Fetch phase, and Instruction 4 will not execute at all.</p> <pre><code>   1  2  3  4  5  6  7\n1  F  D  X  M  W  \n2     F  D  D  D  D  X\n3        F  F  F  F  D\n4                    F\n</code></pre> <p>Instruction 3 is halted on Fetch just because Instruction 2 is already in decode. One stage of the processor can only handle one instruction at a time.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/#bypass","title":"Bypass","text":"<p>A Bypass policy involves passing a value from one phase of the pipeline to the next. This can be complicated in practice, so we don't cover it in much detail.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/#example_1","title":"Example","text":"<p>You can think of this as me passing you test-cases in advance before they're uploaded, which I would unfortunately never do.</p> <pre><code>   1  2  3  4  5\n1  F  D  X  M  W\n2     F  D  D  D\n</code></pre> <p>Consider this pipeline diagram, resulting from a stall policy. You may have noticed that we wait two cycles, even though the value of R1 was already confirmed at the end of Cycle 3. </p> <p>In the bypass approach, we put a \"bypass\" line that will pass the value of R1 from <code>X</code>  to <code>D</code> at the end of cycle 3. This way, the value can immediately be read on Cycle 4, eliminating the slow-down caused by stalling.</p> <pre><code>   1  2  3  4  5  6  7\n1  F  D  X  M  W\n          `-,[bypassed]\n2     F  D  D  X  M  W\n</code></pre>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/#scheduling","title":"Scheduling","text":"<p>Scheduling involves requiring the user to arrange their code such as to avoid RAW errors. This involves the usage of <code>nop</code> instructions.</p>"},{"location":"CSE112_CO/6..pipelining_and_hazards/6..data_hazard_resolution/#speculation","title":"Speculation","text":"<p>Speculation involves looking ahead a few instructions, and running them out of order so as to avoid RAW hazards.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/1..OOO_specs/","title":"OOO Specs","text":""},{"location":"CSE112_CO/7..out_of_order_processors/1..OOO_specs/#i4","title":"I4","text":"<ul> <li>Everything is inorder</li> <li>Uses a fixed length pipelines scoreboard</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/1..OOO_specs/#i2o2","title":"I2O2","text":"<ul> <li>Frontend and Issue are inorder</li> <li>Writeback and Commit is out-of-order</li> <li>Uses a scoreboard</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/1..OOO_specs/#i2oi","title":"I2OI","text":"<ul> <li>Frontend and Issue are inorder</li> <li>Writeback is out-of-order</li> <li>Commit is inorder</li> <li>Uses a scoreboard, reorder buffer, and store buffer</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/1..OOO_specs/#io3","title":"IO3","text":"<ul> <li>Frontend is inorder</li> <li>Issue, Writeback, and Commit are out-of-order</li> <li>Uses a scoreboard and an issue queue</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/1..OOO_specs/#io2i","title":"IO2I","text":"<ul> <li>Frontend is inorder</li> <li>Issue and Writeback are out-of-order</li> <li>Uses a scoreboard, an issue queue, a reorder buffer, and a store buffer</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/2..I4/","title":"I4","text":""},{"location":"CSE112_CO/7..out_of_order_processors/2..I4/#pipeline-structure","title":"Pipeline Structure","text":"<p>There are three functional units here: * ALU - X * Memory - M * Multiplication - Y</p>"},{"location":"CSE112_CO/7..out_of_order_processors/2..I4/#scoreboard","title":"Scoreboard","text":"<p>A scoreboard is used for issuing instructions. The architectural register files are where the information is read from / written to.</p> R P F 4 3 2 1 0 R1 1 X 0 0 1 0 0 R2 1 Y 0 1 0 0 0 <p>P =&gt; Pending F =&gt; The functional unit the register is assigned to</p> <p>The numbers indicate how far the register is from being available (i.e. the writeback stage)</p> <p>Bits in the data availability shift right once after every cycle.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/2..I4/#pipeline-diagram","title":"Pipeline Diagram","text":"<pre><code>0 MUL R1, R2, R3    F  D  I  Y0 Y1 Y2 Y3 W \n1 ADDIU R11, R10, 1    F  D  I  X0 X1 X2 X3 W \n2 MUL R5, R1, R4          F  D  I  I  I  Y0 Y1 Y2 Y3 W \n3 MUL R7, R5, R6             F  D  D  D  I  I  I  I  Y0 Y1 Y2 Y3 W \n4 ADDIU R12,R11,1               F  F  F  D  D  D  D  I  X0 X1 X2 X3 W \n5 ADDIU R13,R12,1                        F  F  F  F  D  I  X0 X1 X2 X3 W \n6 ADDIU R14,R12,2                                    F  D  I  X0 X1 X2 X3 W\n</code></pre> <ul> <li>Stalling is done on the issue stage</li> <li>Bypass is done from writeback for instructions in Y unit</li> <li>Bypass is done from X0 for instructions in X unit</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/3..I2O2/","title":"I2O2","text":""},{"location":"CSE112_CO/7..out_of_order_processors/3..I2O2/#pipeline-structure","title":"Pipeline Structure","text":"<p>Similar to I4, but ditches the constant pipeline length.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/3..I2O2/#scoreboard","title":"Scoreboard","text":"<p>Uses scoreboard in the same way as I4, but also uses it to track structural hazards on the writeback port.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/3..I2O2/#pipeline-diagram","title":"Pipeline Diagram","text":"<pre><code>0 MUL R1, R2, R3    F  D  I  Y0 Y1 Y2 Y3 W \n1 ADDIU R11, R10, 1    F  D  I  X0 W \n2 MUL R5, R1, R4          F  D  I  I  I  Y0 Y1 Y2 Y3 W \n3 MUL R7, R5, R6             F  D  D  D  I  I  I  I  Y0 Y1 Y2 Y3 W \n4 ADDIU R12,R11,1               F  F  F  D  D  D  D  I  X0 W \n5 ADDIU R13,R12,1                        F  F  F  F  D  I  X0 W \n6 ADDIU R14,R12,2                                    F  D  I  I X0 W\n</code></pre> <ul> <li>Stalling is done on the issue stage to avoid structural hazard on writeback</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/","title":"I2OI","text":""},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#pipeline-structure","title":"Pipeline Structure","text":""},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#scoreboard","title":"Scoreboard","text":"<p>I2OI uses a scoreboard for avoiding hazards while issuing instructions inorder.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#reorder-buffer-rob","title":"Reorder Buffer (ROB)","text":"State S ST V PReg P 1 0 0 0 F 0 1 1 1 <ul> <li>State - Pending or Free / Finished</li> <li>S - Speculative</li> <li>ST - Store bit</li> <li>V - whether the physical register file is valid</li> <li>PReg - the physical register file specifier</li> </ul> <p>The Reorder Buffer is a queue with a head and a tail. The commit stage waits for the head to be finished.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#finished-store-buffer-fsb","title":"Finished Store Buffer (FSB)","text":"V Op Addr Data 0 ? ? ? <ul> <li>Only one entry is needed if one memory instruction is in flight at a time</li> <li>Single entry FSB makes allocation trivial.</li> <li>If we want to support more than one memory instruction, we need to worry about load / store address aliasing.</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#pipeline-diagram","title":"Pipeline Diagram","text":"<ul> <li>r shows when a value is queued onto the reorder buffer</li> <li>C is when the new value is committed</li> <li>C and r can happen in the same cycle</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#branch-resolution","title":"Branch Resolution","text":"<ul> <li>Squash on branch resolution - more complex, especially for ROB</li> <li>Squash on branch commit</li> <li>Squash as instructions commit - more resources used, simplest</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/4..I2OI/#store-misses","title":"Store Misses","text":"<p>Store misses add delays of ~3 cycles which is kinda bad. You can use retire stages to conduct them asynchronously while further instructions run (but you should be wary of any related hazards)</p>"},{"location":"CSE112_CO/7..out_of_order_processors/5..IO3/","title":"IO3","text":""},{"location":"CSE112_CO/7..out_of_order_processors/5..IO3/#pipeline-structure","title":"Pipeline Structure","text":""},{"location":"CSE112_CO/7..out_of_order_processors/5..IO3/#issue-queue","title":"Issue Queue","text":"Opc Imm S V Dest V P Src0 V P Src1 <ul> <li>P - Pending</li> <li>S - Speculative</li> <li>V - Valid (needs to be checked for availability)</li> <li>P - Pending (for operands to be producted)</li> </ul> <p>Instruction is ready when \\((!V_{src0}\\ ||\\ !P_{src0}) \\ \\&amp;\\&amp; \\ (!V_{src1}\\ || \\ !P_{src1})\\)</p> <pre><code>0 MUL R1, R2, R3   F   D   I   Y0  Y1  Y2  Y3  W \n1 ADDIU R11,R10,1      F   D   I   X0  W \n2 MUL R5, R1, R4           F   D   i       I   Y0  Y1  Y2  Y3  W \n3 MUL R7, R5, R6               F   D   i                   I   Y0 Y1 Y2 Y3 W \n4 ADDIU R12,R11,1                  F   D   i   I   X0  W \n5 ADDIU R13,R12,1                      F   D   i   I   X0  W \n6 ADDIU R14,R12,2                          F   D   i   I   X0  W\n</code></pre> <ul> <li>Two instructions can't be issued simultaneously (structural hazard)</li> </ul>"},{"location":"CSE112_CO/7..out_of_order_processors/6..IO2I/","title":"IO2I","text":"<p>Ok so basically use everything so far.</p>"},{"location":"CSE112_CO/7..out_of_order_processors/6..IO2I/#pipeline-structure","title":"Pipeline Structure","text":""},{"location":"CSE112_CO/7..out_of_order_processors/6..IO2I/#pipeline-diagram","title":"Pipeline Diagram","text":""},{"location":"CSE112_CO/8..superscalar_processors/1..superscalar_pipeline/","title":"Superscalar Pipeline","text":"<ul> <li>Register Files need four read ports, as two simultaneous instructions perform two reads each.</li> <li>Two ALUs are used.</li> <li>Only the bottom pipeline can access the memory. However, both pipelines have the <code>M</code> stage.</li> </ul>"},{"location":"CSE112_CO/8..superscalar_processors/1..superscalar_pipeline/#pipeline-diagram","title":"Pipeline Diagram","text":"<pre><code>ADD   F   D   A0  A1  W\nLD        F   D   B0  B1  W\nLD            F   D   B0  B1  W\nADD               F   D   A0  A1  W\n</code></pre>"},{"location":"CSE112_CO/8..superscalar_processors/2..issue_stage/","title":"Issue Stage","text":"<p>Making the bypass network for a two-way superscalar processor is really complex. This network adds a significant overhead to the decode stage. For this reason, the decode stage is split into two:</p> <ul> <li>Decode - Parse the instruction, resolve structural hazards</li> <li>Issue - Read from register files, conduct bypasses, steer the instructions to the proper functional unit</li> </ul>"},{"location":"CSE112_CO/8..superscalar_processors/3..hazards/","title":"Hazards","text":""},{"location":"CSE112_CO/8..superscalar_processors/3..hazards/#structural-hazard","title":"Structural Hazard","text":"<pre><code>LW    F   D   B0  B1  W\nLW    F   D   (D) B0  B1  W\n</code></pre> <p>As both the instructions are simultaneously asking for the load pipeline, a structural hazard is observed. This is resolved by stalling till the pipeline is freed.</p>"},{"location":"CSE112_CO/8..superscalar_processors/3..hazards/#data-hazard","title":"Data Hazard","text":"<pre><code>ADDIU R5, R6, 1    F   D   B0  B1  W\nADDIU R7, R5, 1    F   D   (D  D   D) A0  A1  W\n</code></pre> <p>The second instruction depends on the first. So stalling is done until the value of R5 is resolved.</p>"},{"location":"CSE112_CO/8..superscalar_processors/4..alignment_constraints/","title":"Alignment Constraints","text":""},{"location":"CSE112_CO/8..superscalar_processors/4..alignment_constraints/#cache-lines","title":"Cache Lines","text":"<p>One cache block is placed in one cache line. Fetching instructions from two cache lines in the same cycle is very hard. Normally, we can only fetch two instructions on the same cache line. So, if two instructions are on different cache lines, the superscalar processor will fetch a nop.</p>"},{"location":"CSE112_CO/8..superscalar_processors/4..alignment_constraints/#execution-obeying-alignment-constraint","title":"Execution Obeying Alignment Constraint","text":"<pre><code>Addr     Instr\n0x000    I0\n0x004    J 0x204\n0x200    -\n0x204    I1\n0x208    I2\n0x20C    I3\n\n       0    4    8    C\n0x00X  | I0 | J  |    |    | \n0x20X  |    | I1 | I2 | I3 |\n0x21X  |    |    |    |    |\n</code></pre>"},{"location":"CSE112_CO/9..exceptions/1..interrupt/","title":"Interrupt","text":"<p>An interrupt is something which alters the regular flow of control of the program. It needs to be processed by some code which lies out of the original program, and is unexpected / rare from the program's point of view.</p>"},{"location":"CSE112_CO/9..exceptions/1..interrupt/#causes","title":"Causes","text":""},{"location":"CSE112_CO/9..exceptions/1..interrupt/#asynchronous-external","title":"Asynchronous / External","text":"<ul> <li>An IO device sends a signal</li> <li>Timer interruption</li> <li>Power disruption or any other hardware failure</li> </ul>"},{"location":"CSE112_CO/9..exceptions/1..interrupt/#synchronous-internal","title":"Synchronous / Internal","text":"<ul> <li>An undefined opcode</li> <li>Arithmetic overflow / FPU exception</li> <li>Virtual memory exception</li> <li>Software / syscall exception</li> </ul>"},{"location":"CSE112_CO/9..exceptions/2..interrupt_handling/","title":"Interrupt Handling","text":""},{"location":"CSE112_CO/9..exceptions/2..interrupt_handling/#precise-interrupt","title":"Precise Interrupt","text":"<p>A precise interrupt maintains the state before the interrupt before handling the exception.</p> <p>Any I/O device that requests attention uses a prioritised interrupt request line.</p> <p>The interrupt handler saves the Exception Program Counter (EPC) to allow nested interrupts. </p> <p>The status register indicates the cause of the interrupt. </p> <p>A special jump instruction RFE (return from exception) is used to resume code. It * enables interrupts * returns the processor to the user mode * restores the hardware status and control state</p>"},{"location":"CSE202_DBMS/","title":"Index","text":"<p>notes for CSE 202 DBMS</p>"},{"location":"CSE202_DBMS/1..introduction/1..why_use_DBMS/","title":"Why Use DBMS","text":"<p>Uses of Database Applications include:</p> <ul> <li>Banking - to store metadata about customers, like their accounts, loans, and transactions</li> <li>FInance - for documenting and predicting sales and purchases of financial commodities</li> <li>Airlines - to keep track of seat reservations, flight schedules, airport information, etc.</li> <li>Telecommunication - to keep records of call, text, and data usage; and generate bills with this information</li> <li>Navigation - to keep track of locations of relevant destinations, along with data on modes of transport (i.e. roadways, railways, bus routes)</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/2..data_definition_language/","title":"Data Definition Language","text":"<p>DDL is the notation used for creating the database schema</p> <p>Example: <pre><code>CREATE TABLE Customers (\n    ID char(5),\n    name varchar(20),\n    city varchar(20),\n    balance numeric(8,2)\n);\n</code></pre></p>"},{"location":"CSE202_DBMS/1..introduction/2..data_definition_language/#the-data-dictionary","title":"The Data Dictionary","text":"<ul> <li>The DDL compiler stores a set of table templates in a data dictionary</li> <li>The data dictionary contains metadata including:<ul> <li>Database schemas</li> <li>Integrity constraints, notably the primary key</li> <li>Authorisation details</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/1..introduction/3..data_manipulation_language/","title":"Data Manipulation Language","text":"<p>Data Manipulation languages come in two flavours</p>"},{"location":"CSE202_DBMS/1..introduction/3..data_manipulation_language/#procedural-dml","title":"Procedural DML","text":"<p>Procedural DMLs require a user to specify * What data is needed * How the DBMS should go about acquiring this data</p>"},{"location":"CSE202_DBMS/1..introduction/3..data_manipulation_language/#declarative-dml","title":"Declarative DML","text":"<p>Declarative DMLs require a user to specify * What data is needed * But not how that data should be acquired</p> <p>Example: <pre><code>SELECT name\nFROM Customers\nWHERE city = 'Agra' AND balance &gt; 100000\n</code></pre></p> <ul> <li>Often, complex SQL functions are embedded in some high-level language</li> <li>Application problems can access SQL via either<ul> <li>Language extensions that allow embedded SQL within the language</li> <li>APIs (like python's <code>sqlite3</code>) that allow SQL queries to be sent to a database</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/1..introduction/4..database_users/","title":"Database Users","text":""},{"location":"CSE202_DBMS/1..introduction/4..database_users/#application-users","title":"Application Users","text":"<p>These will access the application program directly, with no knowledge that a database even exists. The application program will fire a query on their behalf, and use its result to give a desired output</p>"},{"location":"CSE202_DBMS/1..introduction/4..database_users/#application-programmers","title":"Application Programmers","text":"<p>These will write application programs, that are used by the users. These programs are reduced by a compiler and linker to program code, and on the SQL side they are reduced to DML queries. The program code is managed by whatever language the application is written in, and the DML code is handled by SQL</p>"},{"location":"CSE202_DBMS/1..introduction/4..database_users/#data-analysts","title":"Data Analysts","text":"<p>Data analysts will use DML queries directly. These are processed by the query processor, which gives them the raw output in an SQL table format</p>"},{"location":"CSE202_DBMS/1..introduction/4..database_users/#database-administrators","title":"Database Administrators","text":"<p>A DBA manages the database. They have many responsibilities, notably, * Defining schemas * Defining the storage structure, access-methods, and physical organisation * Checking the database for anomalies routinely * Backing up the database routinely * Managing and granting authorisation for database access * Monitoring any jobs running on the database * Ensuring that the database has enough disk space to work with </p>"},{"location":"CSE202_DBMS/1..introduction/5..data_models/","title":"Data Models","text":""},{"location":"CSE202_DBMS/1..introduction/5..data_models/#data-model","title":"Data Model","text":"<p>A data model is a collection of tools used to articulate: * Data entities * Relationships between data entities * The semantics of the data system * The constraints to which the data entities are subjected</p>"},{"location":"CSE202_DBMS/1..introduction/5..data_models/#types-of-data-models","title":"Types of Data Models","text":"<ul> <li>Entity-Relationship data model (primarily for database design only)</li> <li>Relational Model</li> <li>Object-based data models (may be object-oriented or object-relational)</li> <li>Semi-structured data model (XML / YAML / JSON)</li> <li>Archaic models, such as:<ul> <li>Network model</li> <li>Hierarchichal model</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/","title":"Relational Model","text":"<p>All data is stored in tables</p>"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#example","title":"Example","text":"ID name department salary 100 John Physics 100000 125 Doe Chemistry 98000 235 Dane Biology 90000 175 Joe Mathematics 100001"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#layers-of-abstraction","title":"Layers of Abstraction","text":""},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#application-level","title":"Application Level","text":"<ul> <li>This level is beyond the scope of management of the DBMS, but relies on it</li> <li>The application converts data from the DBMS into a human-readable format, which conceals everything irrelevant to the user</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#view-level","title":"View Level","text":"<ul> <li>A view is modified, regulated, and pre-digested access to a certain section of the database</li> <li>A view can perform data hiding for security purposes, and also hide details of datatypes</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#logical-level","title":"Logical Level","text":"<ul> <li>This level describes the nature of the data shown in the database</li> <li>It also regulates the relationships between the data entities</li> <li>Lastly, it tells us about the schema, i.e. the constraints that the data columns are subject to</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#physical-level","title":"Physical Level","text":"<ul> <li>The physical level decides how data is stored on the disk</li> <li>This is, understandably, something that a data analyst doesn't care much about. This decides the efficiency and effectiveness of operations and storage, but doesn't affect the behaviour of the DBMS</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/6..relational_model/#data-independence","title":"Data Independence","text":"<p>Data independence is the ability of a lower layer of abstraction to be independent of a higher layer of abstraction. This is a very helpful paradigm, even besides the scope of database management, it is essential to the working of almost any computer program</p> <p>Logical Data Independence - is the ability to modify the logical schema without needing to rewrite application programs</p> <p>Physical Data Independence - is the ability to modify the physical schema without having to rewrite the logical schema</p>"},{"location":"CSE202_DBMS/1..introduction/7..database_applications/","title":"Database Applications","text":"<p>Databse applications will either be two-tiered or three-tiered</p> <p></p>"},{"location":"CSE202_DBMS/1..introduction/7..database_applications/#two-tiered-architecture","title":"Two-Tiered Architecture","text":"<p>In this format, the application resides within the client's machine. To provide a feature, it sends a query directly to the database system, which is located inside the server machine.</p>"},{"location":"CSE202_DBMS/1..introduction/7..database_applications/#three-tiered-architecture","title":"Three-Tiered Architecture","text":"<p>In this format, the client machine does not send any direct calls to the database. Rather, the application server acts as a mediator between the client program and the DBMS</p>"},{"location":"CSE202_DBMS/1..introduction/8..database_engine/","title":"Database Engine","text":"<p>The job of the database engine is distributed between a handful of components</p> <pre>39bbb7b6-1bdc-4999-9a9b-7acb220eaa6a</pre>"},{"location":"CSE202_DBMS/1..introduction/8..database_engine/#storage-manager","title":"Storage Manager","text":"<p>The storage manager provides an interface that connects low-level data inside the database, to application programs that submit queries to the DBMS</p> <p>The storage manager is responsible for: * Exchanging data with the OS file manager * Efficient storage, retrieval, and updation of data * Effective implementation of data files, data dictionary, and used indices</p>"},{"location":"CSE202_DBMS/1..introduction/8..database_engine/#query-processor","title":"Query Processor","text":"<p>The query processor parses and translates the query given to the DBMS. It converts it to a relational algebra expression, which is passed to the processing engine</p>"},{"location":"CSE202_DBMS/1..introduction/8..database_engine/#transaction-manager","title":"Transaction Manager","text":"<p>A logical function is broken down into a collection of operations. The result of this function is returned by performing these operations and combining their results. This collection of operations is referred to as a Transaction. The component responsible for regulating transations is called the Transaction Manager</p> <p>It also maintains the database's consistency in case of a system failure (power failure / OS crash) or a transaction failure</p>"},{"location":"CSE202_DBMS/1..introduction/8..database_engine/#concurrency-control-manager","title":"Concurrency-Control Manager","text":"<p>The concurrency-control manager ensures that concurrent transactions don't lead to race conditions, and thus preserves the consistency of the database</p>"},{"location":"CSE202_DBMS/1..introduction/9..database_architecture/","title":"Database Architecture","text":"<p>The database architecture decides the way memory and disk space is utilised by the database, and can also affect how applications interact with it</p>"},{"location":"CSE202_DBMS/1..introduction/9..database_architecture/#centralised-database","title":"Centralised Database","text":"<ul> <li>Contains a small number of cores, executing over a shared memory</li> <li>A database running on a personal computer is a good example of this</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/9..database_architecture/#server-style-database","title":"Server-Style Database","text":"<ul> <li>The database is located on a server. It performs work on the behalf of several client machines</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/9..database_architecture/#parallel-database","title":"Parallel Database","text":"<ul> <li>Involves a large number of cores working over a shared memory</li> <li>Disk space is also shared between the cores</li> </ul>"},{"location":"CSE202_DBMS/1..introduction/9..database_architecture/#distributed-database","title":"Distributed Database","text":"<ul> <li>Involves multiple machines, which may be running at different geographic locations</li> <li>There will inevitably be heterogeneity in the schema and data. The database needs to compensate for this</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/1..introduction/","title":"Introduction","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/1..introduction/#volatility","title":"Volatility","text":"<ul> <li>Volatile - Volatile data is lost when the power of the device is switched off</li> <li>Non-Volatile - non-volatile information persists even after the device is switched off</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/1..introduction/#storage-factors","title":"Storage Factors","text":"<ul> <li>Speed of data access</li> <li>Cost of data storage</li> <li>Reliability of data retrieval</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/1..introduction/#storage-hierarchy","title":"Storage Hierarchy","text":"<ul> <li>Primary - storage is the fastest. It is also volatile. Primary data sources include the cache and the main memory.</li> <li>Secondary - storage includes non-volatile on-line storage. It includes flash memory and magnetic (HDD) discs.</li> <li>Tertiary - storage is off-line and used for archival. It is non-volatile. It has a slow access time. It includes magnetic tapes and optical storage.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/1..introduction/#interaction-interfaces","title":"Interaction Interfaces","text":"<ul> <li>SATA - Serial ATA - supports data transfer of up to 6 Gb/s</li> <li>SAS - Serial Attached SCSI - supports upto 12 Gb/s</li> <li>NVMe - Non-Volatile Memory Express - supports up to 24 Gb/s</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/1..introduction/#storage-interfaces","title":"Storage Interfaces","text":"<ul> <li>SAN - Storage Area Network - a large number of disks connected to servers by a high-speed network.</li> <li>NAS - Network Attached Storage - a networked storage that provides a file using networked file system protocol rather than a disc interface.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/10..RAID/","title":"RAID","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/10..RAID/#redundant-arrays-of-independent-disks","title":"Redundant Arrays of Independent Disks","text":"<ul> <li>RAID is a disk organisation technique that manages many redundant disks to store the same piece of information.</li> <li>Having multiple disks in parallel gives higher read speed.</li> <li>Having redundancy gives high reliability, allowing data to be recovered even when a disk fails.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/10..RAID/#mirroring-technique","title":"Mirroring Technique","text":"<ul> <li>In this technique, each disk is duplicated. A logical disk has two physical disks. </li> <li>Each write is carried out on both disks, each read is carried out on either disk.</li> <li>If a disk fails, we restore from the other disk. Theoretically both disks can fail simultaneously, but the probability of that is very small.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/10..RAID/#mean-time-to-repair","title":"Mean Time to Repair","text":"<ul> <li>MTR is the average amount of time required to repair a failed disk.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/10..RAID/#mean-time-to-data-loss","title":"Mean Time to Data Loss","text":"<ul> <li>MTTDL is the amount of time required for one block of data to be unretrievable. </li> <li>\\(MTTDL = MTTF \\cdot MTR \\cdot disk\\_count\\) </li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/11..RAID_level_6/","title":"RAID Level 6","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/11..RAID_level_6/#p-q-redundancy-scheme","title":"P + Q Redundancy Scheme","text":"<p>Similar to level 5, but instead stores two correction blocks \\((P, Q)\\). This protects from multiple simultaneous disk failures.</p> D1 D2 D3 D4 P0 0 1 P1 P2 P3 2 3 4 P4 P5 5 6 7 P6 P7 <p>This increases the single parity block to guard against multiple disk failures. It gives better reliability than RAID 5</p>"},{"location":"CSE202_DBMS/10..storage_-_hardware/12..parallelism_in_RAID/","title":"Parallelism In RAID","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/12..parallelism_in_RAID/#bit-level-striping","title":"Bit-Level Striping","text":"<ul> <li>Splits the bits of each byte into multiple disks. </li> <li>For \\(8\\) disks, each bit \\(8n+i\\) is written to the \\(i^{th}\\) disk.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/12..parallelism_in_RAID/#block-level-striping","title":"Block-Level Striping","text":"<ul> <li>For \\(n\\) disks, block \\(i\\) goes to the disk \\([(i\\, \\% \\,n)+ 1]\\).</li> <li>In this way, requests for different blocks can run in parallel, which will speed up a request for a large file.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/2..hard_disks/","title":"Hard Disks","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/2..hard_disks/#decomposition","title":"Decomposition","text":"<ul> <li>An HDD contains lots of platters. A spindle typically has 1~5 platters on it.</li> <li>Each platter has one read-write head.</li> <li>Each platter has 50~100k tracks in it</li> <li>Each track has sectors in it. Inner tracks have 500~1000 sectors while outer tracks have 1000~2000 sectors</li> <li>A block is the smallest unit of storage allocation. It is typically 4~16 kB in size.</li> <li>A cylinder \\(i\\) contain's the \\(i^{th}\\) track of each platter </li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/2..hard_disks/#read-write","title":"Read-Write","text":"<ul> <li>The read/write head needs to be above the correct sector to read from it. </li> <li>The disk arm swings to the correct position to accomplish this.</li> <li>Meanwhile the disk spins continuously. Eventually, the track head will be in the correct position.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/","title":"Disk Performance","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#access-time","title":"Access Time","text":"<p>Access time is the amount of time between the start of a request and the start of data transfer. It contains * Seek Time - time taken to move the arm over the correct track. Typically 4~10 ms. * Rotational Latency - time taken for the sector to be accessed by the head. Typically 4~11 ms.</p>"},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#data-transfer-rate","title":"Data Transfer Rate","text":"<p>The rate at which data is retrieved from or stored to the disk. Typically 25~200 MBps, slower for inner tracks.</p>"},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#access-patterns","title":"Access Patterns","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#sequential-access-pattern","title":"Sequential Access Pattern","text":"<ul> <li>In this process, data is accessed in successive disk blocks on the same track. This means that disk seek is only performed for the first block.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#random-access-pattern","title":"Random Access Pattern","text":"<ul> <li>In this process, successive requests can be anywhere on the disk. So, each request needs to perform its own seek.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#iops","title":"IOPS","text":"<ul> <li>IOPS is the number of random block reads supported per second. It is 50~200 on current Magnetic Disks.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/3..disk_performance/#mttf","title":"MTTF","text":"<ul> <li>Mean Time To Failure is the expected time that the disk will run theoretically without any failure. It's typically 3~5 years.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/4..RAID_level_0/","title":"RAID Level 0","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/4..RAID_level_0/#non-redundant-block-striping","title":"Non-redundant Block Striping","text":"<ul> <li>This is used in high-performance applications.</li> <li>It is essential that data loss is not critical for this application.</li> <li>Block striping is performed, but no mirroring is done.</li> <li>This is the baseline of RAID, and the most unsafe storage method for data.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/5..RAID_level_1/","title":"RAID Level 1","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/5..RAID_level_1/#block-striping-mirrored-disks","title":"Block Striping, Mirrored Disks","text":"<ul> <li>This arrangement is common for log files in a database system. It's preferred for applications with many random and small updates.</li> <li>In this arrangement, each disk is mirrored once. This is same as the mirroring discussed earlier.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/6..RAID_level_5/","title":"RAID Level 5","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/6..RAID_level_5/#block-interleaved-distributed-parity","title":"Block-Interleaved, Distributed Parity","text":"<ul> <li>In this scheme, we make parity blocks. Parity block \\(j\\) stores the \\(XOR\\) of bits from block \\(j\\) of each disk.</li> </ul> <p>Given 5 Disks</p> D1 D2 D3 D4 D5 P0 0 1 2 3 4 P1 5 6 7 8 9 P2 10 11 12 13 14 P3 15 16 17 18 19 P4 <p>Since block writes occur in parallel, the writing isn't slowed down. The \\(n^{th}\\)  parity block stores the information about the \\(n^{th}\\) row in this table. If one disk is corrupted, we can restore its information from the parity block.</p>"},{"location":"CSE202_DBMS/10..storage_-_hardware/7..choice_of_RAID_level/","title":"Choice Of RAID Level","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/7..choice_of_RAID_level/#factors","title":"Factors","text":"<ul> <li>Cost </li> <li>Performance</li> <li>Performance during failure</li> <li>Performance during rebuild of failed disk </li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/7..choice_of_RAID_level/#level-1-vs-5","title":"Level 1 vs. 5","text":"<ul> <li>Level 1 has better write speed</li> <li>Level 1 has higher storage cost</li> <li>Level 5 is preferrable for massive applications</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/7..choice_of_RAID_level/#level-6-vs-5","title":"Level 6 vs. 5","text":"<ul> <li>Level 6 is better than 5 because it can tolerate two disk failures, but takes more budget</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/8..optimisations/","title":"Optimisations","text":""},{"location":"CSE202_DBMS/10..storage_-_hardware/8..optimisations/#optimisation-of-disk-block-access","title":"Optimisation of Disk-Block Access","text":"<ul> <li>Buffering - A buffer is an in-memory buffer that contains a cache of disk blocks.</li> <li>Read-ahead - looks at blocks ahead in anticipation in case the querier asks for subsequent blocks.</li> <li>Disk-Arm Scheduling - the algorithm reorders the block requests so as to optimise the speed of queries as a whole, via the elevator algorithm.</li> <li>File Oragnisation - involves allocating blocks of a file contiguously as much as possible. Sometimes files are fragmented. To deal with this, some systmes have defragmentation utilities.</li> </ul>"},{"location":"CSE202_DBMS/10..storage_-_hardware/9..disk_controller/","title":"Disk Controller","text":"<ul> <li>A disk controller is the interface between the computer system and the disk drive hardware. </li> <li>When a device wants to read from or write to a sector, it gives that command to the controller.</li> <li>The controller does the job, then attaches checksums to ensure that the data isn't corrupted.</li> <li>If data is corrupted, then the checksum is nearly guaranteed to fail. This ensures that the data is stored reliably.</li> <li>It also performs remapping of bad sectors.</li> </ul>"},{"location":"CSE202_DBMS/11..complex_datatypes/1..semi-structured_data/","title":"Semi-structured Data","text":""},{"location":"CSE202_DBMS/11..complex_datatypes/1..semi-structured_data/#the-need","title":"The Need","text":"<p>Many applications require storage of complex data, where the schema may change often. In many situations, the relational model's requirement of atomic datatypes can be overkill. Storing multi-valued attributes as a set is often simpler than normalising them. Data exchange can benefit from this kind of semi-structured data. It allows us to conveniently pass a chunk of information between the back and front ends.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/1..semi-structured_data/#flexibility","title":"Flexibility","text":"<ul> <li>Wide column representation allows each tuple to have its own set of attributes</li> <li>Sparse column representation has a fixed superset, but tuples may omit certain attributes</li> </ul>"},{"location":"CSE202_DBMS/11..complex_datatypes/1..semi-structured_data/#multi-valued-datatypes","title":"Multi-Valued Datatypes","text":"<ul> <li>Sets / Multisets - <code>{'A', 'B', 'C'}</code></li> <li>Key-value Maps - <code>{('A','X'), ('B','Y'), ('C','Z')</code></li> <li>Arrays - <code>[5, 8, 9, 11]</code></li> </ul> <p>These are modeled with a non first normal form (NFNF) data model. In some cases, a database may provide specialised support for arrays. Examples include GeoRaster, PostGIS, and SciDB.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/2..JSON/","title":"JSON","text":"<p>JSON is one of the most commonly-used formats for aggregating data. To insert a JSON field in an SQL table we use</p> <pre><code>CREATE TABLE &lt;name&gt;(\n    ...\n    &lt;parameter_name&gt; JSON\n    ...\n)\n</code></pre>"},{"location":"CSE202_DBMS/11..complex_datatypes/2..JSON/#example","title":"Example","text":"<pre><code>INSERT INTO Doctor (Did, Dname, Degree) VALUES\n(1234, 'Jai Kishan', \"Degree\":[\n    {\"First_Degree\": \"MS\", \"Dept\": \"Chemistry\"}\n    {\"Second_Degree\": \"MCH\", \"Dept\": \"Physics\"}\n])\n\nSELECT Did, Degree-&gt;First_Degree as Degree FROM Doctor \n</code></pre> <p>will yield (1234, MS)</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/3..XML/","title":"XML","text":"<p>XML uses markup tags like HTML does to categorise text. Tags make teh data self-documenting, and may be hierarchical.</p> <p>Note that MySQL doesn't support XML while IBM DB2, Microsoft SQL Server, or PostgreSQL do. In MySQL, it is stored as a CLOB type.</p> <p>XQuery was developed to query nested XML structures, but it's not widely used currently.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/4..resource_description_format/","title":"Resource Description Format","text":""},{"location":"CSE202_DBMS/11..complex_datatypes/4..resource_description_format/#rdf","title":"RDF","text":"<p>The RDF represents facts and information as a network of sorts. Each object has some connection with other object, and each connection is written down as a triple.</p> <p>(subject, predicate, object) is the format of RDF. One example of this is (New_Delhi, capital_of, India)</p> <p>Models can have attributes and relationships with other objects, but all these connections are modelled as triples.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/4..resource_description_format/#sparql","title":"SPARQL","text":"<p>SPARQL is a language used to query RDF template data. A SPARQL query looks like</p> <pre><code>SELECT ?name\nWHERE {\n    ?cid title \"Introduction to Computer Science\"\n    ?sid course ?cid\n    ?id takes ?sid\n    ?id name ?name\n}\n</code></pre> <p>It also supports aggregation, optional joins, subqueries, and more. It offers transitive closure on paths as well.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/4..resource_description_format/#n-ary-relationships","title":"n-ary Relationships","text":"<p>While a single RDF triple represents a triple, we can use it to represent more. One option is to create a connector node with binary relationships to erlevant nodes.</p> <p>(Barrack Obama, president_of, USA, from, 2008-2016) can be broken down into</p> <p>(e1, represents, Barrack Obama) (e1, president_of, USA) (e1, president_between, 2008-2016)</p> <p>Or it can be broken down as (Barrack Obama, president_of, USA, c1) (c1, president_between, 2008-2016)</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/4..resource_description_format/#usage","title":"Usage","text":"<p>RDF is frequently used to represent interconnected knowledge bases, like DBPedia, Yago, Freebase, or WikiData. Linked open data project aims to connect different knowledge graphs.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/5..object-oriented_database/","title":"Object-oriented Database","text":""},{"location":"CSE202_DBMS/11..complex_datatypes/5..object-oriented_database/#object-oriented-databases","title":"Object-Oriented Databases","text":"<p>Object Orientation gives us an object-relational data model. This has complex data types, and many beneficial features of OOP. Using SQL data directly in OOP is problematic, because you have to switch between SQL and the base language. To solve this, we can build an object-relational database, which adds OOP features to a relational database.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/5..object-oriented_database/#creating-objects","title":"Creating \"Objects\"","text":""},{"location":"CSE202_DBMS/11..complex_datatypes/5..object-oriented_database/#user-defined-types","title":"User-Defined Types","text":"<pre><code>CREATE TYPE person\n    (ID varchar(20) primary key,\n    name varchar(20),\n    address varchar(100)) ref from(ID);\nCREATE TABLE people OF person\n</code></pre>"},{"location":"CSE202_DBMS/11..complex_datatypes/5..object-oriented_database/#table-types","title":"Table Types","text":"<pre><code>CREATE TYPE interest as TABLE(\n    topic varchar(20),\n    amount int\n);\nCREATE TABLE users (\n    ID varchar(20),\n    name varchar(20),\n    interests interest\n);\n</code></pre>"},{"location":"CSE202_DBMS/11..complex_datatypes/5..object-oriented_database/#inheritance","title":"Inheritance","text":"<p>In MySQL, we can use the <code>UNDER</code> keyword to inherit (<code>&lt;child type&gt; UNDER &lt;parent type&gt;</code>) is the used syntax here. PostgreS and OracleDB use the <code>INHERITS</code> keyword to implement inheritance between tables as well.</p> <pre><code>CREATE TYPE student UNDER person\n(degree varchar(20));\n</code></pre>"},{"location":"CSE202_DBMS/11..complex_datatypes/6..textual_data/","title":"Textual Data","text":"<p>Information retrieval refers to querying on unstructured data. Stuff like keyword queries, document relevance, and mental mapping come under this.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/6..textual_data/#tf-idf-ranking","title":"TF-IDF ranking","text":"<ul> <li>Term Frequency (TF) - ranks the number of times a term \\(t\\) appears in the document \\(d\\). It's typically defined as \\(\\log(TF(t, d)+1)\\) as a damping factor.</li> <li>Inverse Document Frequency (IDF) - IDF is defined as the ratio of the total number of documents in a collection that \\(t\\) appears in. It's typically defined as \\(\\log(N/N_d)\\) as a damping factor.</li> </ul>"},{"location":"CSE202_DBMS/11..complex_datatypes/6..textual_data/#relevance-ranking","title":"Relevance Ranking","text":"<p>\\(\\(r(d, Q) = \\sum_{t \\in Q} TF(t, d) * IDF(t)\\)\\) Other definitions take proximity of words into account, and ignore stop words.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/7..hyperlink_ranking/","title":"Hyperlink Ranking","text":"<p>Hyperlink ranking focuses the relevance of webpages by the quantity and quality of hyperlinks that point to them. * Pages hyperlinked by many pages have higher rank * Pages hyperlinked by higher ranked pages have higher rank * This is formalised by a random walk model * This definition is circular, but can be solved with linear equations</p> <p>Other approaches include keywords in anchor text, and click-through rate of hyperlink when provided to users.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/","title":"Spatial Data","text":"<p>Spatial databases store information related to spatial locations, and support efficient storage, indexing, and querying of said data.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/#geographic-data","title":"Geographic Data","text":"<ul> <li>Road maps, land-usage maps, topographic maps, etc. come under this category. Geographic information systems are special-purpose databases made to store this data. </li> </ul>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/#geometric-data","title":"Geometric Data","text":"<ul> <li>Stores information about how objects are constructe. A point is identified by its co-ordinates, a line by n co-ordinates, and so on. </li> </ul>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/#design-data","title":"Design Data","text":"<p>Like geometric data, design data also stores information about something geometric. It may also store other data, like how two objects interact with each other (union / intersection).</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/#raster-data","title":"Raster Data","text":"<p>Raster data contains bitamps of n dimensions. These are typically not stored in design databases.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/#vector-data","title":"Vector Data","text":"<p>Vector data is constructed from basic geometrical objects, like points, line segments, polygons, polyhedra, etc. These are typically used to store designs.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/8..spatial_data/#spatial-queries","title":"Spatial Queries","text":"<p>Spatial queries can ask questions about * Nearest objects * Nearest neighbours satisfying a condition * Graph-related algorithms * Spatial Joins * Questions, such as if region A contains region B</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/9..hash-join/","title":"Hash-join","text":""},{"location":"CSE202_DBMS/11..complex_datatypes/9..hash-join/#hash-join_1","title":"Hash-Join","text":"<p>Given two tables, if one table has a hash-index already, bring that index table to the main memory if possible. Then perform Indexed nested-loop join.</p> <p>The optimiser may generate hash-tables automatically. This is significantly fast if the hash-table fits completely into the memory.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/9..hash-join/#pseudo-code","title":"Pseudo-Code","text":"<p>Pull hashed index \\(H(I)\\) into the memory. For each requested record ID \\(P\\),     put \\(P\\) into the hash function to get \\(H(P)\\).     lookup \\(H(P)\\) in \\(H(I)\\).     over all records \\(\\{R_i\\}\\) in \\(H(I)\\),         if the indices also match,             select the record.</p>"},{"location":"CSE202_DBMS/11..complex_datatypes/9..hash-join/#large-table","title":"Large Table","text":"<p>If the table is large, partition it into smaller tables, by using a hashing function \\(h\\), different from the hashing code for the indices.</p> <p>Generate partitioned hash indices for tables \\(R\\) and \\(S\\). </p> <p>For each partition \\((B_i, B_j)\\) in \\(R, S\\),     Load \\((B_i, B_j)\\) into memory.     For each searched record \\(P\\) in \\(R\\),         Lookup tuples in \\(R\\) with the same hash as \\(P\\),             If the predicate \\(\\theta\\) is satisfied,                 Select the tuple.     Unload \\((B_i, B_j)\\) from memory.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/1..ordered_indices/","title":"Ordered Indices","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/1..ordered_indices/#ordered-index","title":"Ordered Index","text":"<p>In an ordered index, keys are stored in the order that we want to parse them over.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/1..ordered_indices/#clustering-index","title":"Clustering Index","text":"<p>The index whose search key specifies the sequential order of the file is referred to as a clustering or a primary index. It is typically (but not necessarily) the primary key.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/1..ordered_indices/#secondary-index","title":"Secondary Index","text":"<p>An index whose search key is in an order different from the sequential order of the file is referred to as a non-clustering or secondary index.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/1..ordered_indices/#index-sequential-access-file-isam","title":"Index-Sequential Access File (ISAM)","text":"<p>an ISAM is a sequential file ordered on a search key, with a clustering index on it. These are designed for applications that require both sequential and random access to records.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/10..B%2B_tree/","title":"B+ Tree","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/10..B%2B_tree/#why-b-trees","title":"Why B+ Trees?","text":"<ul> <li>ISAM requires overflow block usage, which makes execution slower</li> <li>Periodic reoragnisation of the full file is requied</li> <li>B+ trees reorganise themselves with small local changes, and do not need such reorganisation.</li> <li>Note that they come with a minor space overhead, but it is worth the efficiency.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/10..B%2B_tree/#example","title":"Example","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/10..B%2B_tree/#constraints","title":"Constraints","text":"<ul> <li>All paths from root ot a leaf are of the same length</li> <li>A tree of size n can have up to \\(n-1\\) values and \\(n\\) children</li> <li>Each node has at least half-filled children (rounded up)</li> <li>Each node is at least half-full (rounded up)</li> <li>A B+ Tree counts as a multi-level sparse index.</li> <li>The tree height is no more than \\(\\lceil log_{\\lceil n/2 \\rceil} (K) \\rceil\\)  height, for \\(K\\) search key values.</li> <li>A node is generally the same size as a disk block, 4KB.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/2..indices/","title":"Indices","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/2..indices/#why-indexing","title":"Why Indexing?","text":"<p>Indexing speeds up access to desired data. An index stores key value pairs of a search-key and a pointer. These are typically much smaller than the original file. </p> <p>Note that indicing itself brings an overhead. When an element is added or removed, the index table has to be updated as well.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/2..indices/#kinds-of-indices","title":"Kinds of Indices","text":"<ul> <li>Ordered Index: Search keys are stored in an ordered fashion, and parsed as such.</li> <li>Hash Indices: Search keys are distributed uniformly between buckets via a hash cunction.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/2..indices/#index-evaluation-metrics","title":"Index Evaluation Metrics","text":"<ul> <li>Access Time</li> <li>Insertion Time</li> <li>Deletion Time</li> <li>Space Overhead</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/3..dense_and_sparse_indices/","title":"Dense And Sparse Indices","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/3..dense_and_sparse_indices/#dense-index","title":"Dense Index","text":"<p>In a dense index, a key-value pair is stored for every index in the table. A dense index on a certain attribute of a table stores pointers to the first element where the value of this attribute changes, in a table sorted by this attribute.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/3..dense_and_sparse_indices/#sparse-index","title":"Sparse Index","text":"<p>A sparse index contains records for only some search-key values. This is only sensible when the records are ordered by the search key. These take less space to store than the dense indexes, but are also slower. </p> <p>Note that secondary records always have to be dense indices, since the records wont be sorted in their order.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/3..dense_and_sparse_indices/#clustering-vs-non-clustering-indices","title":"Clustering vs. Non-Clustering Indices","text":"<ul> <li>Sequential scan with a clustering index is cool, because it minimises number of blocks that have to be loaded in from memory.</li> <li>Sequential sacn on a non-clustering index is much slower because blocks have to be repeatedly loaded and unloaded.'</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/3..dense_and_sparse_indices/#multilevel-index","title":"Multilevel Index","text":"<p>A multilevel index is used when even the index is too large to fit in memory. We keep the index as a sequential file and create another sparse index on the index. This is called a multilevel indexing system. It leads to even more insertion / deletion overhead.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/4..index_update_algorithms/","title":"Index Update Algorithms","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/4..index_update_algorithms/#insertion","title":"Insertion","text":"<ul> <li>Look up the search-key value of the record to be inserted<ul> <li>Dense Index - If the record is not in the index, add it. Since all key value pairs are connected via a linked list, we need to make an overflow block to store this new record.</li> <li>Sparse Index - Search for the block of the file closest to the entry. Then search for the entry in the file.</li> </ul> </li> <li>In a compostite key, the sorting can be done on multiple attributes.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/4..index_update_algorithms/#deletion","title":"Deletion","text":"<ul> <li>If the deleted record was the only record in the file with this particular search-key value, then remove it from the index as well.<ul> <li>Dense Index - deletion of search-key is like file record deletion</li> <li>Sparse Index - if this record isn't being indexed, do nothing. If it is being indexed, then index the next element in its place. If the next element was also indexed already, drop the key-value pair.</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/5..operations_on_B%2B_trees/","title":"Operations On B+ Trees","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/5..operations_on_B%2B_trees/#insertion","title":"Insertion","text":"<ul> <li>Search the leaf nodes for the correct place to put the new value (logarthmically complex process). </li> <li>If there's a vacant slot in the node, place it there.</li> <li>If there's no vacant slot in the node, break it into two nodes.</li> <li>Insert the left-most value of the new node in the parent.</li> <li>Repeat this process till you get to the root.</li> <li>If the tree is full, make a new layer (at the top).</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/5..operations_on_B%2B_trees/#deletion","title":"Deletion","text":"<ul> <li>Remove the value from the leaf layer</li> <li>If the size of the node falls below half, merge it with the previous node</li> <li>Propagate changes upwards accordingly</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/6..tree_building/","title":"Tree Building","text":"<p>No matter what you do, inserting in a sorted fashion makes life easy.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/6..tree_building/#bulk-loading","title":"Bulk Loading","text":"<ul> <li>After sorting in ascending order, dump all nodes in the tree from smallest to largest</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/6..tree_building/#bottom-up-construction","title":"Bottom-Up Construction","text":"<ul> <li>After sorting in ascending order, set the value of all the leaf nodes</li> <li>Build up the layers above one by one from these</li> <li>This is what most DBMS use</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/7..hashing/","title":"Hashing","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/7..hashing/#static-hashing","title":"Static Hashing","text":"<ul> <li>A bucket is a unit of storage with one or more entries. (typically a disk block). <ul> <li>A bucket's adress is the output of a hash function, with the input being the index.</li> <li>The bucket contains pointer/s to any row/s.</li> </ul> </li> <li>In a hash index, bucket store pointers to records</li> <li>In a hash file-organisation, buckets store the records themselves</li> <li>This doesn't work well when a database significantly grows or shrinks. We can restructure the index, but that leads to a non-uniform time expenditure.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/7..hashing/#overflow-buckets","title":"Overflow Buckets","text":"<ul> <li>Overflow buckets exist for when a bucket gets filled up.</li> <li>This can happen due to a lack of buckets, or a skew in distribution of records.</li> <li>Overflow buckets are typically kept as a linked list.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/7..hashing/#dynamic-hashing","title":"Dynamic Hashing","text":"<ul> <li>In Periodic Rehashing, hashing is done periodically. Say if the number of entries is 1.5x the size of the hash table. We make a new table of larger size, and reload all entries into them</li> <li>Linear Hashing does rehashing in an incremental manner.</li> <li>Extendable Hashing allows sharing of a bucket by multiple hash values, allowing to increase the number of hash entries without increasing number of buckets.</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/7..hashing/#hashing-vs-ordered-indexing","title":"Hashing vs. Ordered Indexing","text":"<ul> <li>Hashes involve periodic reorganisation, which is suboptimal in databases with lots of insertions and deletions.</li> <li>Hashes are better at retrieving records of a specific key, ordered indices are better at pulling out ranges. Hashing is not used a lot in practice</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/8..index_in_SQL/","title":"Index In SQL","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/8..index_in_SQL/#index-creation","title":"Index Creation","text":"<pre><code>CREATE INDEX &lt;index-name&gt; ON &lt;relation-name&gt; (&lt;attributes&gt;)\n</code></pre> <ul> <li>Use <code>CREATE UNIQUE INDEX</code> to indirectly force uniqueness on the index, although it isn't needed if the SQL dialect has the <code>unique</code> integrity constraint</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/8..index_in_SQL/#index-deletion","title":"Index Deletion","text":"<pre><code>DROP INDEX &lt;index-name&gt;\n</code></pre> <p>Most DBMS allow specification for type of index and clustering.</p>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/9..bitmap_indices/","title":"Bitmap Indices","text":""},{"location":"CSE202_DBMS/12..indexing_and_hashing/9..bitmap_indices/#bitmap-index","title":"Bitmap Index","text":"<p>A bitmap index is a special kind of index designed for querying on multiple keys efficiently.</p> <ul> <li>Records are assumed to be in numerically sequential form.<ul> <li>Given a number \\(n\\) it must be easy to retrieve the \\(n\\)'th record, especially if records are of fixed size</li> </ul> </li> <li>It's great for records that take on a relatively small number of distinct values</li> </ul>"},{"location":"CSE202_DBMS/12..indexing_and_hashing/9..bitmap_indices/#storage-mechanism","title":"Storage Mechanism","text":"<p>A bitmap index is basically an inverted hash table on the row</p> income_level L1 L2 L1 L4 L3 <p>becomes</p> <pre><code>L1 - 10100\nL2 - 01000\nL3 - 00001\nL4 - 00010\nL5 - 00000\n</code></pre> <p>These can be great for applying boolean algebra on (e.g., select all people who have income level L1 or L2, etc.)</p>"},{"location":"CSE202_DBMS/13..transactions/1..transaction/","title":"Transaction","text":"<p>A transaction is a bunch of instructions that are clubbed together. The defining attribute of a transaction is that it's performed either completely or not at all. It may update items in the database.</p>"},{"location":"CSE202_DBMS/13..transactions/1..transaction/#acid-requirements","title":"ACID Requirements","text":""},{"location":"CSE202_DBMS/13..transactions/1..transaction/#atomicity","title":"Atomicity","text":"<ul> <li>Come all, or come none.</li> <li>The transaction is either completed fully, or aborted.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/1..transaction/#consistency","title":"Consistency","text":"<ul> <li>Constraints maintain their truthfulness before and after a transaction.</li> <li>This includes explicit integrity constraints like foreign keys, as well as implicit integrity constraints (consider the sum of money in A and B after a fund transfer between the two).</li> <li>The database may be inconsistent during a transaction.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/1..transaction/#isolation","title":"Isolation","text":"<ul> <li>Result of a set of transactions done concurrently must be the same as the result of serial execution.</li> <li>Each transaction must behave as if it is unaware of concurrently existing transactions.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/1..transaction/#durability","title":"Durability","text":"<ul> <li>The database must be able to restore even if there is a failure, such as a system failure or a software failure.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/2..states_of_transaction/","title":"States Of Transaction","text":""},{"location":"CSE202_DBMS/13..transactions/2..states_of_transaction/#states-of-transaction_1","title":"States of Transaction","text":"<ul> <li>Active - the transaction has begun, but isn't doing anything yet.</li> <li>Partially Committed - the transaction has reached its final statement, but not committed yet.</li> <li>Committed - the transaction has finished successfully.</li> <li>Failed - it has been discovered that the transaction cannot proceed for whatever reason.</li> <li>Aborted - the transaction has been stopped from proceeding.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/3..recoverability/","title":"Recoverability","text":""},{"location":"CSE202_DBMS/13..transactions/3..recoverability/#recoverable-schedule","title":"Recoverable Schedule","text":"<p>A schedule is recoverable if any piece of data read from transaction \\(T\\) by transaction \\(U\\) is committed by \\(T\\) before it is read by \\(U\\).</p>"},{"location":"CSE202_DBMS/13..transactions/3..recoverability/#dirty-read","title":"Dirty Read","text":"<p>A dirty read is a read made on data before it is committed. This is a problem. If the data is corrupted and the transaction needs to be aborted, the dirty read cascades this problem over. * A recoverable schedule will have no dirty reads.</p>"},{"location":"CSE202_DBMS/13..transactions/3..recoverability/#rollback-cascading","title":"Rollback Cascading","text":"<p>An abortion of a single transaction can lead to inconsistency in a chain of transactions. In the absence of a dirty read, cascading rollback is avoided. To fix this, we will need to rollback both the original transaction, as well as the dependent transictions.</p> <p>A schedule where this does not happen is referred to as a cascadeless schedule. All cascadeless schedules are recoverable, but not the other way round.</p>"},{"location":"CSE202_DBMS/13..transactions/4..concurrency/","title":"Concurrency","text":"<ul> <li>Concurrency is achieved when transactions are not executed serially. Their instructions are interleaved with each other.</li> <li>This is good because we can stick instructions working on the same resources together, saving the amount of computation time and disk reads needed.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/4..concurrency/#concurrency-control","title":"Concurrency Control","text":"<ul> <li>A schedule must be serialisable, recoverable, and preferably cascaedeless.</li> <li>This is achieved by a serial execution, but that is not desirable.</li> <li>Concurrency-control schemes perform a tradeoff between concurrency and incurred overhead.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/5..weak_consistency/","title":"Weak Consistency","text":""},{"location":"CSE202_DBMS/13..transactions/5..weak_consistency/#weak-levels-of-consistency","title":"Weak Levels of Consistency","text":"<p>Some applications can make do with lower levels of consistency. This allows for schedules that are not serialisable. These applications trade off consistency for performance.</p>"},{"location":"CSE202_DBMS/13..transactions/5..weak_consistency/#examples","title":"Examples","text":"<ul> <li>Read-only transactions.</li> <li>Database statistics computed for query optimisation.</li> <li>Calculating average of a big data computation.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/6..SQL-92_levels_consistency/","title":"SQL-92 Levels Consistency","text":""},{"location":"CSE202_DBMS/13..transactions/6..SQL-92_levels_consistency/#levels-of-consistency","title":"Levels of Consistency","text":"<ul> <li>Serialisable - the default level of consistency.</li> <li>Repeatable Read - only committed records to be read.<ul> <li>Repeat reads return a same value, but the transaction may not be serialisable.</li> </ul> </li> <li>Read Committed  - only committed records can be read.<ul> <li>Successive reads may return different, but committed values.</li> </ul> </li> <li>Read Uncommitted - even uncommitted records may be read.</li> </ul>"},{"location":"CSE202_DBMS/13..transactions/6..SQL-92_levels_consistency/#lower-consistency-uses","title":"Lower Consistency Uses","text":"<ul> <li>A lower degree of consistency can be useful for gathering approximate information about the database. </li> </ul>"},{"location":"CSE202_DBMS/13..transactions/7..serialisability/","title":"Serialisability","text":"<p>Serialisability is the ability of a schedule to be transformed into a serial schedule without altering the result of the operation. * Conflict Serialisability - avoid RAW / WAW hazards * View Serialisablility - fancier stuff. It's NP-Hard</p>"},{"location":"CSE202_DBMS/13..transactions/7..serialisability/#conflict","title":"Conflict","text":"<ul> <li>Read and Read &gt; No conflict</li> <li>Write and Read &gt; Conflict</li> <li>Read and Write &gt; Conflict</li> <li>Write and Write &gt; Conflict</li> </ul> <p>Any conflict presents an order, which must be preserved during any modification.</p>"},{"location":"CSE202_DBMS/13..transactions/7..serialisability/#conflict-serialisability","title":"Conflict Serialisability","text":"<p>A schedule \\(S\\) is considered conflict serialisable if it is conflict-equivalent to, i.e. can be converted to a serial schedule \\(S'\\) (achieved by swapping no conflicting instructions).</p>"},{"location":"CSE202_DBMS/13..transactions/7..serialisability/#testing-for-conflict-serialisability","title":"Testing for Conflict Serialisability","text":"<p>We test for conflict serialisability by making a precedence graph. If and only if we see a circular dependency, the schedule is non-conflict-serialisable.</p>"},{"location":"CSE202_DBMS/13..transactions/8..schedule/","title":"Schedule","text":"<p>A schedule is a sequence of instructions that tells us how to run multiple transactions concurrently.  * It must preserve all of the instruction. * It must preserve the order of the instructions. * To ensure consistency, it must also preserve the result of the transaction, compared to serial execution.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/1..EXPLAIN/","title":"EXPLAIN","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/1..EXPLAIN/#explain_1","title":"Explain","text":"<p>The explain keyword outputs the execution plan provided by the query optimiser.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/10..comparison_selections/","title":"Comparison Selections","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/10..comparison_selections/#sorted-selection","title":"Sorted Selection","text":"<ul> <li>perform a linear file scan in the part ahead or behind the specified index, depending on the request.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/10..comparison_selections/#unsorted-selection","title":"Unsorted Selection","text":"<ul> <li>For \\(A \\ge V\\) , use index to find the first tuple, then scan relations sequentially from there.</li> <li>For \\(A \\le V\\), scan leaf pages for index to find pointers till first entry \\(&gt; V\\).</li> <li>Retrieve record in either case</li> <li>This requires an IO per record, which can be larger than a linear file scan.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/11..conjunctive_and_disjunctive_selections/","title":"Conjunctive And Disjunctive Selections","text":"<p>For \\(\\sigma_{\\theta_1 \\land \\theta_2 ... \\land \\theta_n}(r)\\)</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/11..conjunctive_and_disjunctive_selections/#single-index","title":"Single Index","text":"<ul> <li>Select a combination of previous algorithms that results in the least cost for the selection.</li> <li>Test other conditions on tuple after it's fetched into the buffer.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/11..conjunctive_and_disjunctive_selections/#composite-index","title":"Composite Index","text":"<ul> <li>Search over a composite index if available</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/11..conjunctive_and_disjunctive_selections/#intersection-of-identifiers","title":"Intersection of Identifiers","text":"<ul> <li>Store passed condition by each record, then select the records that pass all the conditions.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/12..sorting/","title":"Sorting","text":"<ul> <li>sorting can be done logically through an index, as opposed to physical. This may lead to a lot of disk block accesses though. So, it's desirable to sort the records physically.</li> <li>for relations that fit in memory, quicksort is good.</li> <li>for relations that don't fit in memory, external sort-merge works better because it's in-place.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/12..sorting/#differences-from-mergesort","title":"Differences from Mergesort","text":"<ul> <li>processing is done block-by-block</li> <li>the merging is an \\(N\\)-way merge between \\(M\\) blocks.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/2..join/","title":"Join","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/2..join/#nested-loop","title":"Nested Loop","text":"<ul> <li>Chosen very typically when multiple values have to be compared.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/2..join/#pseudo-code","title":"Pseudo-Code","text":"<p>For each pair \\(P = (R_i, R_j)\\) in the cartesian product,     if \\(P\\) satisfies predicate \\(\\theta\\),         select P.     else,         do nothing.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/2..join/#block-nested-loop","title":"Block Nested Loop","text":"<ul> <li>Similar to nested loop, but compares block by block as well</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/2..join/#pseudo-code_1","title":"Pseudo-Code","text":"<p>For each block pair \\(B = (B_r, B_s)\\),     For each record pair \\(P = (R_i, R_j)\\),         If \\(P\\) satisfies predicate \\(\\theta\\),             select P.         else,             do nothing.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/3..indexed_nested_loop_join/","title":"Indexed Nested Loop Join","text":"<p>This is used over file scan when, * the join is an equi-join / natural join, and, * the inner relation has an index. note that an index may be created just for a join.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/3..indexed_nested_loop_join/#pseudo-code","title":"Pseudo-Code","text":"<p>store the index column of one table in memory. for each needed tuple,     use the index to look up needed tuple by index,         if found, select it.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/4..merge-join/","title":"Merge-join","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/4..merge-join/#merge-join_1","title":"Merge-Join","text":"<ul> <li>similar to the join function of the merge sort, provided that the join attributes aren't indices</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/4..merge-join/#pseudo-code","title":"Pseudo-Code","text":"<p>Sort tables by both attributes (if needed). Place a pointer at first record of each table. Proceed like merge function of mergesort,     If match is found between the record pair \\(P\\),         Select \\(P\\).</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/5..multi-query_optimisation/","title":"Multi-query Optimisation","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/5..multi-query_optimisation/#multi-query-optimisation_1","title":"Multi-Query Optimisation","text":"<p>Alternatives for evaluating operation trees are</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/5..multi-query_optimisation/#materialisation","title":"Materialisation","text":"<p>Store the result of one query element directly on the disk. The optimiser generates a plan such that common elements of queries \\(\\{Q_i\\}\\) are executed only once, stored on the disk, then reused.  * This is also referred to as flyweighting or caching.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/5..multi-query_optimisation/#pipelining","title":"Pipelining","text":"<p>As the result of one tuple comes, pass it to the next operation. Consider WHERE-JOIN-GROUPBY-HAVING as a pipeline similar to the RISC pipeline. This can be parallelised for speed-up. * not possible in something like a sort or a hash join. * cheaper than materialisation, as there is no need to store to disk.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/5..multi-query_optimisation/#demand-driven","title":"Demand Driven","text":"<ul> <li>a.k.a. lazy evaluation, pull model.</li> <li>The operation latter in the pipeline pulls records from the operation behind in the pipeline.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/5..multi-query_optimisation/#producer-driven","title":"Producer Driven","text":"<ul> <li>a.k.a. eager ealuation, push model.</li> <li>The operation behind in the pipeline immediately pushes ahead in the pipeline</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/6..evaluation_plans/","title":"Evaluation Plans","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/6..evaluation_plans/#evaluation-plan","title":"Evaluation Plan","text":"<p>An evaluation plan gives atomic implementation-specific steps to the processing engine to do its job.</p> <p>For example, the query <code>SELECT * FROM instructors WITH salary &lt; 75000</code> will process to something like * Perform complete relation scan and discard instructors with salary &gt; 7500</p> <p>An RAE can be evaluated in several different ways, and its performance can be evaluated on various metrics as well.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/7..query_cost/","title":"Query Cost","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/7..query_cost/#query-optimisation","title":"Query Optimisation","text":"<p>Query optimisation involves choosing the evaluation plan with the smallest query cost of all possible equivalent options.</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/7..query_cost/#query-cost_1","title":"Query Cost","text":"<p>Factors that come into calculating time cost of query are * number of disk accesses * resource availability (CPU, memory) * network communication speed (for distributed or server-client databases)</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/7..query_cost/#estimates","title":"Estimates","text":"<ul> <li>number of seeks * average seek cost</li> <li>number of blocks read * average block read cost</li> <li>number of blocks written * average block write cost</li> </ul> <p>In our case, we will use number of block transfers and number of seeks as the cost measures. The cost is the sum of total time spent on block transfer and seeking data. \\(\\(\\omega = b * t_T + S * t_S\\)\\)</p>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/7..query_cost/#reasons-for-inaccuracy","title":"Reasons for Inaccuracy","text":"<ul> <li>Does not account for data stored in cache / buffer, which has faster IO. This is incredibly tough to estimate.</li> <li>Amount of actual memory available depends on other processes executing in the system, which cannot be consistently known or reliably estimated.</li> <li>We use (optimistic) estimates of data stored in buffer and memory needed for operations, but those are just estimates.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/8..file_scan/","title":"File Scan","text":"<p>File Scan is a form of linear search. It scans each file block and tests each record \\(R_i\\) to see if it satisfies the given predicate \\(\\theta\\).</p> <ul> <li>If there are \\(n\\) blocks in the file, transferred in this query, then the cost is \\(n + 1 (seek)\\). </li> <li>If selection is done on a key attribute, then we can stop on finding the record. This gives an average cost of \\(n/2 + 1\\)</li> <li>Binary search can also be performed on a sorted attribute, but a sorted attribute is just an index, which we can search far more quickly on.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/9..index_scans/","title":"Index Scans","text":""},{"location":"CSE202_DBMS/14..optimisation_algorithms/9..index_scans/#equality-on-key","title":"Equality on Key","text":"<ul> <li>to retrieve a single record that satisfy equality predicate \\(\\theta\\)</li> <li>\\(\\omega = h_i * (t_t + t_s) + t_s + t_t\\) </li> <li>all blocks of the index table are loaded into the memory, then a seek is done to fetch the one matching the index.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/9..index_scans/#equality-on-non-key","title":"Equality on Non-Key","text":"<ul> <li>to retrieve \\(n\\) records that match general predicate \\(\\theta\\) </li> <li>\\(\\omega = h_i * (t_T + t_S) + t_S + t_T * n\\) </li> <li>assuming seek has to be done only once because blocks are adjacent</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/9..index_scans/#secondary-index-single-key","title":"Secondary Index - Single Key","text":"<ul> <li>\\(\\omega = (h_i+1)*(t_T+t_S)\\) </li> <li>cost to load the index table into the memory, then to seek and fetch the desired element.</li> </ul>"},{"location":"CSE202_DBMS/14..optimisation_algorithms/9..index_scans/#secondary-index-non-key","title":"Secondary Index - Non-Key","text":"<ul> <li>\\(\\omega = (h_i + n)*(t_T + t_S)\\)</li> <li>this scenario is assuming that each of the non-key records is in a different block. in this scenario, it can be quite expensive.</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/1..attributes/","title":"Attributes","text":"<ul> <li>Each attribute has a set of allowed values. This is known as a domain</li> <li><code>null</code> lies within each attribute's domain, unless it is defined as non-null</li> <li>Attribute values are supposed to atomic by default</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/10..cartesian_product%2C_join_operations/","title":"Cartesian Product, Join Operations","text":""},{"location":"CSE202_DBMS/2..relational_model/10..cartesian_product%2C_join_operations/#cartesian-product","title":"Cartesian Product","text":"<ul> <li>The cartesian product operation allows us to combine informationf rom two relations</li> <li>If a column appears in two relations, we distinguish between them by writing the name of each table as a prefix in the combined table</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/10..cartesian_product%2C_join_operations/#example","title":"Example","text":"<p>Table Instructors</p> ID name dept_name salary 222 Hello Physics 165005 351 World Chemistry 150406 <p>Table Topics</p> ID chapter_name days_alloted dept 20 Atomic Theory 12 Physics 13 Molecular Mixing 15 Chemistry 25 Waves and Particles 10 Physics <p>We can merge these tables with \\(\\(\\quad instructors\\; X \\; topics\\)\\) To select these values, we will use \\(\\sigma(instructors\\; X \\; topics)\\)</p>"},{"location":"CSE202_DBMS/2..relational_model/10..cartesian_product%2C_join_operations/#join","title":"Join","text":"<ul> <li>The Join operation allows us to combine a \\(\\sigma\\) and an \\(X\\) into a single operation</li> <li>\\(r \\, \\bowtie_{\\,\\theta}\\; s\\)  is the same thing as \\(\\sigma_\\theta \\;(r\\; X \\; s)\\)</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/10..cartesian_product%2C_join_operations/#example_1","title":"Example","text":"<p>The statement \\(\\(\\quad \\sigma_{\\,instructors.dept\\_name\\,=\\,topics.dept}\\;(instructor\\;X\\; topics)\\)\\) is equivalent to \\(\\(\\quad instructors\\; \\bowtie_{\\;instructors.dept\\_name\\,=\\,topics.dept}\\;topics\\)\\)</p>"},{"location":"CSE202_DBMS/2..relational_model/2..database_schema/","title":"Database Schema","text":"<ul> <li>The Database Schema is the logical structure of the database</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/2..database_schema/#database-instance","title":"Database Instance","text":"<ul> <li>The database instance is a snapshot of data inside the database at any point</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/3..union%2C_intersection%2C_difference_operations/","title":"Union, Intersection, Difference Operations","text":"<ul> <li>The union operator allows us to combine two relations</li> <li>The intersection operator allows us to take the intersection of two relations</li> <li>The difference operator allows us to take the difference of two relations</li> <li>For \\(r \\cup s\\),  \\(r \\cap s\\) ,  or \\(r-s\\)  to be valid,<ul> <li>r and s must have the same number of attributes</li> <li>the attribute domains must be compatible with each other</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/3..union%2C_intersection%2C_difference_operations/#example","title":"Example","text":"<p>$$\\begin {align}</p> <p>\\quad &amp;\\Pi_{\\,course_ID}\\,(\\sigma_{semester=^\"Fall^\" \\,\\land\\,year=2017}\\,(section)) \\ \\  \\cup \\ </p> <p>\\quad &amp;\\Pi_{\\,course_ID}\\,(\\sigma_{semester=^\"Spring^\" \\,\\land\\,year=2018}\\,(section)) \\</p> <p>\\end {align}$$</p>"},{"location":"CSE202_DBMS/2..relational_model/4..relational_query/","title":"Relational Query","text":"<ul> <li>While SQL queries are implementation-agnostic, Relational Query operations also tell the DBMS how to get the information requested</li> <li>There exist also Tuple Relational Calculus and Domain Relational Calculus, but we won't meddle with those here</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/4..relational_query/#relational-algebra","title":"Relational Algebra","text":"<ul> <li>Relational Algebra consists of sets, and operations performed on those set. It runs on six fundamental operators<ul> <li>select (\\(\\sigma\\))</li> <li>project (\\(\\pi\\))</li> <li>union (\\(\\cup\\))</li> <li>set difference (-)</li> <li>cartesian product (\\(X\\))</li> <li>rename (\\(\\rho\\)) </li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/5..relation_implementation/","title":"Relation Implementation","text":"<ul> <li>\\(A_1, A_2, A_3, ... A_n\\) are attributes</li> <li>\\(R\\) = (\\(A_1, A_2, A_3, ... A_n\\)) is a relation schema</li> <li>A relation instance over a schema \\(R\\) is given as \\(r(R)\\)</li> <li>All elements in a relation or an entity are stored as tuples</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/5..relation_implementation/#ordering","title":"Ordering","text":"<ul> <li>All tuples in a table are unordered. </li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/6..key/","title":"Key","text":"<ul> <li>A key is something which is used to identify an index in a table</li> <li>Superkey - \\(K\\) is called a superkey of \\(R\\) if it is sufficient to identify each possible relation \\(r(R)\\)</li> <li>Candidate Key - \\(K\\) is called a minimal / candidate superkey of \\(R\\) if it is a superkey of minimal cardinality </li> <li>Primary Key - The primary key \\(P\\) is an arbitrarily chosen candidate key</li> <li>Foreign Key - A foreign key in one table is the primary key in another table<ul> <li>A foreign key must appear in either a referencing relation, or a referenced relation </li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/7..select_operation/","title":"Select Operation","text":"<ul> <li>Select operation selects all tuples that satisfy a given predicate</li> <li>It is written as \\(\\sigma_p(r)\\) , where p is the selection predicate</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/7..select_operation/#example","title":"Example","text":"<p>Take the table <code>instructor</code></p> ID name dept_name salary 222 Hello Physics 165005 351 World Chemistry 150406 <p>SELECT * FROM instructor WHERE name = \"World\"      becomes \\(\\(\\sigma_{dept\\_name\\,=\\,^\"Physics^\"}\\,(instructor)\\)\\)</p> <p>This gives the result</p> ID name dept_name salary 222 Hello Physics 165005"},{"location":"CSE202_DBMS/2..relational_model/7..select_operation/#sub-operations","title":"Sub-operations","text":"<ul> <li>relational operators are allowed. Use ligatures (like \\(\\neq\\), \\(\\geq\\), \\(\\leq\\))</li> <li>logical operators are also allowed (like \\(\\land\\) , \\(\\lor\\) , \\(\\lnot\\) )</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/7..select_operation/#example_1","title":"Example","text":"<pre><code>SELECT * FROM instructors WHERE dept_name = 'Physics' AND salary &gt; 100000\n</code></pre> <p>becomes \\(\\(\\sigma_{dept\\_name\\,=\\,'Physics'\\,\\land\\,salary\\,&gt;\\,100000 }(instructor)\\)\\)</p>"},{"location":"CSE202_DBMS/2..relational_model/8..project_operation/","title":"Project Operation","text":""},{"location":"CSE202_DBMS/2..relational_model/8..project_operation/#project","title":"Project","text":"<ul> <li>The project operation eliminates some attributes from the returned query</li> <li>If \\(A_1, A_2, ... A_n\\)  are attributes and \\(r\\) is the relation, then the operation is written as \\(\\Pi_{A_1, A_2, ... A_k}(r)\\)</li> <li>This will also remove any rows that are now duplicates</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/8..project_operation/#example","title":"Example","text":"ID name dept_name salary 222 Hello Physics 165005 351 World Chemistry 150406 <p><code>SELECT ID, name FROM instructors</code>  becomes \\(\\(\\Pi_{ID,\\,name}(instructor)\\)\\)</p>"},{"location":"CSE202_DBMS/2..relational_model/9..assignment%2C_rename_operators/","title":"Assignment, Rename Operators","text":""},{"location":"CSE202_DBMS/2..relational_model/9..assignment%2C_rename_operators/#assignment","title":"Assignment","text":"<ul> <li>As the name suggests, the assignment operator allows us to assign the result of one query to a variable $$ \\quad Physics \\leftarrow \\sigma_{\\,dept_name\\,=\\,^\"Physics^\"}\\, (instructors) $$</li> </ul>"},{"location":"CSE202_DBMS/2..relational_model/9..assignment%2C_rename_operators/#rename","title":"Rename","text":"<ul> <li>The rename operator allows us to rename the result of a query under another name \\(\\(\\quad \\rho_{x(A_1, A_2, ... A_n)}(E)\\)\\)</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/1..roles/","title":"Roles","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/1..roles/#role","title":"Role","text":"<p>A role is what we call a position that an entity from an entity set fills in</p> <ul> <li>Two different roles need not be occupied by different entities. They may very well be filled in by the same entity</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/1..roles/#example","title":"Example","text":"<pre><code>Handing any weapon to any soldier is not a very smart idea. So, our civilisation needs to come up with an intelligent and hierarchical way to grant soldiers access to weaponry.\n\nTo have access to a gun, one must first be adept with the bow-and-arrow. To have access to a poleaxe, one must first be adept with the lance. The general wants us to generalise this concept to all weapons.\n</code></pre> <p>This leads us to a special kind of relation</p> <ul> <li>Weapon &lt;- prerequisite of -&gt; Weapon</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/1..roles/#representation-of-a-self-relationship","title":"Representation of a Self-Relationship","text":"<pre>69170b54-6ce9-4e62-9d93-34969b7aae7f</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/10..relationship_degree/","title":"Relationship Degree","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/10..relationship_degree/#degree","title":"Degree","text":"<p>The degree expresses the number of participants involved in a relationship. This is typically two</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/10..relationship_degree/#example","title":"Example","text":"<pre><code>A soldier's expertise in a weapon is measured using a number of parameters. These include accuracy, precision, consistency, and ease of manouevering. The general wants us to store the expertise reports of each soldier, so he can think of the thresholds for expertise tiers [he is insistent on relative grading].\n</code></pre> <p>Here we see the ternary relationship manifest as:</p> <ul> <li>Soldier &lt;- has an expertise in -&gt; Weapon                                  -&gt; Expertise report</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/10..relationship_degree/#representation-of-a-ternary-relationship","title":"Representation of a Ternary Relationship","text":"<pre>0ee66f33-23fb-4da5-a79b-19933dd39af1</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/11..cardinality/","title":"Cardinality","text":"<p>Cardinality decides how many members are present in a relationship on either side. The mapping of cardinality between relationships can be:</p> <ul> <li>One to One</li> <li>One to Many</li> <li>Many to One</li> <li>Many to Many</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/11..cardinality/#example","title":"Example","text":"<p>In the examples discussed so far:</p> <ul> <li>soldier --&gt; tent is a one-one relationship. one soldier can only reside in one tent, and one tent can house only soldier</li> <li>army --&gt; soldier is a one-many relationship. there are many soldiers fighting in the army, but a soldier can be present in only one army</li> <li>soldier --&gt; weapon is a many-many relationship. a soldier can hold many kinds of weapons, and a weapon variety can be used by any amount of soldiers</li> <li>soldier --&gt; leader is a many-one relationship. each soldier is placed under only one leader, and one leader can lead a group of many soldiers</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/11..cardinality/#representation-of-cardinality","title":"Representation of Cardinality","text":"<ul> <li>a directed line signifies one</li> <li>an undirected line signifies many</li> </ul> <pre>89c39ab0-e1af-4cab-8878-98cd4b0e8d7d</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/11..cardinality/#specific-cardinality-constraints","title":"Specific Cardinality Constraints","text":"<p>Like UML diagrams, ER diagrams can also show numbered cardinality constraints. These are also indicated in the same manner</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/11..cardinality/#example_1","title":"Example","text":"<p>A soldier must be present in one army, and one army only. Meanwhile an army must contain at least one soldier</p> <pre>60159ce1-1559-410c-8cc2-c4c8e5c781fe</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/12..primary_key/","title":"Primary Key","text":"<p>A primary key is the attribute / set of attributes by which we can distinctly identify a: 1) Entity 2) Relationship 3) Weak Entity</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/12..primary_key/#entity","title":"Entity","text":"<ul> <li>Each entity in an entity set is distinct</li> <li>To materialise this distinctness, the primary key is used. No two entities are allowed to have the same primary key</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/12..primary_key/#relationship","title":"Relationship","text":"<ul> <li>Let \\(R\\) be a relationship involving \\(n\\) entities, \\(\\{E_i\\}^n\\)</li> <li>The primary key for \\(R\\) is the union of the primary keys of each set in \\(\\{E_i\\}^n\\)</li> <li>If the relationship set has attributes \\(\\{A_i\\}^m\\)  associated with it, then its primary key will also have \\(\\{A_i\\}^m\\) associated with it</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/12..primary_key/#choosing-based-on-cardinality-of-relationship","title":"Choosing based on cardinality of relationship","text":"<ul> <li>Many-to-Many: The preceding union of primary keys is a minimal superkey, and is chosen as the primary key</li> <li>One-to-Many: The primary key of the \"Many\" side is a minimal superkey and is used as the primary key</li> <li>One-to-One: The primary key of either one of the participating entity sets can count as the primary key</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/12..primary_key/#weak-entity","title":"Weak Entity","text":"<ul> <li>A weak entity can be discriminated using the foreign key pointing to the identifying entity, on which it relies for existence</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/13..entity_set_corelations/","title":"Entity Set Corelations","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/13..entity_set_corelations/#specialisation","title":"Specialisation","text":"<ul> <li>A specialisation indicates that the child has all attributes of the parent, but also adds something more</li> <li>This is referred to as attribute inheritance, and drawn with a triangular arrow</li> </ul> <pre>22ec80e0-1a15-4fc9-82d5-b424afd7e898</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/13..entity_set_corelations/#overlapping-and-disjoint-specialisation","title":"Overlapping and Disjoint Specialisation","text":"<ul> <li>Overlapping children sets means that that a member of a parent class can belong in both child classes</li> <li>Meanwhile in disjoint specialisation, a parent can only have one of its children's attributes</li> </ul> <pre>f14277b7-d226-4547-a99f-e83baa66eca6</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/13..entity_set_corelations/#generalisation","title":"Generalisation","text":"<p>A generalisation is the inverse of a specialisation. In it, a number of entity sets sharing some features are combined into a higher-level set. The terms specialisation and generalisation are interchangeable</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/13..entity_set_corelations/#completeness-contstraint","title":"Completeness Contstraint","text":"<p>The completeness constraint specifies whether or not a parent entity must overlap with one of its children * Total Generalisation - The parent must share all attributes with one of its children * Partial Generalisation - The parent need not have the exact same attributes as any of its children</p> <p>A total generalisation is indicated with a broken line, and a total label on the diagram.</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/13..entity_set_corelations/#aggreagation","title":"Aggreagation","text":"<p>In an aggregation, multiple entities and / or relationships are abstracted into one aggregate entity</p> <pre>fb4998d4-5e47-4268-896c-ce5485f27707</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/2..relationship/","title":"Relationship","text":"<p>A relationship in E-R databases is akin to a relation of the mathematical kind. It is a construct that connects two or more entities</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/2..relationship/#relationship-set","title":"Relationship Set","text":"<p>A relationship set is a mathematical relation between b &gt;= 2 entities, each taken from entity sets. It describes a relationship between two or more entity sets</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/2..relationship/#example","title":"Example","text":"<pre><code>In the Battling Civilisations videogame, the player enters a stage (one of many, with their own designs) with their army. Both the player and foe's armies contain soldiers, who carry different kinds of weapons. While traversing the stage, one must watch out for traps set by the enemy's civilisation. \n\nAfter the battle is over, the soldiers come back to spend the night in their tents. Each soldier has his own tent.\n</code></pre> <p>The relations in this extract are:</p> <ul> <li>Army &lt;- contains -&gt; Soldiers</li> <li>Soldier &lt;- carries -&gt; Weapon</li> <li>Soldier &lt;- rests in -&gt; Tent</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/2..relationship/#representation-of-a-relationship","title":"Representation of a Relationship","text":"<pre>4c08f8c3-249c-4656-99fa-3dc7e228048d</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/3..relationship_attributes/","title":"Relationship Attributes","text":"<p>A relationship set may carry attribute/s. These attributes tell us something noteworthy about the relation, such as when it was created or what its strength is</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/3..relationship_attributes/#example","title":"Example","text":"<pre><code>In the Battling Civilisations videogame, the player enters a stage (one of many, with their own designs) with their army. Both the player and foe's armies contain soldiers, who carry different kinds of weapons. While traversing the stage, one must watch out for traps set by the enemy's civilisation. \n\nSoldiers are divided into regiments, each with a regiment leader. Depending upon the leader's performance in battle, the soldiers will get attached to their leader, in different capacity each. The more attached a soldier is, the higher morale they will show in battle.\n\nAfter the battle is over, the soldiers come back to spend the night in their tents. Each soldier has his own tent. For each night that he spends in the tent, they will fall asleep more comfortably, leading to them regenerating more HP over the night.\n\nOver time, each soldier develops an expertise in a category of weapon. This expertise increases their efficacy in use of the weapon.\n</code></pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/3..relationship_attributes/#relation-attributes","title":"Relation Attributes","text":"<ul> <li>Soldier &lt;- obeys { Morale } -&gt; Leader</li> <li>Soldier &lt;- lives in { Resting Ability } -&gt; Tent</li> <li>Soldier &lt;- carries { Expertise in } -&gt; Weapon</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/3..relationship_attributes/#representation-of-relation-attributes","title":"Representation of Relation Attributes","text":"<pre>54cc3534-9b9a-476e-a4b4-cb807413074b</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/4..total_and_partial_participation/","title":"Total And Partial Participation","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/4..total_and_partial_participation/#total-participation","title":"Total Participation","text":"<p>Total Participation indicates that every entity in this set participates in the relationship set. It is indicated with a double line</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/4..total_and_partial_participation/#partial-participation","title":"Partial Participation","text":"<p>Partial Participation is observed when some entities don't necessarily participate in any relationship in the relationship set. These can be represented with just a single line</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/4..total_and_partial_participation/#example","title":"Example","text":"<p>Each soldier serves in an army, and each army must have soldiers in it. Additionally, the soldier-army relation is many-to-one</p> <pre>92c2b6a3-9e9a-4fb5-9047-9883beb08911</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/5..weak_entity_set/","title":"Weak Entity Set","text":"<p>A weak entity set is one that has to rely on another entity set for its existence</p> <ul> <li>This entity is known as its identifying entity</li> <li>Instead of associating a primary key with a weak entity, we use the identifying entity, (along with extra attributes called the discriminator) to uniquely identify a weak entity</li> <li>The identifying entity set is said to own the weak entity set</li> <li>This relationship is referred to as an identifying relationship</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/5..weak_entity_set/#representation-of-weak-entity-set","title":"Representation of Weak Entity Set","text":"<ul> <li>A weak entity is represented in an E-R diagram with a double rectangle</li> <li>The discriminator is underlined with a dashed line</li> <li>The relationship set connecting the weak entity to the identifying relationship is depicted as a double diamond</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/5..weak_entity_set/#example","title":"Example","text":"<pre>96c83d66-2831-4b43-95ce-0fdd6a1d325a</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/6..redundancy/","title":"Redundancy","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/6..redundancy/#redundant-attributes","title":"Redundant Attributes","text":"<p>A redundant attribute is one that is present in multiple related entity sets. The solutiont to this redundancy is to remove it from all entity sets except one.</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/6..redundancy/#example","title":"Example","text":"<pre>229a34c2-347d-4bab-974e-989256cc08b9</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/7..design_phases/","title":"Design Phases","text":"<pre>15b5469a-a927-40eb-b4e8-126f6d421e2d</pre>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/7..design_phases/#initial-analysis-phase","title":"Initial / Analysis Phase","text":"<ul> <li>The initial phase involves almost no code. It is completely planning-oriented</li> <li>It is similar to the analysis phase of programming. It involves looking at the problems of the users and thinking of potential ways to solve them</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/7..design_phases/#design-phase","title":"Design Phase","text":"<ul> <li>The defining step of this phase is choosing a data model</li> <li>One this is done, the requirements of the problem description are applied onto the data model</li> <li>Then, this data model is converted into a conceptual schema. Metadata related to the operations needed to be performed on said data is taken</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/7..design_phases/#implementation-phase","title":"Implementation Phase","text":"<ul> <li>The abstract data model is turned into a concrete database</li> <li>Logical Design - involves deciding relation schemas. Additionally, attributes are shortlisted and pruned from a business perspective, and the organisation is optimised for memory and speed efficiency</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/8..entity/","title":"Entity","text":"<p>An entity is an object that: * has attributes that confirm its existence * is differentiable from every other entity in some way</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/8..entity/#entity-set","title":"Entity Set","text":"<p>An entity set is a set of entitites that share the same attributes * For example, a table containing all employees and their attributes is an entity set.</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/8..entity/#attribute","title":"Attribute","text":"<p>An attribute is a value that tells us something noteworthy about the entity that it belongs to</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/8..entity/#primary-key","title":"Primary Key","text":"<p>A primary key is a special attribute, that helps us uniquely identify each member of the identity set. Think of it as an ID card</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/8..entity/#example","title":"Example","text":"<pre><code>In the Battling Civilisations videogame, one enters a stage (one of many, with their own distinct designs) with their army. Both the player's and foe's armies contain soldiers, who carry weapons. While traversing the stage, one has to watch out for traps set by the enemy's civilisation\n</code></pre> <p>The entities identifiable in this extract are: * Stage  * Civilisation * Army * Soldier  * Weapon (another Weak Entity) * Trap</p> <p>The identification of an entity in E-R Database Design is very similar to the identification of a class in Object Oriented Design</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/8..entity/#representation-of-an-entity","title":"Representation of an Entity","text":"<pre>4c947498-c480-4577-bd4c-145edf23cf6c</pre> <ul> <li>A rectangle represents the collective attributes of an entity set</li> <li>The title of the entity is present in the header</li> <li>Attributes are present in the body</li> <li>The Primary Key is underlined</li> </ul>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/","title":"Attributes","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#attribute","title":"Attribute","text":"<p>An attribute is a value that gives us some noteworthy information about an entity.</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#taxonomy-of-attributes","title":"Taxonomy of Attributes","text":""},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#composite-attributes","title":"Composite Attributes","text":"<p>Composite attributes are those which can be divided into multiple simple attributes. Opposite of simple attributes * It is worth noting that a composite attribute can be expressed as a weak entity too, depending upon the context</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#derived-attributes","title":"Derived Attributes","text":"<p>Derived attributes are those whose values depend completely on other attribute/s</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#multivalued-attributes","title":"Multivalued Attributes","text":"<p>Multivalued attributes are those which contain multiple values inside of them. Opposite of single-valued attributes</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#example","title":"Example","text":"<pre><code>A soldier may end up gaining expertise in multiple weapons. The database should be capable of accomodating a soldier with an A rank in guns and a C rank in swords. The general is insistent that his troops don't specialise too much. The rank itself is granted based on the general's evaluation, which depends on the information given in the evaluation report. Combining all of these, the soldier is given an overall rank, which may be used to calculate his pay in the future.\n\nAfter working with you for a few months, the general is impressed with your work. He wants you to help document the battle history of his soldiers, so he can award medals to the most meritorious ones. This battle history is divided into the days of battle fought, the number of battles won, and the number of enemies defeated by each soldier.\n</code></pre> <p>In this scenario: * Rank is a multivalued attribute, as it includes the ranks in multiple weapons * Rank is also a derived attribute, based on attributes in the evaluation report * Battle history is a composite attribute, containing child attributes like battle count * Evaluation report is written as a weak entity, but its purpose is identical to that of a composite attribute</p>"},{"location":"CSE202_DBMS/3..database_design%2C_E-R_model/9..attributes/#representation-of-these-attributes","title":"Representation of these Attributes","text":"<pre>8dd41786-001e-4672-ad9c-e9bd953b1c2a</pre>"},{"location":"CSE202_DBMS/4..SQL/1..domain_types/","title":"Domain Types","text":"<ul> <li>char(n) - fixed-length string with n characters</li> <li>varchar(n) - variable-length string with upto n characters</li> <li>int - integer, the range is decided by the machine</li> <li>smallint - similar to short</li> <li>numeric(p,d) - a number with p digits, up to d of which can be to the right of the decimal point</li> <li>float(n) - a floating point number with a precision of at least n digits</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/10..database_modification/","title":"Database Modification","text":""},{"location":"CSE202_DBMS/4..SQL/10..database_modification/#deletion","title":"Deletion","text":"<ul> <li><code>DELETE from &lt;table&gt;</code>  - deletes everything in the table</li> <li><code>DELETE FROM &lt;table&gt; WHERE &lt;condition&gt;</code> - deletes rows conditionally from the table</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/10..database_modification/#insertion","title":"Insertion","text":"<ul> <li><code>INSERT into &lt;table&gt; values (tuple)</code> - inserts a row into the table</li> <li>null may be inserted in any cell as well</li> <li><code>INSERT into &lt;table&gt;(tuple) values (tuple)</code> - inserts into some rows only</li> <li>you can also insert the result of one query into another table</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/10..database_modification/#update","title":"Update","text":"<ul> <li><code>UPDATE &lt;table&gt; SET &lt;attribute&gt; = &lt;value&gt;</code></li> <li><code>UPDATE instructor SET salary = 1.05 * salary</code> is a valid query</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/10..database_modification/#case","title":"Case","text":"<ul> <li>case is used to implement an if else <pre><code>UPDATE instructor\n    SET salary = CASE\n        WHEN salary &lt;= 100000 THEN salary * 1.05\n                              ELSE salary * 1.03\n        END\n</code></pre></li> </ul>"},{"location":"CSE202_DBMS/4..SQL/11..create_table/","title":"Create Table","text":"<p>The CREATE command is used to create any table:</p> <pre><code>    CREATE TABLE &lt;name&gt; (\n        A1  D1,\n        A2  D2,\n        A3  D3,\n        &lt;integrity constraint 1&gt;,\n        &lt;integrity constraint 2&gt;\n    );\n</code></pre> <ul> <li>A\\(\\,_i\\) is an attribute of the table. D\\(\\,_i\\) is the domain / datatype of said attribute</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/11..create_table/#integrity-constraints","title":"Integrity Constraints","text":"<ul> <li>Integrity constraints tells us some information about the attribute that decides its integrity. They include:<ul> <li>primary key (\\(A_1, A_2, ... A_n\\))</li> <li>foreign key (\\(A_m, .... A_n\\))</li> <li>not null</li> </ul> </li> </ul> <pre><code>    CREATE TABLE instructor (\n        ID          char(5),\n        name        varchar(20) NOT NULL,\n        dept_name   varchar(20),\n        salary      numeric(8, 2),\n\n        primary key (ID),\n        foreign key (dept_name) references department\n    );\n</code></pre>"},{"location":"CSE202_DBMS/4..SQL/12..data_definition_language/","title":"Data Definition Language","text":"<p>The DDL allows the specification of information about relations. including: * The schema for each relation * The datatype of each attribute * The integrity constraints for each attribute * The indices to maintain for each relation * The security and authorisation details for each relation * The amount of disk storage reserved for each relation</p>"},{"location":"CSE202_DBMS/4..SQL/2..data_manipulation/","title":"Data Manipulation","text":""},{"location":"CSE202_DBMS/4..SQL/2..data_manipulation/#insert","title":"Insert","text":"<ul> <li><code>INSERT INTO &lt;table&gt; VALUES (&lt;data tuple&gt;)</code></li> </ul>"},{"location":"CSE202_DBMS/4..SQL/2..data_manipulation/#delete","title":"Delete","text":"<ul> <li>Remove all tuples from a realtion.</li> <li><code>DELETE FROM &lt;table&gt;</code></li> </ul>"},{"location":"CSE202_DBMS/4..SQL/2..data_manipulation/#drop-table","title":"Drop Table","text":"<ul> <li><code>DROP TABLE r</code></li> </ul>"},{"location":"CSE202_DBMS/4..SQL/2..data_manipulation/#alter","title":"Alter","text":"<ul> <li><code>ALTER TABLE r ADD &lt;A&gt; &lt;D&gt;</code><ul> <li>All new cells are assigned <code>null</code></li> </ul> </li> <li><code>ALTER TABLE r DROP &lt;A&gt;</code> <ul> <li>Drops an attribute from a table</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/4..SQL/3..SQL_query/","title":"SQL Query","text":""},{"location":"CSE202_DBMS/4..SQL/3..SQL_query/#basic-structure","title":"Basic Structure","text":"<pre><code>    SELECT A1, A2, ... An\n    FROM r1, r2, ... rm\n    WHERE P\n</code></pre> <p>The result of this query is a relation (a table)</p>"},{"location":"CSE202_DBMS/4..SQL/4..select/","title":"Select","text":""},{"location":"CSE202_DBMS/4..SQL/4..select/#select_1","title":"SELECT","text":"<ul> <li>SELECT lists all the mentioned attributes from the result of a query</li> <li>corresponds to project (\\(\\Pi\\)) from relational algebra</li> <li>note that names are case-insensitive</li> <li>by default, select chooses duplicate entries as well</li> <li>use DISTINCT to specify that duplicates should not be selected, and ALL to specify that they ahould be</li> <li>an asterisk denotes all attributes</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/4..select/#slightly-more-funky-stuff","title":"Slightly more funky stuff","text":"<ul> <li>the select parameter can be a literal. If it is so, then it will return a table with one column and one row for each entry in the table, and each entry is filled with the given literal</li> <li>you can perform arithmetic operations on attributes while selecting them</li> <li>use the <code>AS</code> keyword to rename anything you want to select</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/5..string_operations/","title":"String Operations","text":""},{"location":"CSE202_DBMS/4..SQL/5..string_operations/#like","title":"LIKE","text":"<ul> <li>the like keyword is used to match a pattern in SQL</li> <li><code>%</code> matches any substring, while <code>_</code> matches any character</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/5..string_operations/#escape","title":"ESCAPE","text":"<ul> <li>the escape keyword is used to clarify the escape character in the string</li> <li><code>LIKE '100 \\%' ESCAPE '\\'</code> tells us to escape the % sign</li> </ul> <p>### Miscellaneous  * concatenation can be done with <code>||</code>  * case can be changed with <code>LOWER()</code> and <code>UPPER()</code>  * we can find string length or take substrings too</p>"},{"location":"CSE202_DBMS/4..SQL/6..where%2C_from%2C_and_others/","title":"Where, From, And Others","text":""},{"location":"CSE202_DBMS/4..SQL/6..where%2C_from%2C_and_others/#where","title":"WHERE","text":"<ul> <li>the where clause tells us the predicate to pick entries</li> <li>relational and logical operators may be used in the predicate</li> <li>we can also compare tuples against tuples, i.e. WHERE (A, B) &lt; (C, D)</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/6..where%2C_from%2C_and_others/#from","title":"FROM","text":"<ul> <li>the from clause gives us the domain of the query</li> <li><code>instructor X teaches</code> becomes <code>FROM instructor, teaches</code></li> <li>common attributes get a prefix of  at the start of their name"},{"location":"CSE202_DBMS/4..SQL/6..where%2C_from%2C_and_others/#as","title":"AS","text":"<ul> <li>the <code>AS</code> operator can be used to rename pretty much anything</li> <li>it can also be omitted giving the same result. <code>instructor AS T</code> and <code>instructor T</code> do the same thing</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/6..where%2C_from%2C_and_others/#order-by","title":"ORDER BY","text":"<ul> <li>ORDER BY tells SQL the order to return result rows of the query in</li> <li>can be done on multiple attributes for primary and secondary sorting keys</li> <li><code>desc</code> or <code>asc</code> can be put at the end of each sorting key to specify the order of sorting</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/6..where%2C_from%2C_and_others/#between-and","title":"Between / and","text":"<ul> <li>the between operator is a really dumb way to use \\(\\geq\\) and \\(\\leq\\) together. </li> <li><code>WHERE salary between 90000 and 100000</code></li> </ul>"},{"location":"CSE202_DBMS/4..SQL/7..set_operations/","title":"Set Operations","text":""},{"location":"CSE202_DBMS/4..SQL/7..set_operations/#and-not-or","title":"And, Not, Or","text":"<ul> <li>these are put inside the <code>WHERE</code> or <code>HAVING</code> clauses</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/7..set_operations/#union-intersection","title":"UNION, INTERSECTION","text":"<ul> <li>these are put between two <code>SELECT</code> clauses</li> <li>append <code>all</code> to the keyword to retain duplicates</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/7..set_operations/#impact-of-null-values","title":"Impact of null values","text":"<ul> <li>the result of any comparison with a null value is returned as <code>unknown</code></li> <li>false and unknown is false, true and unknown is true (absorption law)</li> <li>every other comparison returns unknown</li> <li>if unknown is returned to the <code>WHERE</code> clause, then it is treated as false.</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/8..aggregation_functions/","title":"Aggregation Functions","text":"<p>Aggregation functions perform some operations on a set of rows. These include</p> <ul> <li>Avg </li> <li>Min</li> <li>Max</li> <li>Sum</li> <li>Count</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/8..aggregation_functions/#example","title":"Example","text":"<pre><code>SELECT count(*) FROM my_table\n</code></pre>"},{"location":"CSE202_DBMS/4..SQL/8..aggregation_functions/#group-by","title":"GROUP BY","text":"<p>GROUP BY keyword groups entries by some equality on some attribute * note that whichever attribute you choose to group by must be present in the SELECT clause</p>"},{"location":"CSE202_DBMS/4..SQL/8..aggregation_functions/#having","title":"HAVING","text":"<p>HAVING keyword is the same as where, except it acts after group by</p>"},{"location":"CSE202_DBMS/4..SQL/8..aggregation_functions/#example_1","title":"Example","text":"<pre><code>SELECT dept_name, avg(salary) as avg_salary\nFROM instructor\nGROUP BY dept_name\nHAVING avg(salary) &gt; 42000;\n</code></pre>"},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/","title":"Nested And Set Queries","text":""},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/#nested-queries","title":"Nested Queries","text":"<p>in </p> <pre><code>SELECT A1, A2, ... An\nFROM r1, r2, ... rn\nWHERE P\n</code></pre> <ul> <li>one can replace \\(r_i\\)  with any valid subquery</li> <li>an operation can be used on a subquery for the predicate in WHERE</li> <li>\\(A_i\\) can be replaced with any subquery that generates a single value</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/#in","title":"IN","text":"<ul> <li>If a query returns a set (i.e., single column), we can check if a member is present in it using the <code>IN</code> keyword</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/#example","title":"Example","text":"<pre><code>SELECT course_ID\nFROM course_offerings\nWHERE course_year=2018 and course_ID in\n    (SELECT course_ID\n     FROM course_offerings\n     WHERE course_year=2017)\n</code></pre>"},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/#some-all-clauses","title":"Some, All clauses","text":"<ul> <li>The some clause selects all elements that fulfil a condition with some elements in a set</li> <li>The all clause selects all elements that fulfil a condition with all elements of a set</li> </ul>"},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/#example_1","title":"Example","text":"<pre><code>SELECT name\nFROM instructor\nWHERE dept_name = 'Physics' AND\n    salary &gt; ALL (\n        SELECT salary\n        FROM instructor\n        WHERE dept_name = 'Biology'\n    )\n</code></pre> <p>This query selects physics professors who earn more than every biology professor</p>"},{"location":"CSE202_DBMS/4..SQL/9..nested_and_set_queries/#exists-not-exists-except-clauses","title":"Exists, Not exists,  Except clauses","text":"<pre><code>SELECT course_ID\nFROM section as S\nWHERE \n    semester = 'Fall' \n    and course_year = 2017 \n    and EXISTS (\n        SELECT *\n        FROM section as T\n        WHERE\n            semester = 'Spring'\n            and course_year = 2018\n            and S.course_ID = T.course_ID\n    )\n</code></pre> <p>EXCEPT is just a set difference. A EXCEPT B returns A - B</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/1..views/","title":"Views","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/1..views/#creation","title":"Creation","text":"<p><pre><code>CREATE view &lt;view name&gt; (columns) as &lt;query&gt;\n</code></pre>  creates a view</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/1..views/#updation","title":"Updation","text":"<p><pre><code>INSERT INTO &lt;view name&gt;\n    VALUES (tuple)\n</code></pre> may add the tuple to the view. The view can * insert the tuple * reject the tuple</p> <p>this is really bad because the tuple's insert condition can be super ultra ambiguous</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/","title":"Joins","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/#join","title":"Join","text":"<ul> <li>a join operation takes two relations and returns the result of their merging</li> <li>a join operation is a cartesian product that is followed by the application of some condition</li> <li>the result is frequently used in the FROM clause</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/#natural-join","title":"Natural Join","text":"<ul> <li>the natural join returns tuples which have the same values for all common attributes, and retains a single copy of each common column</li> <li>the natural join is the simplest way to normalise</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/#example","title":"Example","text":"<p>table customer</p> customer_ID customer_name 100 Hello 200 World <p>table seller</p> seller_ID customer_ID seller_name 500 100 Panda 560 200 Cat <pre><code>SELECT * \nFROM customer NATURAL JOIN seller\n</code></pre> <p>returns</p> customer_ID seller_ID customer_name seller_name 100 500 Hello Panda 200 560 World Cat"},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/#outer-join","title":"Outer Join","text":"<ul> <li>outer join is a type of join that involves zero information loss</li> <li>all rows are preserved between relations, adding null values if needed</li> <li>we can use left outer join, right outer join, or full outer join</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/#inner-join","title":"Inner Join","text":"<ul> <li>inner join is the default type of join, unless mentioned otherwise</li> <li>inner join preserves consistency (no nulls present), but may cause loss of data</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/2..joins/#example_1","title":"Example","text":"customer_ID customer_name 100 Hello 200 World 300 Is 400 A 500 Square seller_ID seller_name customer_ID 150 Green 100 250 Is 200 350 My 600 450 Pepper 800 <pre><code>seller INNER JOIN customer ON seller.customer_ID = customer.customer_ID\n</code></pre> seller_ID seller_name customer_ID customer_name 150 Green 100 Hello 250 Is 200 World <p><pre><code>seller LEFT JOIN customer ON seller.customer_ID = customer.customer_ID\n</code></pre> (left join is shorthand for left outer join)</p> seller_ID seller_name customer_ID customer_name 150 Green 100 Hello 250 Is 200 World 350 My null null 450 Pepper null null <pre><code>seller FULL OUTER JOIN customer ON seller.customer_ID = customer.customer_ID\n</code></pre> seller_ID seller_name customer_ID customer_name 150 Green 100 Hello 250 Is 200 World 350 My null null 450 Pepper null null null null 300 Is null null 400 A null null 500 Square"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/","title":"Integrity Constraints","text":"<p>An integrity constraint guards your databse from accidental damage by checking all inserted data for consistency</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#constraints-on-a-single-relation","title":"Constraints on a single relation","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#not-null","title":"not null","text":"<ul> <li>an attribute cannot be null. </li> <li><code>name varchar(20) NOT NULL</code></li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#unique","title":"unique","text":"<ul> <li>no two values under this attribute can be equal</li> <li>this makes the attribute a candidate key</li> <li>this candidate key is allowed to be null, as opposed to a primary key</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#check","title":"check","text":"<ul> <li>ensures that any insertion obeys a certain predicate</li> <li><code>check (season in ('Fall', 'Winter', 'Spring', 'Summer'))</code></li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#referential-integrity","title":"Referential Integrity","text":"<ul> <li>ensures that each foreign key has two ends. </li> <li>we do this by defining attribute A in table R as a foreign key pointing to primary key P in table S</li> <li><code>FOREIGN KEY (dept_name) REFERENCES department (dept_name)</code> is how we declare a foreign key</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#integrity-violation-response","title":"Integrity violation response","text":"<ul> <li>In response to any violation of referential integrity, the DBMS can respond in the following ways<ul> <li>reject the action that causes the violation</li> <li>cascade that action to the referencers</li> <li>set the value to null</li> <li>set the value to a preset default</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/3..integrity_constraints/#inserting-without-causing-violations","title":"Inserting without causing violations","text":"<p>You can do this by: * insert parent before child * set parents to null initially, update on inserting child * defer constraint checking</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/4..indices/","title":"Indices","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/4..indices/#index","title":"Index","text":"<ul> <li>an index allows you to fetch a row by that attribute in a query in O(1) time</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/4..indices/#example","title":"Example","text":"<pre><code>CREATE INDEX student_ID_index on student(ID)\n</code></pre> <p>after this</p> <pre><code>SELECT *\nFROM student\nWHERE ID = 12345\n</code></pre> <p>will run in O(1)</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/","title":"Miscellaneous Datatypes","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#datetime","title":"Datetime","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#date","title":"Date","text":"<ul> <li>stored in <code>YYYY-MM-DD</code></li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#time","title":"Time","text":"<ul> <li>stored in <code>HH:MM:SS</code></li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#timestamp","title":"Timestamp","text":"<ul> <li>stored in <code>YYYY-MM-DD HH:MM:SS</code></li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#interval","title":"Interval","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#an-interval-between-two-timestamps","title":"* an interval between two timestamps","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#large-entries","title":"Large Entries","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#blob","title":"Blob","text":"<ul> <li>binary large object - a large collection of uninterpreted binary data</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#clob","title":"Clob","text":"<ul> <li>character large object - a humongous string</li> </ul> <p>both of these return a pointer to the data</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#user-defined-types","title":"User-Defined Types","text":"<p><pre><code>CREATE TYPE dollars AS numeric (12,2) FINAL\n</code></pre> creates a type dollars which can be used in other queries</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/5..miscellaneous_datatypes/#domains","title":"Domains","text":"<ul> <li>domains are like types that can have constraints</li> </ul> <pre><code>CREATE DOMAIN degree_level VARCHAR(10) \n    CONSTRAINT degree_level_test \n        CHECK (value in ('Bachelors', 'Masters', 'Doctorate'))\n</code></pre>"},{"location":"CSE202_DBMS/5..advanced_SQL/6..authorisation/","title":"Authorisation","text":""},{"location":"CSE202_DBMS/5..advanced_SQL/6..authorisation/#data-access-privileges","title":"Data Access Privileges","text":"<ul> <li>Read / Select - can read data, but not modify it</li> <li>Insert - can insert data, but not modify existing data</li> <li>Update - allows modification, but not deletion of data</li> <li>Delete - allows deletion of data</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/6..authorisation/#schema-modification-privileges","title":"Schema Modification Privileges","text":"<ul> <li>Index - allows the creation and deletion of indices</li> <li>Resources - allows the creation of new relations</li> <li>Alteration - allows addition or deletion of attributes within a relation</li> <li>Drop - allows deletion for relation</li> <li>Reference - allows the user to create a foreign key referencing this table</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/6..authorisation/#grant-revoke","title":"GRANT / REVOKE","text":"<ul> <li>the GRANT keyword is used to give some privileges to some user<ul> <li><code>GRANT &lt;privilege list&gt; ON &lt;relation / view&gt; TO &lt;user list&gt;</code></li> </ul> </li> <li>the REVOKE keyword has the opposite function</li> <li>any privilege between select, insert, update, delete, and <code>all</code> can be granted to a user, on a certain table or view</li> <li>the granter must already have the privilege that they are granting</li> <li>the default role is <code>public</code>. it includes all users without a role</li> <li>revoke may contain cascade or restrict at the end, which decides how to handle permission dependency conflicts</li> </ul> <p>### Roles  * an SQL role is like a discord server role  * <code>CREATE role &lt;name&gt;</code> creates a role  * <code>GRANT &lt;role&gt; to &lt;users&gt;</code> grants this role to some users</p>"},{"location":"CSE202_DBMS/5..advanced_SQL/7..transaction/","title":"Transaction","text":"<ul> <li>a transaction is the SQL unit of work</li> <li>it consists of queries, updates, etc.</li> <li>the transaction must end with<ul> <li>Commit - store updates made by the transaction</li> <li>Rollback - ignore any updates made by the transaction</li> </ul> </li> <li>an atomic transaction is one that is either fully executed or fully rolled back</li> </ul>"},{"location":"CSE202_DBMS/5..advanced_SQL/7..transaction/#acid-properties","title":"ACID properties","text":"<ul> <li>Atomicity - happens as a whole or none at all</li> <li>Consistency - consistency is maintained before and after the consistency</li> <li>Isolation - transactions can occur independently of each other simultaneously</li> <li>Durability - changes are written to the permanent storage even if power failure occurs during writing</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/1..for-each_loops/","title":"For-each Loops","text":""},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/1..for-each_loops/#for-each-loops_1","title":"For-Each Loops","text":"<p>For-Each loops allow us to iterate over all results of a query.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/1..for-each_loops/#example","title":"Example","text":"<pre><code>DECLARE n integer default 0;\nFOR r AS\n    SELECT budget\n    FROM department\n    WHERE dept_name = 'Music'\nDO\n    SET n = n + r.budget\nEND FOR\n</code></pre>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/2..external_subroutines/","title":"External Subroutines","text":"<p>SQL allows us to define functions that run in external languages, like Java, C#, C, or C++ * These may be more efficient than SQL functions, or be able to perform operations not possible in SQL</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/2..external_subroutines/#example","title":"Example","text":"<pre><code>CREATE PROCEDURE dept_count_proc(IN dept_name varchar(20)\n                                 OUT dept_count integer)\nLANGUAGE C\nEXTERNAL NAME '/usr/yash/bin/dept_count_proc'\n\nCREATE FUNCTION dept_count(dept_name varchar(20))\nRETURNS integer\nLANGUAGE C\nEXTERNAL NAME '/usr/yash/bin/dept_count'\n</code></pre>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/2..external_subroutines/#security-measures","title":"Security Measures","text":"<p>It is recommended to use sandboxing while calling an external subroutine. Java doesn't allow the subroutine to access or damage other parts of the database code, by wrapping this code in a sandbox.</p> <p>Alternatively, we can run this external processing in a separate process, and communicate information via IPC methods.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/3..windowing/","title":"Windowing","text":"<p>Windowing is a process that allows us to smooth out random variations. A good example is a moving average style of windowing.</p> <pre><code>SELECT sale_date, SUM(value) OVER\n    (ORDER BY sale_date BETWEEN rows 1 preceding and 1 following)\nFROM sales\n</code></pre>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/3..windowing/#alternative-windowing-style","title":"Alternative windowing style","text":"<ul> <li>between rows unbounded preceding and current</li> <li>rows unbounded preceding</li> <li>range between between 10 preceding and current row</li> <li>range interval 10 day preceding</li> </ul> <p>Windowing can also be done within partitions</p> <pre><code>SELECT account_number, date_time,\n    SUM(value) OVER\n        (PARTITION BY account_number\n        ORDER BY date_time\n        rows unbounded preceding)\nAS balance\nFROM transaction\nORDER BY account_number, date_time  \n</code></pre>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/4..functions/","title":"Functions","text":"<p>SQL functions fill the same role as functions in any other language. They allow you to generalise a set of queries. You can enter a parameter into a function and the function will fit it into the query.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/4..functions/#example","title":"Example","text":"<pre><code>CREATE FUNCTION dept_count (dept_name varchar(20))\n    returns integer\n    begin\n        DECLARE d_count integer;\n        SELECT \n            COUNT(*) INTO d_count\n            FROM instructor\n            WHERE instructor.dept_name = dept_name\n        RETURN d_count\n    end\n</code></pre> <p>It is also possible to return a table with a function, although this isn't supported in MySQL.</p> <pre><code>CREATE FUNCTION...\nreturns TABLE(\n    ID varchar(5)\n    name varchar(20)\n    dept_name varchar(20)\n    salary numeric(8,2)\n)\nreturn TABLE(&lt;QUERY&gt;)\n</code></pre>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/5..trigger/","title":"Trigger","text":"<p>A trigger is something that the system executes automatically when it sees some change in the database. The two key parts of a trigger are:</p> <ul> <li>The triggering condition. This can be update based or time based</li> <li>The action to perform when said condition is satisfied</li> </ul> <pre><code>CREATE TRIGGER credits_earned AFTER UPDATE OF takes ON (grade)\nREFERENCING new row as nrow\nREFERENCING old row as orow\nFOR EACH row\nWHEN nrow.grade &lt;&gt; 'F' AND nrow.grade is not null\n    AND (orow.grade = 'F' OR orow.grade is null)\nBEGIN ATOMIC\n    UPDATE student\n    SET tot_cred = tot_cred + (\n    SELECT credits\n    FROM course\n    WHERE course.course_id = nrow.course_id)\nEND\n</code></pre> <p>A trigger can be executed on a statement level instead of a row level too.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/5..trigger/#use-cases","title":"Use Cases","text":"<ul> <li>Maintaining summary data</li> <li>Replicating databases by recording delta relations (VCS basically)</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/5..trigger/#alternatives","title":"Alternatives","text":"<ul> <li>Materialised views are good for maintaining summaries</li> <li>Databases provide built-in replication support</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/5..trigger/#risks","title":"Risks","text":"<ul> <li>Loading data from a backup copy by accident</li> <li>Replicating updates where not needed</li> <li>Cascading execution</li> <li>Error from failure of critical transactions that set off the trigger</li> </ul> <p>Triggers may be disabled before taking such a risk</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/6..ranking/","title":"Ranking","text":"<p>Ranking is possible with a regular SQL query</p> <pre><code>SELECT ID, (1+(SELECT COUNT(*)\n               FROM student_grades B\n               WHERE B.GPA &gt; A.GPA)) as s_rank\nFROM student_grades A\nORDER BY s_rank\n</code></pre> <p>but this is inefficient and ugly. SQL has a better alternative built-in</p> <pre><code>SELECT ID, dept_name, \n    rank() OVER (partition by dept_name ORDER BY GPA desc)\n    AS dept_rank\nFROM dept_grades\nORDER BY dept_name, dept_rank\n</code></pre> <p>Finds the rank of students within their own department</p> <p>Note that ranking is done after the group by clause. It can be used to find top n ranks, while being better than the limit keyword because it supports ranking within partitions as well.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/6..ranking/#other-ranking","title":"Other Ranking","text":"<ul> <li>percent_rank - calculated the rank within partition</li> <li>cume_dist - cumulative distribution</li> <li>row_number - returns the row number, random in presence of duplicates</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/6..ranking/#nulls-in-order-by","title":"nulls in ORDER BY","text":"<ul> <li>SQL allows the user to add <code>nulls first</code> or <code>nulls last</code> at the end of an order by to choose where to place them.</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/6..ranking/#ntile","title":"ntile","text":"<p>ntile allows breaking a table into n buckets <pre><code>SELECT ID, ntile(4) OVER (ORDER BY GPA desc) AS quartile\n    FROM student_grades\n</code></pre></p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/","title":"OLAP","text":""},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#oltp","title":"OLTP","text":"<p>Online Transaction Protocol is what SQL uses to perform transactions - commits, rollbacks, etc.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#olap_1","title":"OLAP","text":"<p>Online Analysis Protocol, however, is used to analyse existing data. Types of OLAP are</p> <ul> <li>Multi-Dimensional OLAP (MOLAP) - store database cubes in multidimensional arrays</li> <li>Relational OLAP (ROLAP) - uses relational databases with an OLAP engine</li> <li>Hybrid OLAP (HOLAP) - store some summaries in memory and some in a relational database</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#multi-dimensional-data","title":"Multi-Dimensional Data","text":"<p>Data that can be expressed as axes and values along those axes is referred to as multi-dimensional data.</p> <ul> <li>Measure attributes - they measure some value, and can be aggregated upon</li> <li>Dimension attributes - they define the dimensions on which measure attributes are viewed. Think of them as axis labels</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#data-cube","title":"Data Cube","text":"<p>A Data Cube places data of a cross-tab in three axes. Cross-Tabs can be used as views on a data cube. In SQL we use <code>GROUP BY CUBE</code> to select as a cube.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#cross-tabs","title":"Cross-Tabs","text":"<p>Cross-Tabs are multi-indexed tables, where one category has many rows inside it. It is possible to drill up or roll down on a hierarchy.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#rollup","title":"Rollup","text":"<p>The rollup operation is similar to group-by, but has slight differences. If you use <code>ROLLUP(c1, c2, c3)</code>, it gives you the union of <code>{(c1, c2, c3), (c1, c2), (c1), ()}</code> This implies a sort of hierarchy between the columns. It can be used to generate multi-level aggregates.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/7..OLAP/#olap-operations","title":"OLAP operations","text":"<ul> <li>Pivoting - changing the dimension in a cross-tab</li> <li>Slice - create cross tab for fixed values only</li> <li>Rollup - moving from finer granularity to coarser granularity</li> <li>Drill Down - moving from coarser granularity to finer granularity</li> </ul>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/8..recursive_queries/","title":"Recursive Queries","text":"<p>Recursive queries are queries which call themselves within their contents.</p>"},{"location":"CSE202_DBMS/6..even_more_advanced_SQL/8..recursive_queries/#example","title":"Example","text":"<pre><code>WITH RECURSIVE rec_prereq(course_id, prereq_id) AS (\n    SELECT course_ID, prereq_ID\n    FROM prereq\nUNION\n    SELECT rec_prereq.course_ID, prereq.prereq_ID\n    FROM rec_rereq INNER JOIN prereq \n    ON rec_prereq.prereq_ID = prereq.course_ID\n)\n\nSELECT *\nFROM rec_prereq\n</code></pre>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/1..entity_set_corelations/","title":"Entity Set Corelations","text":""},{"location":"CSE202_DBMS/7..reduction_to_schemas/1..entity_set_corelations/#specialisation","title":"Specialisation","text":""},{"location":"CSE202_DBMS/7..reduction_to_schemas/1..entity_set_corelations/#minimal-approach","title":"Minimal Approach","text":"<pre>7abd5f29-e66b-4a40-b35a-51c55a955b8d</pre> <p>feline(ID, fur_colour) cat(ID, meowing_pitch) leopard(ID, teeth_sharpness)</p> <p>The ID gives us access to the fur colour attribute of each feline. Two objects exist for each feline, the child class and the parent class</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/1..entity_set_corelations/#normalised-approach","title":"Normalised Approach","text":"<pre>5b5cb5da-ce7e-4e59-96b0-ec681ce4486e</pre> <p>cat(ID, meowing_pitch) wet_cat(ID, meowing_pitch, dampness_constant) spotted_cat(ID, meowing_pitch, spots_size)</p> <p>This layout gives us access to all needed attributes directly. But imagine the case in which a cat is both a wet cat and a spotted cat. In a situation like this, we see redundancy. The attribute meowing_pitch is defined for both wet_cat and spotted_cat, but only needs to be used once.</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/1..entity_set_corelations/#aggregation","title":"Aggregation","text":"<pre>beedd957-10e5-4d4a-a9ad-b4c2d29a7143</pre> <p>Here, is_given has the schema</p> <p>is_given(writer_id, guide_ID, remarks)</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/2..relation_schema/","title":"Relation Schema","text":"<p>A schema refers to a bunch of tables incorporated in a database</p> <ul> <li>Any database with a valid E-R specification can be converted into a collection of schema</li> <li>Note that the schema gives information about the columns in the table, but not the data inside the table</li> </ul>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/3..entity_sets/","title":"Entity Sets","text":""},{"location":"CSE202_DBMS/7..reduction_to_schemas/3..entity_sets/#strong-entity-set","title":"Strong Entity Set","text":"<p>A strong entity set is reduced to a schema containing the attributes of the set</p> <pre>1fb379e1-f022-4555-872e-7df90e1ab6a1</pre> <p>soldier(soldier_ID, experience, weapon_id)</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/3..entity_sets/#weak-entity-set","title":"Weak Entity Set","text":"<p>A weak entity set is reduced to a schema containing its attributes, including the primary key of its identifying set</p> <pre>64afb3ad-cdbd-4bf7-aa09-40970a8ba013</pre> <p>expertise(accuracy, consistency, ease_of_manuoevering, weapon_ID)</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/4..composite_attributes/","title":"Composite Attributes","text":"<ul> <li>Composite attributes are flattened into their sub-attributes</li> <li>One possible way is to use a form of Hamiltonian Notation, adding the name of the parent attribute as a prefix for each child attributes</li> <li>The prefix may be omitted if it is redundant, but if you choose proper names for child attributes then it shouldn't be</li> </ul>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/4..composite_attributes/#example","title":"Example","text":"<pre>ceaaee4c-e469-4549-ba90-260df7e77f49</pre> <p>soldier(soldier_ID, name, equipped_weapon,  b_his_count, b_his_victories, b_his_enemies_defeated)</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/5..multivalued_attributes/","title":"Multivalued Attributes","text":"<ul> <li>Multivalued attributes are not included in the same table</li> <li>They are included in another table, which contains necessary information</li> </ul>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/5..multivalued_attributes/#example","title":"Example","text":"<pre>7781d247-1035-46a1-b7bd-1e3242051abd</pre> <p>soldier(soldier_ID) ranks(soldier_ID, rank)</p> <p>so, if soldier A has rank A in weapon X, and rank B in weapon Y. Then the the ranks table is filled as</p> <p>(A, A, X) (A, B, Y)</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/6..relationship_sets/","title":"Relationship Sets","text":""},{"location":"CSE202_DBMS/7..reduction_to_schemas/6..relationship_sets/#one-one","title":"One-One","text":"<p>A foreign key to either table is stored in the other table</p> <pre>24095a83-203f-43fa-a27e-d2cd81a87735</pre> <p>dam(dam_ID, name, river_ID) river(river_ID, name) {either one can work}</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/6..relationship_sets/#many-one","title":"Many-One","text":"<p>A foreign key to the \"one\" table is stored in the \"many\" table</p> <pre>3b7ee247-0e3d-4e59-9665-9c257faa4680</pre> <p>stream(stream_ID, width, river_ID) flows_into(stream_ID, river_ID)</p>"},{"location":"CSE202_DBMS/7..reduction_to_schemas/6..relationship_sets/#many-many","title":"Many-Many","text":"<p>A new table is made to store the information of the relationship</p> <pre>b28b1174-f5cd-412a-9575-39bdbd50a480</pre> <p>river(stream_ID, name) city(city_ID, population) flows_through(river_ID, city_ID)</p>"},{"location":"CSE202_DBMS/8..normalisation/1..normalisation_theory%2C_dependencies/","title":"Normalisation Theory, Dependencies","text":""},{"location":"CSE202_DBMS/8..normalisation/1..normalisation_theory%2C_dependencies/#normalisation-theory","title":"Normalisation Theory","text":"<ul> <li>The normalisation theory is used to decide whether or not \\(R\\) is in a \"good form\"</li> <li>If \\(R\\) is not in a good form, then it will be decomposed into relations \\(\\{R_i\\}\\) such that<ul> <li>Each relation is itself in good form</li> <li>The decomposition is lossless</li> </ul> </li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/1..normalisation_theory%2C_dependencies/#functional-dependency","title":"Functional Dependency","text":"<p>There are cosntraints on data in the real world. These constraints manifest in the database as functional dependencies. Some examples include: * Students are uniquely identified by ID * Each Student has only one name * Each Professor is only associated with one department * The department has just one budge</p> <p>If an instance of a relation satisfies these real-world constraints, we call it a legal instance. A database is called a legal instance if all relation instances in it are legal. A functional dependency is a generalisation of the notion of a key.</p>"},{"location":"CSE202_DBMS/8..normalisation/1..normalisation_theory%2C_dependencies/#example","title":"Example","text":"<p>Consider \\(\\alpha \\subseteq R\\) and \\(\\beta \\subseteq R\\). The functional dependency \\(\\alpha \\to \\beta\\)  only holds if any two tuples agreeing on a value for \\(\\alpha\\) implies these two tuples will also agree on a value for \\(\\beta\\). Consider the table</p> A B 1 4 1 5 3 7 <p>In this case, \\(B \\to A\\) holds but \\(A \\to B\\) doesn't.</p>"},{"location":"CSE202_DBMS/8..normalisation/1..normalisation_theory%2C_dependencies/#closure","title":"Closure","text":"<p>A set of functional dependencies may be logically implied by the main set. This is decided by basic logic rules. For example, \\(\\(A \\to B,\\  B \\to C,\\  thus\\  A \\to C\\)\\) The set of all functional dependencies logically implied by \\(F\\) is called the closure of \\(F\\), denoted as \\(F^+\\) </p>"},{"location":"CSE202_DBMS/8..normalisation/1..normalisation_theory%2C_dependencies/#armstrongs-axioms","title":"Armstrong's Axioms","text":"<p>These are the axioms we use to compute closures * Reflexive Rule: If \\(\\beta \\subseteq \\alpha\\), then \\(\\alpha \\to \\beta\\)  * Augmentation Rule: If \\(\\alpha \\to \\beta\\), then \\(\\gamma \\alpha \\to \\gamma \\beta\\)  * Transitivity Rule: If \\(\\alpha \\to \\beta\\), and \\(\\beta \\to \\gamma\\), then \\(\\alpha \\to \\gamma\\) </p> <p>These rules are both correct and comprehensive. Some additional derived rules include, * Union Rule: If \\(\\alpha \\to \\beta\\) and \\(\\alpha \\to \\gamma\\), then \\(\\alpha \\to \\beta \\gamma\\)  * Decomposition Rule: If \\(\\alpha \\to \\beta \\gamma\\), then \\(\\alpha \\to \\beta\\) and \\(\\alpha \\to \\gamma\\)  * Pseudo-Transitivity Rule: If \\(\\alpha \\to \\beta\\) and \\(\\beta \\gamma \\to \\delta\\), then \\(\\alpha \\gamma \\to \\delta\\)</p>"},{"location":"CSE202_DBMS/8..normalisation/10..keys_and_attributes/","title":"Keys And Attributes","text":""},{"location":"CSE202_DBMS/8..normalisation/10..keys_and_attributes/#superkey-and-key","title":"Superkey and Key","text":"<ul> <li>Say there is a superkey \\(K\\) that identifies an attribute each tuple in a relation \\(R\\) distinctly. For any item \\(i\\) with attributes \\(\\{A_j\\}\\) in the table, we can say that \\(\\forall A_j, K(i) \\to A_j\\) </li> <li>A key is a superkey such that there \\(\\nexists k \\subset K\\) such that \\(k\\) is a valid superkey of \\(R\\) as well</li> <li>A key may either be a primary key or a candidate key</li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/10..keys_and_attributes/#prime-attribute","title":"Prime Attribute","text":"<ul> <li>A prime attribute is a member of some candidate key</li> <li>A non-prime member is conversely not a part of any candidate key</li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/2..anomalies/","title":"Anomalies","text":""},{"location":"CSE202_DBMS/8..normalisation/2..anomalies/#deletion-anomaly","title":"Deletion Anomaly","text":"<ul> <li>Deleting something takes away information that should not be deleted</li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/2..anomalies/#insertion-anomaly","title":"Insertion Anomaly","text":"<ul> <li>The insertion of something is restricted behind the insertion of something that it shouldn't be</li> <li>You may need to add a null values to a table in poor form to add an attribute to it, and in that case the attribute would be better in its own relation</li> </ul> <p>Consider the table (ID, name, salary, dept_name, dept_fund)</p> <p>In this table, adding a new department would involve adding a new employee as well</p>"},{"location":"CSE202_DBMS/8..normalisation/2..anomalies/#update-anomaly","title":"Update Anomaly","text":"<ul> <li>Updating something can cause an update something else that shouldn't be happening</li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/3..design_guidelines/","title":"Design Guidelines","text":""},{"location":"CSE202_DBMS/8..normalisation/3..design_guidelines/#informal-guidelines-for-good-design","title":"Informal Guidelines for Good Design","text":"<ul> <li> <p>Make sure that the semantics of the attributes are clear in the schema</p> <ul> <li>Everything in an entity set except the foreign key should only describe something about the entity itself</li> </ul> </li> <li> <p>Reduce redundant information</p> <ul> <li>DRY - don't repeat yourself. This wastes space</li> </ul> </li> <li> <p>Avoid NULL value as much as possible</p> <ul> <li>null is a super-common reason that code gets messed up. avoid it wherever possible</li> </ul> </li> <li> <p>Disallow the possibility of generating spurious tuples</p> <ul> <li>This is seen in lossy reconstruction, which generates spurious (extra) tuples</li> </ul> </li> <li> <p>Extra guideline - Preserve dependencies as much as possible</p> </li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/4..lossy_and_lossless_decompositions/","title":"Lossy And Lossless Decompositions","text":"<p>Consider decomposing the relational schema \\(R\\) into \\(R_1\\) and \\(R_2\\). </p> <p>We say that this decomposition is lossless if no information is lost when we recompose the two schemas. Formally, this is written as \\(\\(\\Pi_{R1}(r) \\bowtie \\Pi_{R2}(r)=r\\)\\) On the other hand,  \\(\\(\\Pi_{R1}(r) \\bowtie \\Pi_{R2}(r) \\subset r\\)\\) is referred to as a lossless decomposition.</p> <p>(ID, name, street, city, salary) &gt; (ID, name, street, city) , (name, salary) is a lossy decomposition</p> <p>(ID, name, street, city, salary) &gt; (ID, name, street, city) , (ID, salary) is a lossless decomposition</p> <p>In general, we can get a lossless decomposition by carrying over the primary key into tables that we decompose the main table into.</p>"},{"location":"CSE202_DBMS/8..normalisation/5..normalisation/","title":"Normalisation","text":"<p>Normalisation is the process of analysing functional dependencies and primary keys so as to achieve relational schemas that adhere as closely to design guidelines as possible * It involves decomposing improper relations into relations with good forms * These decompositions must be lossless and preserve dependencies</p>"},{"location":"CSE202_DBMS/8..normalisation/5..normalisation/#the-need-for-normalisation","title":"The Need for Normalisation","text":"<ul> <li>Normalisation ensures that the database is of the highest quality, and with minimal insertion, deletion, and update anomalies</li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/6..first_normal_form/","title":"First Normal Form (1NF)","text":"<ul> <li>1NF predicates that values be as atomic as possible</li> <li>It does not allow nested relations</li> <li>It does not allow multivalued attributes to be stored in a single cell</li> <li>This is typically the baseline for any relational schema</li> </ul>"},{"location":"CSE202_DBMS/8..normalisation/7..third_normal_form/","title":"Third Normal Form (3NF)","text":"<ul> <li>3NF mandates that no non-prime attribute \\(A\\) in \\(R\\) is transitively dependent on the primary key</li> <li>In somewhat poetic terms, each attribute must tell something about a key, a whole (candidate) key, and nothing but that key</li> <li>1NF guarantees that there is a key. 2NF guarantees that every attribute tells something about a whole key. 3NF guarantees that every attribute tells something about a key, and about nothing else in the table.</li> <li>If \\(X \\to A\\) holds in \\(R\\), then either<ul> <li>\\(X\\) is a superkey of \\(R\\), or</li> <li>\\(A\\) is a prime attribute of \\(R\\) </li> </ul> </li> </ul> <p>Note that normalisation is only necessary if \\(A\\) is a non-prime attribute of r. Consider the scenario with the relation <code>Employee(SSN, emp_ID, salary)</code>. In this, while SSN -&gt; emp_ID and emp_ID -&gt; salary, this is not a concern because both are valid candidate keys</p> <p>The previous example can be converted to 3NF as follows</p> <p>The functional dependencies are</p> <p>The schemas are         Lot - (property_ID, state, lot, lot_area, base_price)         Tax - (state, tax_rate)</p> <p>lot_area is not a candidate key. <code>base_price</code> is not directly dependent on property_ID. So, it does not obey 3NF, and we will add it to a new table         property_ID -&gt; lot, lot_area, state         lot, lot_area -&gt; base_price         state -&gt; tax_rate</p> <p>The new schema is     Prices - (lot_area, price)</p>"},{"location":"CSE202_DBMS/8..normalisation/8..second_normal_form/","title":"Second Normal Form (2NF)","text":"<p>2NF predicates that in a relation \\(R\\), there is no (non-prime) attribute \\(A\\) such that, * \\(c \\to A\\) , such that \\(c \\subset C\\) , where C is a candidate key * It means that each attribute is dependent on the whole candidate key</p> <p>Consider the following schema         Lot - (property_ID, , lot_area, base_price, tax_rate) <p>Here are the functional dependencies         property_ID -&gt; lot, lot_area, state         lot, lot_area -&gt; base_price         state -&gt; tax_rate</p> <p>State, lot is a candidate key. But tax_rate is only dependent on a part of this candidate key, so we will move it to a new table.         Lot - (property_ID, , lot_area, base_price)         Tax - (state, tax_rate) <p>This schema in 2NF now.</p>"},{"location":"CSE202_DBMS/8..normalisation/9..boyce-codd_normal_form/","title":"Boyce-codd Normal Form","text":""},{"location":"CSE202_DBMS/8..normalisation/9..boyce-codd_normal_form/#boyce-codd-normal-form-bcnf","title":"Boyce-Codd Normal Form (BCNF)","text":"<p>In Boyce-Codd Normal Form, for any dependency \\(X \\to A\\), one of the following must be obeyed * \\(A \\subset X\\) * \\(X\\) is a superkey for \\(R\\)</p> <ul> <li>While 3NF mandates that all non-prime attributes depend on nothing but a key, BCNF mandates this on prime attributes as well, unless they by themselves count as a superkey</li> </ul> <p>This format is somewhat esoteric, you won't see it very frequently.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/1..file_organisation/","title":"File Organisation","text":"<ul> <li>A database is, at some level, a collection of files. Each file has records in it. Each record has fields in it.</li> <li>The way in which we organise files depends upon the model we use for our set of files. Each model comes with some assumptions and some constraints.</li> </ul>"},{"location":"CSE202_DBMS/9..storage_-_software/2..fixed-length_records/","title":"Fixed-length Records","text":""},{"location":"CSE202_DBMS/9..storage_-_software/2..fixed-length_records/#assumptions","title":"Assumptions","text":"<ul> <li>Each record fits within some fixed size.</li> <li>Each cell of each record fits within the column width.</li> </ul> <p>Records are stored next to each other in the memory. Some padding may be used to prevent records from spreading over block boundaries.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/2..fixed-length_records/#deletion-approaches","title":"Deletion Approaches","text":"<ul> <li>Move all records below the row upwards to fill up the space.</li> <li>Move the last record to fill up the space.</li> <li>Keep a list of all free rows, which is ready to be filled whenever a new row has to be added.</li> </ul>"},{"location":"CSE202_DBMS/9..storage_-_software/3..variable-length_records/","title":"Variable-length Records","text":""},{"location":"CSE202_DBMS/9..storage_-_software/3..variable-length_records/#variable-length-records_1","title":"Variable Length Records","text":"<ul> <li>These occur in databases in several ways. The data fields might be heterogenous, and there may be variations of size between even the same datatype.</li> <li>Attributes are still stored in order.</li> </ul>"},{"location":"CSE202_DBMS/9..storage_-_software/3..variable-length_records/#slotted-page-structure","title":"Slotted Page Structure","text":"<ul> <li>In this structure, there are two segments - the header and the records. </li> <li>The header tells us about a record as a (offset, length) tuple.</li> <li>Records may be rearranged to ensure they are contiguous with minimal empty space between them. The header will be rearranged according to this. </li> <li>A pointer to the entry will not point to a record, but its header.</li> <li>The end of the header section has a single number, which tells the amount of free space in the block.</li> </ul>"},{"location":"CSE202_DBMS/9..storage_-_software/4..sequential_file_organisation/","title":"Sequential File Organisation","text":"<p>Each record is stored next to each other, in order of the primary key value. It is, in a sense, a linked list.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/4..sequential_file_organisation/#deletion","title":"Deletion","text":"<p>Deletion is performed as done in a linked list.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/4..sequential_file_organisation/#insertion","title":"Insertion","text":"<ul> <li>If there is a free space, it is inserted there.</li> <li>If there is no free space, an overflow block is added in the middle.</li> <li>To restore sequentiality, some rearrangement time may be needed.</li> </ul>"},{"location":"CSE202_DBMS/9..storage_-_software/5..clustering_and_partitioning/","title":"Clustering And Partitioning","text":""},{"location":"CSE202_DBMS/9..storage_-_software/5..clustering_and_partitioning/#multitable-clustering","title":"Multitable Clustering","text":"<p>Multitable clustering is great for storing tables that are often joined with each other. In a cluster, tables records are stored interleaved with each other.</p> dept_name building budget CSE Taylor 100000 PHY Watson 70000 ID name dept_name salary 100 Hector CSE 65000 101 Robin PHY 57000 102 Seth CSE 58000 103 Genny CSE 70000 <p>These two tables will be merged into the cluster</p> __ __ __ __ CSE Taylor 100000 100 Hector CSE 65000 102 Seth CSE 58000 103 Genny CSE 70000 PHY Watson 70000 101 Robin PHY 57000 <p>This won't work very well when you're querying for only one table, but makes joins faster. This is stored in variable-length records</p>"},{"location":"CSE202_DBMS/9..storage_-_software/5..clustering_and_partitioning/#partitioning","title":"Partitioning","text":"<p>In partitioning, a single table \\(X\\) is broken into multiple tables \\(\\{X_i\\}\\) . Each table \\(X_i\\) can be stored separately.  * This reduces cost of some operations like free space management. * It allows different partitions to be stored on different devices, like storing more important partitions on an SSD and less important ones on a magnetic disk.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/6..data_dictionary/","title":"Data Dictionary","text":"<p>The data dictionary, also called the system catalog, stores metadata about the database. This includes * Information about relations     * names     * attributes     * views     * integrity constraints     * triggers * User and role information * Statistical data, like the number of records in each relation * Physical organisation * Location of each relation * Information about indices</p>"},{"location":"CSE202_DBMS/9..storage_-_software/7..buffer-block_replacement_policies/","title":"Buffer-block Replacement Policies","text":"<p>similar to cache replacement policies</p>"},{"location":"CSE202_DBMS/9..storage_-_software/7..buffer-block_replacement_policies/#lru","title":"LRU","text":"<ul> <li>replace the block that was used last</li> <li>the past is a predictor of the future</li> <li>can be really bad sometimes</li> </ul> <pre><code>for each tuple tr of r:\n    for each tuple ts of s:\n        if ts == tr:\n            do something\n</code></pre> <p>Typically, a mixed strategy with some hints on replacement is provided by the query optimiser</p>"},{"location":"CSE202_DBMS/9..storage_-_software/7..buffer-block_replacement_policies/#toss-immediate","title":"Toss-Immediate","text":"<p>done with a block? get rid of it.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/7..buffer-block_replacement_policies/#mru","title":"MRU","text":"<p>Keep track of the most recently used block. When the buffer is full, get rid of it.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/7..buffer-block_replacement_policies/#speculation","title":"Speculation","text":"<p>The buffer manager may speculate the next request by looking at usage statistics. It may pre-emptively store that block in the buffer. </p>"},{"location":"CSE202_DBMS/9..storage_-_software/8..buffer/","title":"Buffer","text":"<p>A buffer is the potrtion of the main memory designated to store copies of disk blocks. This reduces the number of average calls sent to the disk to fetch data.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/8..buffer/#buffer-manager","title":"Buffer Manager","text":"<p>This is the subsystem that demarcates the buffer space in the main memory. * when a block is needed from the disk, a request is sent to the buffer manager * the buffer manager behaves as a cache, and returns an address from within the buffer</p>"},{"location":"CSE202_DBMS/9..storage_-_software/8..buffer/#rearrangement","title":"Rearrangement","text":"<p>The OS or buffer manager can reorder writes to keep consistency in check, depending upon how the data structure is stored</p>"},{"location":"CSE202_DBMS/9..storage_-_software/8..buffer/#pinned-block","title":"Pinned Block","text":"<p>A pinned block cannot be written back to the disk. A block is pinned when work is started on the block, and unpinned when this work is  done. This helps in maintaining concurrency.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/8..buffer/#buffer-locks","title":"Buffer Locks","text":"<p>A reader gets a shared lock, while a writer gets an exclusive lock. This means that many readers can read simultaneously, but no writer can write as long as even one reader is reading. When a writer is writing, no one can read.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/9..column-oriented_storage/","title":"Column-oriented Storage","text":""},{"location":"CSE202_DBMS/9..storage_-_software/9..column-oriented_storage/#column-oriented-storage_1","title":"Column-Oriented Storage","text":"<p>A.k.a. columnar representation. Each attribute of a relation is stored separately.</p>"},{"location":"CSE202_DBMS/9..storage_-_software/9..column-oriented_storage/#benefits","title":"Benefits","text":"<ul> <li>Reduced IO if a limited number of attributes are accessed</li> <li>Better CPU cache performance and compression</li> <li>Good for vector processing</li> </ul>"},{"location":"CSE202_DBMS/9..storage_-_software/9..column-oriented_storage/#costs","title":"Costs","text":"<ul> <li>Tuple reconstruction from the columnar representation</li> <li>Tuple deletion and update</li> <li>Decompression</li> </ul> <p>We typically prefer row-oriented representation for transaction processing. Some databases support both styles though, and they're called hybrid row/column stores.</p> <p>When stored in memory, there's an indirection table that links each column's head to a pointer.</p>"},{"location":"CSE222_ADA/","title":"Index","text":"<p>notes for CSE222 ADA</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/","title":"Minimum Distances","text":"<p>This algorithm involves finding the closest point pair in a lattice of \\(n\\) points.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#motivation","title":"Motivation","text":"<p>This algorithm can be used to find the closest entity to a user, whether it is a cab or a restaurant, by knowing the locations of both bodies. </p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#1-d-case","title":"1-D Case","text":"<p>In 1 dimension, there's a straightforward solution to this. We can sort the points and measure the distance between each point and its adjacent point, while storing the minimum distance so far. This will give us an easy way to get the minimum distance between any pair.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#2-d","title":"2-D","text":"<p>Unfortunately such an algorithm won't work in 2D. An alternative approach helps us though.</p> <p>For the array \\(P\\), let \\(P_x\\) be \\(P\\) sorted by the x co-ordinate, and \\(P_y\\) be \\(P\\) sorted by the y co-ordinate. </p> <p>The next step is to divide \\(P_x\\) by its median. Since these points are sorted, the median can be derived in \\(O(1)\\) time.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#divide-and-conquer","title":"Divide and Conquer","text":"<p>The median divides \\(P_x\\) into \\(Q_x\\) and \\(R_x\\), and \\(P_y\\) into \\(Q_y\\) and \\(R_y\\). \\(Q_x\\) and \\(R_x\\) can be computed in \\(O(1)\\), \\(Q_y\\) and \\(R_y\\) in \\(O(n)\\) time. </p> \\[ClosestPair(P_x, P_y)\\] <p>can be defined as </p> <p>\\(ClosestOf\\) (\\(ClosestPair(Q_x, Q_y),\\) \\(ClosestPair(Q_x, Q_y), ClosestSplitPair(Q_x, Q_y, R_x, R_y)\\))</p> <p>The first two can be found recursively, the last one however, is more complex.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#closest-split-pair","title":"Closest Split Pair","text":"<p>The closest split pair can be found by such. Call the minimum distance of closest pair on either side as \\(\\delta\\). Then look at all points within a \\(\\delta\\) neighbourhood of the median. Any point outside this neighbourhood cannot possibly be the closest split pair.</p> <p>Traverse through points ahead of the top-most point. While you traverse downward, compare each point to the \\(7\\) points ahead of it. Find the situation with the minimum distance in such a scenario.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#why-7","title":"Why 7?","text":"<p>Take an arbitrary point \\(p\\). Assume there is at least one point ahead of it in a \\(\\delta\\) neighbourhood. Now draw 8 boxes of width and height \\(\\delta/2\\) below \\(p\\). Anything outside these boxes is further than \\(\\delta\\) away from p, and thus cannot be a candidate point. Furthermore, there can be only as many as 7 other points in these boxes. Any more and you have 2 in the same box, which gives us a distance between those two points of no more than \\(\\delta/\\sqrt2\\) .</p> <p>Suppose there is a point \\(q\\) less than \\(\\delta\\) away from \\(p\\) . This point must lie in one of the eight boxes. The maximum number of points we can fit between \\(q\\) and \\(p\\) is seven. Any more, and the closest point pair wil no longer be \\(p\\) and \\(q\\), but two of the internal points.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#time-complexity","title":"Time Complexity","text":"<p>Sorting Points &gt; \\(O(n.logn)\\) Closest Split Pair &gt; \\(O(n)\\)</p> <p>Recurrence &gt; \\(T(n) = 2T(n/2) + O(n)\\) Solution &gt; \\(O(n.logn)\\) from Master's Theoreom</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/1..minimum_distances/#psuedo-code","title":"Psuedo-Code","text":"<pre><code>ClosestSplitPair(Q, R):\n    select all points in delta-neighbourhood of divider\n    start with top-most point\n    min_dist = really_really_large\n    for point S in set:\n        calculate distance with 7 points ahead of S as T\n        if any of these &lt; min_dist:\n            min_dist = this\n    return min_dist \n\nClosestPair(P):\n    start with point set P\n    sort in X and Y to get Px and Py\n    divide P by median of Px, making Q and R\n    get Qx, Qy, Rx, and Ry\n\n    pair_q = ClosestPair(Q)\n    pair_r = ClosestPair(R)\n    pair_split = ClosestSplitPair(Q, R)\n    return closest(pair_q, pair_r, pair_split)\n</code></pre>"},{"location":"CSE222_ADA/1..divide_and_conquer/2..recurrences%2C_master_theorem/","title":"Recurrences, Master Theorem","text":""},{"location":"CSE222_ADA/1..divide_and_conquer/2..recurrences%2C_master_theorem/#input-size","title":"Input Size","text":"<p>The input size describes the amount of space required to describe or set up the problem. It is proportional to the amount of bits needed to store the input. The input size is the only parameter that is needed to predict Time / Space Complexity for deterministic algorithms.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/2..recurrences%2C_master_theorem/#divide-and-conquer","title":"Divide and Conquer","text":"<p>Divide and Conquer algorithms come in three steps</p> <ul> <li>Divide the problem into smaller (typically disjoint) subproblems</li> <li>Assume that you have already solved these subproblems below a certain layer</li> <li>With this help from the recursion fairy, combine these solutions to get the solution of the current layer</li> </ul>"},{"location":"CSE222_ADA/1..divide_and_conquer/2..recurrences%2C_master_theorem/#recurrence","title":"Recurrence","text":"<p>A recurrence describes the runtime of an input size of \\(n\\) in terms of the same algorithm applied to smaller input sizes, and some additional overhead (the combine step).</p> <p>Example: for Karatsuba multiplication</p> \\[ \\begin {align} \\quad &amp;T(n) = 4T \\left(\\frac{n}{2} \\right) + c_1 \\cdot n \\\\ &amp;T(1) = c \\end {align} \\] <p>\\(T(1)\\) describes the base case of the algorithm</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/2..recurrences%2C_master_theorem/#master-theorem","title":"Master Theorem","text":"<p>Define the recurrence as</p> \\[ \\quad T(n) = a \\cdot T(n/b) + c\\cdot n^d \\] <p>The master theorem gives a quick way to solve simple recurrences like this</p> \\[ \\quad T(n) =  \\begin {cases}     O(n^d \\log n)&amp; a = b^d\\\\     O(n^d)&amp; a &lt; b^d\\\\     O(n^{\\log_b a})&amp; a &gt; b^d\\\\ \\end {cases} \\]"},{"location":"CSE222_ADA/1..divide_and_conquer/2..recurrences%2C_master_theorem/#loose-proof","title":"Loose Proof","text":"<p>The number of levels in the recursion tree - \\(\\log_b n\\)</p> <p>The number of subproblems at level \\(j\\) -  \\(a^j\\)</p> <p>The subproblem size at level \\(j\\) - \\(n/b^j\\) </p> <p>Combine Phase at level \\(j\\) : </p> \\[\\quad a^j \\cdot \\left(\\frac{n}{b^j}\\right)^d \\cdot c\\] <p>So the total work is</p> \\[\\quad \\sum_{j=0}^{\\log_b(n)} n^d \\cdot c \\cdot \\left(\\frac{a}{b^d}\\right)^j\\] <p>This is a geometric progression. If \\(a=b^d\\) , the sum evaluates to \\(O(n^d \\log(n))\\) </p> <p>If \\(a &lt; b^d\\), this can be totalled as an infinite sum.</p> \\[\\quad = n^d \\cdot c \\cdot \\left(\\frac{1}{1-\\frac{a}{b^d}}\\right) = O(n^d)\\] <p>If \\(a&gt;b^d\\), we can't do the same because the ratio is &gt; 1</p> \\[\\quad=n^d \\cdot c \\cdot \\left(\\left(\\frac{a}{b^d}\\right)^{\\log_b n}-1\\right) = O(n^{\\log_ba})\\]"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/","title":"Selection","text":"<p>The Goal of this is to select the i'th smallest number in an unsorted array.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/#motivation","title":"Motivation","text":"<p>Calculating the \\(n'th\\) place of winners in a race with unsorted arrays of time, or finding the median from disorganised data.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/#basic-solution","title":"Basic Solution","text":"<p>Pick a pivot element \\(p\\). Partition the array around \\(p\\). Push elements smaller than \\(p\\) to its left and larger than \\(p\\). Suppose the desired position is \\(i\\), and the median ends up at \\(j\\) after arranging the partitions.</p> <p>If \\(i = j\\) , then the \\(p\\) is in fact, the desired element. If \\(i &gt; j\\) , then \\(p\\) must be to the left of the desired element. So, we will scan to the right of \\(p\\). If \\(i &lt; j\\), then \\(p\\) must be to the right of the desired element. So, we will scan to the left of \\(p\\).</p> <p>In the ideal scenario, \\(p\\) will be the median, allowing us to shorten our search group to half size with \\(c.n\\) workcload. This will give us a recurrence relation of </p> \\[T(n) = c.n + T(n/2)\\] <p>which evaluates to \\(O(n)\\). Great! Our job's done, now all we need to do is find the median... uh-oh.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/#the-paradox","title":"The Paradox","text":"<p>Finding the median in \\(O(n)\\) time is something that we've taken for granted, and it is unfortunately a non-trivial algorithm. But it is possible! To do so though, we need an algorithm that uses a value other than the median for a pivot.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/#the-new-median-algorithm","title":"The New Median Algorithm","text":"<p>There exists a median of \"good enough\" quality, which gives a 30-70 split in the worst-case scenario. If we use this pivot for partitioning, we can find the \\(n/2'th\\) element (i.e. the median) in \\(O(n)\\) time.</p> <p>Step 1 -&gt; Group \\(A\\) into chunks of five. Conduct matches among these which end in \\(O(n)\\) time roughly.  Step 2 -&gt; The winners of these matches, i.e. the individual medians, are taken to the next round. Step 3 -&gt; Find the actual median of these winners. This will give us the final pivot. Use this pivot to partition \\(A\\), and calculate the actual median with this pivot.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/#time-complexity","title":"Time Complexity","text":"\\[T(n) = c.n + T(n/5) + T(7n/10)\\] <p>The rearrangement across partition element, and calculating the sub-medians takes \\(c.n\\) time. \\(T(n/5)\\) is taken to calculate the median of the sub-medians. A further \\(T(7n/10)\\) is used to search for the desired element in the remaining array.</p> <p>This evaluates to a final complexity of \\(O(n)\\), which is derived purely by guessing.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/3..selection/#why-70","title":"Why 70%?","text":"<p> This table taken from wikipedia shows that the MoM (median of medians) is well.. smack in the middle here. But consider the worst case scenario. It has to be larger than the 2nd and 1st element in at least half the sets, because it's larger than half the sub-medians. This means its larger than at least 30% of the sample space. For a similar reason, it must be smaller than at least 30% of the sample space.</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/4..inversion_counting/","title":"Inversion Counting","text":"<p>Consider the sorted array,</p> \\[[1, 2, 3, 4, 5, 6, 7, 8]\\] <p>and the unsorted array,</p> \\[[4, 3, 5, 2, 7, 6, 8, 1]\\] <p>The pair \\((4,3)\\) is out of order in the unsorted array. Let us call this out-of-order-pair as an inversion. </p>"},{"location":"CSE222_ADA/1..divide_and_conquer/4..inversion_counting/#motivation","title":"Motivation","text":"<p>Why do we care about the number of inversions? Suppose we ask two people to give ratings to elements in a set. This is done often with movie or show review websites, but can also be applied to other things like elections. </p>"},{"location":"CSE222_ADA/1..divide_and_conquer/4..inversion_counting/#naive-approach","title":"Naive Approach","text":"<p>A naive approach would look at every pair of elements from \\(1\\) to \\(n\\). It will then check if this \\((i, j)\\) pair is inverted or not. There will be \\(nC2\\) such pairs, giving us a complexity of \\(O(n^2)\\)</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/4..inversion_counting/#divide-and-conquer-approach","title":"Divide-and-Conquer Approach","text":"<p>The key observation here is:</p> <p>Suppose we partition the unsorted array \\(A\\) of length \\(n\\) into two partitions \\(X\\) and \\(Y\\). We can find the inversions of the format \\((X_i, Y_j)\\) in \\(O(n)\\) time. These can be referred to as cross-inversions or split-inversions, provided that they are sorted. How is this possible?</p> <p>Say \\(X = [3, 4, 6]\\) and \\(Y = [1, 2, 5]\\). A mergesort-style algorithm will combine these two into \\(A\\). We start by picking \\(1\\) from \\(Y\\), but that immediately tells us that it's smaller than every element in \\(X\\). So we know there are \\(3\\) inversions here {\\((3, 1), (4, 2), (6, 2)\\)}. In fact, every time we pick an element from \\(Y\\), we know it is smaller than every element remaining in \\(X\\) {call this \\(x\\)}. So, every time we pick an element from \\(Y\\), we add \\(x\\) inversions to the total. If an element is picked from \\(X\\), the natural order has been maintained. </p>"},{"location":"CSE222_ADA/1..divide_and_conquer/4..inversion_counting/#recurrence-relation","title":"Recurrence Relation","text":"<p>The subproblems are divided as: -&gt; Sort and count inversions to the left of the partition -&gt; Sort and count inversions to the right of the partition -&gt; Count the number of split inversions, add all three values, merge the array, and return that.</p> <p>This gives us a Recurrence relation of</p> \\[T(0, n) = 2T(n/2) + O(n)\\] <p>Because this is a tiny modification of MergeSort, it comes to a complexity of \\(O(n.logn)\\) as well</p>"},{"location":"CSE222_ADA/1..divide_and_conquer/4..inversion_counting/#pseudo-code","title":"Pseudo-Code","text":"\\[CountAndMerge(X, Y)\\] <pre><code>toret = []\nfor j, k in X, Y:\n    if j &lt;= k:\n        append j to toret\n        move head of X ahead\n    else:\n        append k to toret\n        move head of Y ahead\n        count += len(X, j onwards)\nreturn (toret, count)\n</code></pre> \\[SortAndCountInv(A)\\] <pre><code>if len(A) == 1:\n    return 0   &lt;- base case\n\nX = first half of A\nY = second half of A\n\n(B, x) = SortAndCountInv (X)\n(C, y) = SortAndCountInv (Y)\n(D, z) = CountAndMerge(B, C)\nreturn (D, x+y+z)\n</code></pre>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/","title":"Balls Maximisation","text":""},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#problem-statement","title":"Problem Statement","text":"<p>Say you have an array of balls with weights</p> \\[[2, 5, 6, 4, 3, 5]\\] <p>You aren't allowed to pick adjacent balls. The goal is to maximise the weight of balls that you choose.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#greedy-approach","title":"Greedy Approach","text":"<p>Pick the ball with the maximum weight, unless you have picked any of its neighbours. Repeat till no other ball can be picked.</p> <p>This seems (somewhat) sound, but can be disproved quickly via counterexample.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#recursive-approach","title":"Recursive Approach","text":"<p>The goal of the recursive approach is to break this problem into subproblems, such that an inner subproblem should never have to rely on the output of an outer subproblem.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#problem-definition","title":"Problem Definition","text":"<p>\\(B_i\\) := the i'th ball \\(W_i\\) := the weight of the i'th ball \\(S_i\\) := the optimal selection of balls up till the i'th ball \\(\\Omega_{\\,i}\\) := the optimal weight possible from selecting balls \\(B_1\\) through \\(B_i\\)</p> <p>The desired solution is \\(\\Omega_{\\,n}\\)</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#recurrence-relation","title":"Recurrence Relation","text":"<p>Consider the series </p> \\[\\quad B_1, B_2, B_3, B_4, ... B_{n-1}, B_n\\] <p>\\(B_i\\) may or may not be a part of \\(S_i\\). In the event that it is, \\(B_{i-1}\\) cannot be a part of \\(S_i\\). So, the optimal weight becomes</p> \\[\\quad W_i  + \\Omega_{\\,i-2}\\] <p>In the other case, \\(B_i\\) is not a part of \\(S_i\\) ,anything from \\(B_1\\) to \\(B_{i-1}\\) may be included in \\(S_i\\). However, this means that \\(S_i\\) and \\(S_{i-1}\\) must be the same (by definition). This gives us the final weight as</p> \\[\\quad \\Omega_{i-1}\\] <p>We have the power to choose from either of these cases. And because we want the maximum weight overall, we will choose maximum of the two possibilities. This gives us the optimal solution as</p> \\[ \\quad \\Omega_i = max     \\begin {cases}         W_i + \\Omega_{i-2} &amp; \\; B_i \\; selected\\\\         \\Omega_{i-1} &amp; \\; B_i \\; not\\  selected\\\\     \\end {cases} \\]"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#recursive-algorithm","title":"Recursive Algorithm","text":"<p>Base Case: if i=1, \\(\\Omega_i\\) = \\(W_i\\) Recursive Case: if i &gt; 1, \\(\\Omega_i\\) = \\(max(W_i + \\Omega_{i-2},\\; \\Omega_{i-1})\\)  Starting: \\(\\Omega_n\\)</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#runtime","title":"Runtime","text":"<p>The runtime is observed as \\(O(2^n)\\), which honestly sucks.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#memoisation","title":"Memoisation","text":"<p>With memoisation, we can boost the runtime to \\(O(n)\\), which is honestly epic. However, this means we also needs \\(O(n)\\) space alongside it.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#iterative-algorithm","title":"Iterative Algorithm","text":"<p>The iterative approach converts this recursive algorithm to an iterative one.</p> <p>for each \\(i\\)'th ball from left to right,     \\(SelectUpto(i)\\)</p> <p>Where \\(SelectUpto\\) is the same as that of the recursive algorithm.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/1..balls_maximisation/#reconstruction","title":"Reconstruction","text":"<p>A ball is only added to the set if Case I is achieved, i.e. if it's better to add it.</p> <p>so we can modify the algorithm as such,</p> <pre><code>S = []\ni = n\nwhile i &gt;= 1:\n    if maxw[i] = maxw[2] + W[i]:\n        add B[i] to S\n        i -= 2\n    else:\n        i -= 1\n</code></pre>"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/","title":"Knapsack","text":""},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#problem-statement","title":"Problem Statement","text":"<p>Say you have a knapsack of size \\(W\\) (\\(W \\in I, W &gt; 0\\)). You have \\(n\\) indivisible items to put in that knapsack. Each item has a weight of \\(w_i\\) , and value of \\(v_i\\) . Your goal is to maximise the value inside the knapsack, without overfilling it.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#greedy-approach","title":"Greedy Approach","text":"<p>Start by picking the ball with highest value of \\(v_i / w_i\\) . Repeat this until the knapsack is filled.</p> <p>While this approach seems intuitive, it may fail. </p> <p>Consider this example</p> \\(w_i\\) \\(v_i\\) \\(v_i/w_i\\) 1 2 2 100 199 1.99 <p>\\(W = 100\\)</p> <p>in this case, the greedy approach starts by picking the first item. Then it picks one of the remaining items. This gives us a final value of 2 in the knapsack. Meanwhile, by picking the last item, one can get a value of 199. This can be extended to get a ratio between the optimal and greedy value as high as possible.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#recursive-approach","title":"Recursive Approach","text":"<p>\\(I_i\\) := the \\(i'th\\) item \\(w_i\\) := weight of the \\(i'th\\) item \\(v_i\\) := value of the \\(i'th\\) item \\(W\\) := maximum weight in knapsack \\(w\\) := weight of knapsack in subproblem \\(vmax(i, w)\\) := optimal solution for given value of \\(i\\) and \\(w\\)</p> <p>Consider the final state in which we have every optimal item in the knapsack. Now let us follow the steps of this approach in reverse. </p> <p>Say the last item was in the knapsack. So we remove this item. Now the knapsack is optimal for a weight up till \\(W - w_n\\) , and can only contain items between \\(1\\) and \\(n-1\\).</p> <p>Otherwise, the last item cannot be in the knapsack. So, the knapsack must contain items between \\(1\\) and \\(n-1\\), and must be optimal up to a weight of \\(W\\). </p> <p>Now we end up in the same state as before. We can repeat this till we get to any \\(i'th\\) item. So, the recurrence relation for the \\(i'th\\) item is given as</p> \\[ vmax(i, w) = max \\begin {cases}     vmax(i-1, w-w_i) + v_i &amp; \\quad I_i \\; chosen \\\\     vmax(i-1, w) &amp; \\quad I_i\\; not\\ chosen\\\\     \\end {cases} \\]"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#recursive-algorithm","title":"Recursive Algorithm","text":"<p>Base Cases: </p> <ul> <li>for \\(w=0\\), \\(vmax =0\\) {there is no space to add elements}</li> <li>for \\(i = 0\\), \\(vmax = 0\\) {there are no elements to add}</li> </ul> <p>Recursive Case:</p> <ul> <li>\\(vmax(i,w) = max(vmax(i-1, w-w_i) + v_i, vmax(i-1, w))\\) </li> </ul> <p>Starting:</p> <ul> <li>\\(vmax(n, W)\\) </li> </ul>"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#runtime","title":"Runtime","text":"<p>This algorithm has a runtime of \\(O(2^n)\\), which again kinda sucks.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#memoisation","title":"Memoisation","text":"<p>With the power of memoisation, we can reduce the runtime to \\(O(nW)\\) {which needn't necessarily be better than \\(2^n\\), but it'll mostly be}.</p> <p>This is referred to as a pseudopolynomial runtime.</p> <p>However, we will need \\(O(nW)\\) space to use memoisation. This isn't always desirable.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#runtime-demonstration","title":"Runtime Demonstration","text":"I w v 1 4 3 2 3 2 3 2 4 4 3 4 <p>\\(W=6\\)</p> i\\w 0 1 2 3 4 5 6 0 0 0 0 0 0 0 0 1 0 0 0 0 3 3 3 2 0 0 0 2 3 3 3 3 0 0 4 4 4 6 7 4 0 0 4 4 4 8 8"},{"location":"CSE222_ADA/2..dynamic_programming/2..knapsack/#iterative-approach","title":"Iterative Approach","text":"<p>An iterative approach to this problem just focuses on filling the table. We can traverse row major or column major.</p> <pre><code>for w = 0 to W:\n    vmax[0, w] = 0\nfor i = 1 to n:\n    for i = 0 to W:\n        if (wt[i] &gt; w):\n            vmax[i, w] = vmax[i-1, w]\n        else:\n            vmax[i, w] = max(vmax[i-1, w], vmax[i-1, w - wt[i]] + v[i])\nreturn vmax[n, W]\n</code></pre>"},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/","title":"Matrix Chaining","text":""},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/#problem-definition","title":"Problem Definition","text":"<p>Given a set of \\(n\\) matrices {\\(A_i\\)}, the goal of this algorithm is to multiply matrices in the optimal order, to minimise number of computations required.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/#motivation","title":"Motivation","text":"<p>Multiplying matrices quickly is obviously important for many mathematical tools (cough cough, linear algebra), but the applications of this algorithm are far beyond that. Anything which involves optimally combining \\(n\\) elements, with known costs / benefits of combining \\(k\\; (&lt; n)\\) elements can utilise this algorithm.   </p>"},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/#subproblem-definition","title":"Subproblem Definition","text":"<p>\\(\\Omega(X)\\) = Optimal cost for computing value of \\(X\\) (contiguous) matrix set. \\(\\sigma(X, Y)\\) = Cost of multiplying the result of \\(X\\) matrix set and \\(Y\\) matrix set. Equivalent to \\(rows(X_1) \\cdot cols(Y_y)\\)</p>"},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/#recurrence","title":"Recurrence","text":"\\[\\Omega(A) = min(\\Omega(X) + \\Omega(X') + \\sigma(X,X'))\\] <p>provided \\(X\\) and \\(X'\\) constitute the optimal breakdown of matrix set \\(A\\), assuming \\(X\\) ends at element \\(A_k\\). We can do this by considering every breakdown, and taking the minimum of those. </p> <p>Base Case: \\(\\Omega(M) = 0\\) if \\(|M| = 1\\)</p>"},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/#complexity","title":"Complexity","text":"<p>The DP table has a size of \\(n^2\\), and filling each cell takes a time of \\(O(n)\\). This gives us a total complexity of \\(O(n^3)\\). The space complexity is proportional to the size of the table, which is just \\(O(n^2)\\).  Note that we only fill the upper triangle of the DP table.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/3..matrix_chaining/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>for i form 1 to n:\n    DP[i, i] = 0 \nfor length from 1 to n-1:\n    for i from 1 to n-length:\n        j = i + length\n        tot_min = 1000000000\n        for k from i+1 to j:\n            DP[i, j] = min{tot_min, DP[i, k] + DP[k+1, j] + sigma(k)}\nreturn DP[1, n]\n</code></pre>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/","title":"Sequence Alignment","text":""},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#edit-distance","title":"Edit Distance","text":"<p>The Needleman-Wunsch score defines the edit distance as the sum of 'cost' of mismatches and the 'cost' of gaps.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#motivation","title":"Motivation","text":"<p>Edit distance is useful for finding the difference between two strings. This is helpful in using <code>diff</code> operations, autocorrect, speech recognition, and computational biology.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#problem-definition","title":"Problem Definition","text":"<p>We are given two strings, \\(X = [x_i]\\) and \\(Y = [y_i]\\)  of lengths \\(m\\) and \\(n\\) respectively. The goal is to find the insertion of gaps so as to achieve the minimum edit distance for these two strings.</p> <p>\\(\\Omega(A, B)\\) = optimal solution for strings \\(A\\) and \\(B\\).</p>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#subproblem-definition","title":"Subproblem Definition","text":"<p>Let's look at the end of these strings. We have three possibilities. One is that they're both perfectly aligned, the other two involve one of them ending in a gap. Let us define the ideal strings (gaps included) as \\(X'\\) and \\(Y'\\), of lenths \\(m'\\) and \\(n'\\)  respectively. We may end up with three cases:</p> <ul> <li>\\(m = n\\) =&gt; \\(\\Omega(X,Y) = f(\\Omega(X-x_m, Y-y_n))\\) </li> <li>\\(m &gt; n\\)  =&gt; \\(\\Omega(X,Y) = f(\\Omega(X-x_m, Y))\\)</li> <li>\\(m &lt; n\\)  =&gt; \\(\\Omega(X,Y) = f(\\Omega(X, Y-y_n))\\)</li> </ul>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#proof-of-optimal-substructure","title":"Proof of Optimal Substructure","text":"<p>Claim: \\(\\Omega(X,Y)\\) with \\(x_m, y_n\\) removed = \\(\\Omega(X-x_m, Y-y_n)\\).</p> \\[\\quad Let \\ \\Omega (X, Y) = \\Omega (X-x_m, Y-y_n) + \\alpha (x_m, y_n)\\] <p>If there is any other string \\(\\overline{X}\\) , \\(\\overline{Y}\\) that gives \\(\\Omega(\\overline{X},\\overline{Y}) &gt; \\Omega(X-x_m, Y-y_m)\\), that would imply that \\(\\Omega(\\overline{X}, \\overline{Y}) + \\alpha (x_m, y_m)\\) &gt; \\(\\Omega(X, Y)\\), which would violate the optimality of the solution. Therefore, no such string is possible.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#recurrence-relation","title":"Recurrence Relation","text":"<p>Define \\(A_i\\) as the string \\(A\\) up till \\(i\\) (included)</p> \\[ \\quad \\Omega(X_i, Y_j) = min     \\begin{cases}     \\Omega(X_{i-1}, Y_{j-1}) + \\alpha(x_i, y_j) \\\\      \\Omega(X_{i-1}, Y_j) + \\alpha(gap) \\\\     \\Omega(X_i, Y_{j-1}) + \\alpha(gap) \\\\     \\end{cases} \\] <p>Base Case:</p> \\[\\begin{align}   \\quad &amp; \\Omega(X_i, 0) = i \\cdot \\alpha(gap)\\\\         &amp; \\Omega(0, Y_j) = j \\cdot \\alpha(gap) \\end{align}\\]"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#complexity","title":"Complexity","text":"<p>This gives us time and space complexities of \\(\\Theta(mn)\\) , when we memoise the DP table.</p>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>Define P[m+1, n+1]\nAlign (X, Y):\n    for each i: P[i, 0] = i.alpha(gap)\n    for each j: P[0, j] = j.alpha(gap)\n    for i from 1 to m:\n        for j from 1 to n:\n            P[i, j] = min(P[i-1, j-1] + alpha(x[i], y[j]),\n                          P[i, j-1] + alpha(gap)\n                          P[i-1, j] + alpha(gap)\n                          )\n    return P[m, n]\n</code></pre>"},{"location":"CSE222_ADA/2..dynamic_programming/4..sequence_alignment/#space-optimisation","title":"Space Optimisation","text":"<p>Each row only needs the values in the current row, and the row above it to fill (provided we are filling the table in row major). In this scenario, we choose to only store the current row and the row above it. This improves our time complexity to \\(O(min(m, n))\\).</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/1..maximum_element_count/","title":"Maximum Element Count","text":""},{"location":"CSE222_ADA/3..greedy_algorithms/1..maximum_element_count/#problem-definition","title":"Problem Definition","text":"<p>You are given \\(n\\) tasks, with starting and finishing times \\((s_i, f_i)\\). The goal now is to find the maximum number of non-overlapping tasks that can be performed by one worker.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/1..maximum_element_count/#greedy-algorithm","title":"Greedy Algorithm","text":"<p>Sort [\\(t_i\\)] by Shortest Finishing Time First. Then pick the first element. Pick the next element if it doesn't overlap, or go to the one after if it does. Repeat this till you get to the last element.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/1..maximum_element_count/#proof-via","title":"Proof via ???","text":"<p>TODO</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/1..maximum_element_count/#time-complexity","title":"Time Complexity","text":"<p>This algorithm has a time complexity of \\(O(n \\log n)\\), which is the time taken to sort these \\(n\\) tasks. The picking afterwards is done in \\(O(n)\\) time.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/3..minimal_partitioning/","title":"Minimal Partitioning","text":""},{"location":"CSE222_ADA/3..greedy_algorithms/3..minimal_partitioning/#problem-definition","title":"Problem Definition","text":"<p>You have been given \\(n\\) tasks, [\\(t_i\\)], just like last time. Each task has a starting time and a finishing time \\((s_i, f_i)\\). The goal is to find the minimum number of workers needed to perform these tasks without needing any amount of overlap.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/3..minimal_partitioning/#motivation","title":"Motivation","text":"<p>A life-scale application is deciding the number of lecture halls needed to teach \\(n\\) courses.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/3..minimal_partitioning/#greedy-algorithm","title":"Greedy Algorithm","text":"<p>Start by sorting the tasks by their starting times. After doing this, start with one worker. Give the first task to the first worker. If another task is started before the current one ends, assign it to another worker. Repeat this for all \\(n\\) tasks.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/3..minimal_partitioning/#proof-via-lower-bound-for-the-optimal","title":"Proof via Lower Bound for the Optimal","text":"<p>Define the depth \\(d\\) of \\(T\\) as the maximum number of intervals active at any given time. The optimal number of workers needed must surely be equal to or more than the \\(d\\). We will prove that the greedy algorithm will never use more than \\(d\\) workers.</p> <p>Say, when we got to a task \\(t\\), there were \\(d\\) active workers already. So, the algorithm gave \\(t\\) to the \\(d+1th\\) worker. This means that there are \\(d+1\\) intervals overlapping with each other, so the depth must be \\(d+1\\). This is a direct contradiction, and thus not possible. </p>"},{"location":"CSE222_ADA/3..greedy_algorithms/3..minimal_partitioning/#time-complexity","title":"Time Complexity","text":"<p>\\(O(n \\log n)\\) is the time complexity of sorting these \\(n\\) tasks. The picking afterwards is done in \\(O(n^2)\\) time, giving a total time complexity of \\(O(n^2)\\).</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/","title":"Minimum Weighted Completion Time","text":""},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/#problem-definition","title":"Problem Definition","text":""},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/#minimum-total-completion-time","title":"Minimum Total Completion Time","text":"<p>We are given \\(n\\) tasks with lengths [\\(l_i\\)]. Our goal is to perform these tasks in an order such as to minimise the sum of finishing time for each task.</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/#minimum-weighted-completion-time_1","title":"Minimum Weighted Completion Time","text":"<p>In this problem, each tasks also have a weight \\(w_i\\). we have to miminise weighted completion time where weighted completion time \\(\\overline{C_i} = w_i \\cdot C_i\\)</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/#greedy-algorithm","title":"Greedy Algorithm","text":"<p>We sort the problems in order of \\(w_i\\,/\\,l_i\\) . Then we perform the tasks in that order. Now why would that give the optimal solution?</p>"},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/#proof-via-exchange-argument","title":"Proof via Exchange Argument","text":"<p>Say we arrange the tasks in the ideal order, which we are saying is also the hypothesised optimal order. Call this order as \\(\\sigma = [t_1, t_2, ... t_n]\\) </p> <p>Now suppose we create an order \\(\\sigma^*\\). This order is totally different from \\(\\sigma\\), but must have at least one inversion between adjacent elements. (Lemma: Having an inversion between non-adjacent elements will also create an inversion between some adjacent elements. Proof is (decently) trivial). Let's call these inverted adjacent elements as \\(t_q\\) and \\(t_p\\), </p> \\[ \\begin{align} &amp; \\overline{C}(\\sigma^*) = \\left(\\sum_{i\\neq p,q} C_i \\cdot w_i\\right) + C^*_p \\cdot w_p + C^*_q \\cdot w_q \\\\ &amp; \\overline{C}(\\sigma') = \\left(\\sum_{i\\neq p,q} C_i \\cdot w_i\\right) + C'_p \\cdot w_p + C'_q \\cdot w_q \\\\\\\\ &amp; \\overline{C}(\\sigma') - \\overline{C}(\\sigma^*) \\\\&amp;=   (C'_p - C^*_p)\\cdot w_p + (C'_q - C^*_q)\\cdot w_q \\\\&amp;= -l_q \\cdot w_p + l_p \\cdot w_q &gt; 0  \\end{align} \\] <p>By this, we can deduce that \\(\\sigma'\\) is a better arrangement. This means that no matter which arrangement we pick besides \\(\\sigma\\), a better arrangement exists. This means that \\(\\sigma\\) is the optimal arrangement. </p>"},{"location":"CSE222_ADA/3..greedy_algorithms/4..minimum_weighted_completion_time/#time-complexity","title":"Time Complexity","text":"<p>This algorithm has a time complexity of \\(O(n\\log n)\\), which is the time taken to sort these \\(n\\) tasks.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/1..depth_first_search/","title":"Depth First Search","text":""},{"location":"CSE222_ADA/4..graph_algorithms/1..depth_first_search/#problem-definition","title":"Problem Definition","text":"<p>Consider the following undirected graph.</p> <p><pre><code>graph LR;\n    1((1));\n    2((2));\n    3((3));\n    4((4));\n    5((5));\n    6((6));\n    7((7));\n    1 --- 2\n    2 --- 3\n    1 --- 3\n    4 --- 5\n    3 --- 6\n    6 --- 7\n    4 --- 6\n    2 ---- 6\n    2 --- 4\n    4 --- 7</code></pre> Say you want to go from \\(1\\) to \\(7\\). You want a traversal that will reliably and efficiently find this path.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/1..depth_first_search/#algorithm-definition","title":"Algorithm Definition","text":"<p>The algorithm goes from the source, and picks nodes randomly. It doesn't go to any node twice, and backtracks when it reaches a dead end. Eventually, it will approach the desired node (provided the source and destination are connected to each other).</p>"},{"location":"CSE222_ADA/4..graph_algorithms/1..depth_first_search/#observations","title":"Observations","text":"<ul> <li>Since each node is visited only once, we can model the path that the algorithm travels over as a tree. This will be referred to as a DFS Tree.</li> <li>There are no cross-edges possible. If subtree \\(S1\\) and \\(S2\\) have no forward edges connecting them, this means that there is no edge present between \\(S1\\) and \\(S2\\) (otherwise, it would have been visited before backtracking). Therefore, any edge besides a forward edge is a back edge.</li> </ul> <pre><code>graph LR\n    1((1));\n    2((2));\n    3((3));\n    4((4));\n    5((5));\n    6((6));\n    7((7));\n    1 --- 2\n    1 --- 3\n    3 --- 6\n    6 --- 7\n    4 --- 5\n    2 --- 4;\n    2 -.- 6\n    2 -.- 3\n    4 -.- 6</code></pre> <p>For example, one might think a graph with dotted edges as cross-edges here works. But, the algorithm must traverse \\(3\\) and \\(6\\) before backtracking from \\(2\\), so it is invalid.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/1..depth_first_search/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>make an array with 'visited' flag for all nodes\nmake an array for visiting time of all nodes\nstart time t as 0\n\nDFS(node v){\n    mark node v as visited\n    set time of visit as t\n    increment t\n    for every vertex w in adjacency list of v {\n        if w is not visited {\n            perform DFS(w)\n        }\n    }\n}\n</code></pre>"},{"location":"CSE222_ADA/4..graph_algorithms/1..depth_first_search/#runtime","title":"Runtime","text":"<ul> <li>Each node has to be visited at least once before the algorithm ends.</li> <li>Each edge need not be visited before the algorithm ends. However, since we are using an adjacency list, each edge is checked twice.</li> </ul> <p>This puts the total runtime as \\(O(V + E)\\)</p>"},{"location":"CSE222_ADA/4..graph_algorithms/2..bridge_edge_detection/","title":"Bridge Edge Detection","text":""},{"location":"CSE222_ADA/4..graph_algorithms/2..bridge_edge_detection/#problem-definition","title":"Problem Definition","text":"<p>Consider the following graph</p> <p><pre><code>graph LR;\n    1((1));\n    2((2));\n    3((3));\n    4((4));\n    5((5));\n    6((6));\n    7((7));\n    1 --- 2\n    2 --- 3\n    4 --- 5\n    3 --- 6\n    2 --- 6\n    2 --- 4\n    4 --- 7\n    5 --- 7 </code></pre> In this case, if we remove the edge \\(2-4\\), we get two disconnected components. Therefore, \\(2-4\\) counts as bridge edge. However, \\(2-6\\) does not do this, so it's not a bridge edge.</p> <p>We define a graph as 2-Edge Connected, If and only if it has no bridge edge. The problem, now, is to find whether or not a graph is 2EC.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/2..bridge_edge_detection/#observation","title":"Observation","text":"<ul> <li>Consider you are traversing \\(2-4\\). We know that it is a bridge edge. We can remark that there are no edges moving up from the subraph below \\(4\\) to the subgraph above \\(2\\). If that were true, the edge would no longer be a bridge edge, because there would be two edges connecting the subgraph to \\(1\\).</li> <li>Additionally, we only need to check backward edges, as a DFS tree is incapable of having cross edges.</li> <li>All we need to know about a subgraph to detect the bridge edge is the highest back-edge coming out of it.</li> </ul>"},{"location":"CSE222_ADA/4..graph_algorithms/2..bridge_edge_detection/#pseudocode","title":"Pseudocode","text":"<pre><code>define arrays for visited flags and visit time\ndefine time as 0\ndefine HBED (highest back edge destination) as 0\n\ncheck-2EC([vertex v] -&gt; HBED s) {\n    mark v as visited.\n    set time[v] and HBED as t++.\n\n    for each (vertex w) in adjacency list of (v),\n        if w is unvisited,\n            reduce HBED to check-2EC(w) or pass.\n        else,\n            reduce HBED to depth of w or pass.\n\n    if HBED is below or at v,\n        report bridge edge and terminate.\n    else, \n        report success.\n}\n</code></pre>"},{"location":"CSE222_ADA/4..graph_algorithms/2..bridge_edge_detection/#runtime","title":"Runtime","text":"<p>As this algorithm is a small modification on depth-first traversal, it retains the same runtime of \\(O(E + V)\\)</p>"},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/","title":"Dijkstra's Algorithm","text":""},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/#problem-statement","title":"Problem Statement","text":"<p>The goal of Dijkstra's algorithm is to find all the shortest paths connecting a source vertex \\(S\\), to every vertex in a directed graph. It can also be used to calculate just one Source-Destination pair efficiently.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/#approach","title":"Approach","text":"<p>We will divide the graph into two components, \\(S\\) and \\(S'\\). In \\(S\\), we store all the points whose shortest distance from the origin we know of. The goal will be to expand \\(S\\) till eventually it includes every point in the graph.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/#observations","title":"Observations","text":"<ul> <li>If we know the distance to every point in some region \\(S\\), we can tell with certainty the shortest distance to one and only one point in \\(S'\\) with absolute certainty. </li> <li>This point is the one with the minimum value of shortest distance to any point in \\(S\\), i.e. the closest point to \\(S\\).</li> <li>Any other point may have a shorter path to it from \\(S\\), which flows through this optimal path. </li> <li>At the start, the optimal path is only known for the source.</li> </ul>"},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>set distance of each vertex from source to +inf.\ncreate a min heap H with all vertices in, measured by their distance.\n\nwhile H is not empty,\n    frontier vertex f = delete_min(H).           - O(V. log V)\n    for all outgoing edges from (fv) to S',      - O(E. log V)\n        relax d[v] till d[f] + l[fv].\n        update the heap to reflect this change.  - O(log V)\n</code></pre>"},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/#runtime","title":"Runtime","text":"<p>We run heapify every time a point is moved into the frontier. As this happens no more than \\(V\\) times, the time taken by this is \\(O(V \\log V)\\). </p> <p>We check each edge once for potential relaxation and heap reset. So the runtime of this part is \\(O(E \\log V)\\). Therefore the total runtime is \\(O((E + V)log(V))\\) </p>"},{"location":"CSE222_ADA/4..graph_algorithms/3..dijkstra%27s_algorithm/#proof-of-correctness","title":"Proof Of Correctness","text":"<p>This proof is performed via induction.</p> <p>At any iteration, suppose \\(f\\) is the frontier vertex, being moved from \\(S'\\) to \\(S\\). Then, d[w] is the shortest path length from \\(S\\) to \\(w\\) </p> <p>Base Case - \\(S = \\{s_0\\}\\)</p> <p>Induction Hypothesis - Suppose we have \\(S\\) of some arbitrary size, and are moving a vertex \\(f\\). </p> \\[d[f] = d[u] + l_{uw} \\quad [U \\in S]\\] <p>Suppose an alternate shorter path \\(P\\) exists. If it connects directly from \\(S\\) to \\(f\\), then it cannot be shorter than the existing path. If it connects through \\(S'\\), it connects through another vertex, which is further apart, making it longer than the path given by Dijkstra.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/4..bellman-ford_algorithm/","title":"Bellman-ford Algorithm","text":""},{"location":"CSE222_ADA/4..graph_algorithms/4..bellman-ford_algorithm/#problem-statement","title":"Problem Statement","text":"<p>Imagine a directed graph, except now it has negative length edges too. Dijkstra's algorithm is no longer sufficient for this purpose. </p>"},{"location":"CSE222_ADA/4..graph_algorithms/4..bellman-ford_algorithm/#observation","title":"Observation","text":"<p>This problem may not have a defined solution in the first place. In the presence of a negative cycle, there will be no longest path connecting a source to its destination.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/4..bellman-ford_algorithm/#approach","title":"Approach","text":"<p>Bellman-Ford handles this problem with a Dynamic Programming approach. The optimal path to a point is defined with both the terminal point, and the maximum number of edges in the path.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/4..bellman-ford_algorithm/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>define dp table of size V x V.\nset dp of all lengths pointing to the source as 0.\nset dp of zero length pointing to any vertex as 0.\n\nfor each (length i) from (1 to n-1),                   - O(V^3)\n    for each (vertex v),                               - O(V^2) \n        for each edge going into v (parent vertex u),  - O(V) \n            dp[v, i] = min(dp[u, i-1] + l[u, v]).      - O(1)\n</code></pre>"},{"location":"CSE222_ADA/4..graph_algorithms/4..bellman-ford_algorithm/#runtime-analysis","title":"Runtime Analysis","text":"<p>This algorithm has a runtime of \\(O(VE)\\). </p>"},{"location":"CSE222_ADA/4..graph_algorithms/5..kruskal%27s_algorithm/","title":"Kruskal's Algorithm","text":""},{"location":"CSE222_ADA/4..graph_algorithms/5..kruskal%27s_algorithm/#problem-statement","title":"Problem Statement","text":"<p>The goal is to find the minimum spanning tree of the graph. Define the weight of the tree as the sum of weight of each edge. Then, the MST is the tree touching each edge which has the minimum weight.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/5..kruskal%27s_algorithm/#observations","title":"Observations","text":"<ul> <li>Divide the graph into two subgraphs, \\((S, V-S)\\). The lightest edge crossing between these two sub-graphs must be in the MST. If it is not, then we can generate a minimum spanning structure of smaller size by replacing it with the smallest such edge.</li> </ul>"},{"location":"CSE222_ADA/4..graph_algorithms/5..kruskal%27s_algorithm/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>sort edges in increasing order of weight.      -- O(E.log(E))\ndefine T as the set of all edges in the MST.\n\nfor each (edge e) with (i from 0 to E),        -- O(E)\n    if adding e to T does not make a cycle,   \n        add e to T.                            -- O(1), upto V times\n</code></pre> <p>This requires \\(V\\) Find / Union operations to add a point to the chart. We can perform a find union in \\(O(log^*n)\\) time with path compression technique, which makes the complexity basically \\(O(V)\\). Total complexity is given as \\(O(E.\\log(V))\\) </p>"},{"location":"CSE222_ADA/4..graph_algorithms/5..kruskal%27s_algorithm/#find-union-algorithm","title":"Find-Union Algorithm","text":"<p>Basically, we link each element to it's \"parent\". For each chain, there is a ternimating element. We consider this as the parent of the graph component. If two elements share the same parent / leader, a cycle is formed. This can be evaluated very quickly.</p>"},{"location":"CSE222_ADA/4..graph_algorithms/5..kruskal%27s_algorithm/#correctness","title":"Correctness","text":"<p>Correctness of Kruskal's algorithm can be proven by contradiction.</p> <p>Kruskal's algorithm produces a graph with no cycles, by definition of the algorithm. That is the first requirement.</p> <p>Secondly, Consider a situation with components \\(S\\) and \\(S'\\), and an edge \\(e\\) between them, given by Kruskal's algorithm. Assume there exists an edge \\(e'\\), such that picking \\(e'\\) over \\(e\\) gives a better outcome. Since the weights inside \\(S\\) and \\(S'\\) are independent of which edge is chosen, the difference comes only from this bridge edge. Since Kruskal's algorithm selects edges in ascending order of weight, if \\(e'\\) were truly lighter than \\(e\\), it would've been chosen first. </p>"}]}